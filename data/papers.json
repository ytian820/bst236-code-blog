{
  "last_updated": "2026-02-19 01:51 UTC",
  "keywords": [
    "Medical reasoning LLM",
    "Large language model",
    "Agentic AI"
  ],
  "papers": [
    {
      "id": "http://arxiv.org/abs/2602.15791v1",
      "title": "Enhancing Building Semantics Preservation in AI Model Training with Large Language Model Encodings",
      "authors": [
        "Suhyung Jang",
        "Ghang Lee",
        "Jaekun Lee",
        "Hyunjun Lee"
      ],
      "abstract": "Accurate representation of building semantics, encompassing both generic object types and specific subtypes, is essential for effective AI model training in the architecture, engineering, construction, and operation (AECO) industry. Conventional encoding methods (e.g., one-hot) often fail to convey the nuanced relationships among closely related subtypes, limiting AI's semantic comprehension. To address this limitation, this study proposes a novel training approach that employs large language model (LLM) embeddings (e.g., OpenAI GPT and Meta LLaMA) as encodings to preserve finer distinctions in building semantics. We evaluated the proposed method by training GraphSAGE models to classify 42 building object subtypes across five high-rise residential building information models (BIMs). Various embedding dimensions were tested, including original high-dimensional LLM embeddings (1,536, 3,072, or 4,096) and 1,024-dimensional compacted embeddings generated via the Matryoshka representation model. Experimental results demonstrated that LLM encodings outperformed the conventional one-hot baseline, with the llama-3 (compacted) embedding achieving a weighted average F1-score of 0.8766, compared to 0.8475 for one-hot encoding. The results underscore the promise of leveraging LLM-based encodings to enhance AI's ability to interpret complex, domain-specific building semantics. As the capabilities of LLMs and dimensionality reduction techniques continue to evolve, this approach holds considerable potential for broad application in semantic elaboration tasks throughout the AECO industry.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15791v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.15769v1",
      "title": "ViTaB-A: Evaluating Multimodal Large Language Models on Visual Table Attribution",
      "authors": [
        "Yahia Alqurnawi",
        "Preetom Biswas",
        "Anmol Rao",
        "Tejas Anvekar",
        "Chitta Baral",
        "Vivek Gupta"
      ],
      "abstract": "Multimodal Large Language Models (mLLMs) are often used to answer questions in structured data such as tables in Markdown, JSON, and images. While these models can often give correct answers, users also need to know where those answers come from. In this work, we study structured data attribution/citation, which is the ability of the models to point to the specific rows and columns that support an answer. We evaluate several mLLMs across different table formats and prompting strategies. Our results show a clear gap between question answering and evidence attribution. Although question answering accuracy remains moderate, attribution accuracy is much lower, near random for JSON inputs, across all models. We also find that models are more reliable at citing rows than columns, and struggle more with textual formats than images. Finally, we observe notable differences across model families. Overall, our findings show that current mLLMs are unreliable at providing fine-grained, trustworthy attribution for structured data, which limits their usage in applications requiring transparency and traceability.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15769v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.15725v1",
      "title": "Recursive Concept Evolution for Compositional Reasoning in Large Language Models",
      "authors": [
        "Sarim Chaudhry"
      ],
      "abstract": "Large language models achieve strong performance on many complex reasoning tasks, yet their accuracy degrades sharply on benchmarks that require compositional reasoning, including ARC-AGI-2, GPQA, MATH, BBH, and HLE. Existing methods improve reasoning by expanding token-level search through chain-of-thought prompting, self-consistency, or reinforcement learning, but they leave the model's latent representation space fixed. When the required abstraction is not already encoded in this space, performance collapses. We propose Recursive Concept Evolution (RCE), a framework that enables pretrained language models to modify their internal representation geometry during inference. RCE introduces dynamically generated low-rank concept subspaces that are spawned when representational inadequacy is detected, selected through a minimum description length criterion, merged when synergistic, and consolidated via constrained optimization to preserve stability. This process allows the model to construct new abstractions rather than recombining existing ones. We integrate RCE with Mistral-7B and evaluate it across compositional reasoning benchmarks. RCE yields 12-18 point gains on ARC-AGI-2, 8-14 point improvements on GPQA and BBH, and consistent reductions in depth-induced error on MATH and HLE.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15725v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.15689v1",
      "title": "A Content-Based Framework for Cybersecurity Refusal Decisions in Large Language Models",
      "authors": [
        "Meirav Segal",
        "Noa Linder",
        "Omer Antverg",
        "Gil Gekker",
        "Tomer Fichman",
        "Omri Bodenheimer",
        "Edan Maor",
        "Omer Nevo"
      ],
      "abstract": "Large language models and LLM-based agents are increasingly used for cybersecurity tasks that are inherently dual-use. Existing approaches to refusal, spanning academic policy frameworks and commercially deployed systems, often rely on broad topic-based bans or offensive-focused taxonomies. As a result, they can yield inconsistent decisions, over-restrict legitimate defenders, and behave brittlely under obfuscation or request segmentation. We argue that effective refusal requires explicitly modeling the trade-off between offensive risk and defensive benefit, rather than relying solely on intent or offensive classification. In this paper, we introduce a content-based framework for designing and auditing cyber refusal policies that makes offense-defense tradeoffs explicit. The framework characterizes requests along five dimensions: Offensive Action Contribution, Offensive Risk, Technical Complexity, Defensive Benefit, and Expected Frequency for Legitimate Users, grounded in the technical substance of the request rather than stated intent. We demonstrate that this content-grounded approach resolves inconsistencies in current frontier model behavior and allows organizations to construct tunable, risk-aware refusal policies.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15689v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.15678v1",
      "title": "Revisiting Northrop Frye's Four Myths Theory with Large Language Models",
      "authors": [
        "Edirlei Soares de Lima",
        "Marco A. Casanova",
        "Antonio L. Furtado"
      ],
      "abstract": "Northrop Frye's theory of four fundamental narrative genres (comedy, romance, tragedy, satire) has profoundly influenced literary criticism, yet computational approaches to his framework have focused primarily on narrative patterns rather than character functions. In this paper, we present a new character function framework that complements pattern-based analysis by examining how archetypal roles manifest differently across Frye's genres. Drawing on Jungian archetype theory, we derive four universal character functions (protagonist, mentor, antagonist, companion) by mapping them to Jung's psychic structure components. These functions are then specialized into sixteen genre-specific roles based on prototypical works. To validate this framework, we conducted a multi-model study using six state-of-the-art Large Language Models (LLMs) to evaluate character-role correspondences across 40 narrative works. The validation employed both positive samples (160 valid correspondences) and negative samples (30 invalid correspondences) to evaluate whether models both recognize valid correspondences and reject invalid ones. LLMs achieved substantial performance (mean balanced accuracy of 82.5%) with strong inter-model agreement (Fleiss' $κ$ = 0.600), demonstrating that the proposed correspondences capture systematic structural patterns. Performance varied by genre (ranging from 72.7% to 89.9%) and role (52.5% to 99.2%), with qualitative analysis revealing that variations reflect genuine narrative properties, including functional distribution in romance and deliberate archetypal subversion in satire. This character-based approach demonstrates the potential of LLM-supported methods for computational narratology and provides a foundation for future development of narrative generation methods and interactive storytelling applications.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15678v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.15532v1",
      "title": "Quantifying construct validity in large language model evaluations",
      "authors": [
        "Ryan Othniel Kearns"
      ],
      "abstract": "The LLM community often reports benchmark results as if they are synonymous with general model capabilities. However, benchmarks can have problems that distort performance, like test set contamination and annotator error. How can we know that a benchmark is a reliable indicator of some capability that we want to measure? This question concerns the construct validity of LLM benchmarks, and it requires separating benchmark results from capabilities when we model and predict LLM performance.   Both social scientists and computer scientists propose formal models - latent factor models and scaling laws - for identifying the capabilities underlying benchmark scores. However, neither technique is satisfactory for construct validity. Latent factor models ignore scaling laws, and as a result, the capabilities they extract often proxy model size. Scaling laws ignore measurement error, and as a result, the capabilities they extract are both uninterpretable and overfit to the observed benchmarks.   This thesis presents the structured capabilities model, the first model to extract interpretable and generalisable capabilities from a large collection of LLM benchmark results. I fit this model and its two alternatives on a large sample of results from the OpenLLM Leaderboard. Structured capabilities outperform latent factor models on parsimonious fit indices, and exhibit better out-of-distribution benchmark prediction than scaling laws. These improvements are possible because neither existing approach separates model scale from capabilities in the appropriate way. Model scale should inform capabilities, as in scaling laws, and these capabilities should inform observed results up to measurement error, as in latent factor models. In combining these two insights, structured capabilities demonstrate better explanatory and predictive power for quantifying construct validity in LLM evaluations.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15532v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.15461v1",
      "title": "Emergent Morphing Attack Detection in Open Multi-modal Large Language Models",
      "authors": [
        "Marija Ivanovska",
        "Vitomir Štruc"
      ],
      "abstract": "Face morphing attacks threaten biometric verification, yet most morphing attack detection (MAD) systems require task-specific training and generalize poorly to unseen attack types. Meanwhile, open-source multimodal large language models (MLLMs) have demonstrated strong visual-linguistic reasoning, but their potential in biometric forensics remains underexplored. In this paper, we present the first systematic zero-shot evaluation of open-source MLLMs for single-image MAD, using publicly available weights and a standardized, reproducible protocol. Across diverse morphing techniques, many MLLMs show non-trivial discriminative ability without any fine-tuning or domain adaptation, and LLaVA1.6-Mistral-7B achieves state-of-the-art performance, surpassing highly competitive task-specific MAD baselines by at least 23% in terms of equal error rate (EER). The results indicate that multimodal pretraining can implicitly encode fine-grained facial inconsistencies indicative of morphing artifacts, enabling zero-shot forensic sensitivity. Our findings position open-source MLLMs as reproducible, interpretable, and competitive foundations for biometric security and forensic image analysis. This emergent capability also highlights new opportunities to develop state-of-the-art MAD systems through targeted fine-tuning or lightweight adaptation, further improving accuracy and efficiency while preserving interpretability. To support future research, all code and evaluation protocols will be released upon publication.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15461v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.15449v1",
      "title": "TAROT: Test-driven and Capability-adaptive Curriculum Reinforcement Fine-tuning for Code Generation with Large Language Models",
      "authors": [
        "Chansung Park",
        "Juyong Jiang",
        "Fan Wang",
        "Sayak Paul",
        "Jiasi Shen",
        "Jing Tang",
        "Jianguo Li"
      ],
      "abstract": "Large Language Models (LLMs) are changing the coding paradigm, known as vibe coding, yet synthesizing algorithmically sophisticated and robust code still remains a critical challenge. Incentivizing the deep reasoning capabilities of LLMs is essential to overcoming this hurdle. Reinforcement Fine-Tuning (RFT) has emerged as a promising strategy to address this need. However, most existing approaches overlook the heterogeneous difficulty and granularity inherent in test cases, leading to an imbalanced distribution of reward signals and consequently biased gradient updates during training. To address this, we propose Test-driven and cApability-adaptive cuRriculum reinfOrcement fine-Tuning (TAROT). TAROT systematically constructs, for each problem, a four-tier test suite (basic, intermediate, complex, edge), providing a controlled difficulty landscape for curriculum design and evaluation. Crucially, TAROT decouples curriculum progression from raw reward scores, enabling capability-conditioned evaluation and principled selection from a portfolio of curriculum policies rather than incidental test-case difficulty composition. This design fosters stable optimization and more efficient competency acquisition. Extensive experimental results reveal that the optimal curriculum for RFT in code generation is closely tied to a model's inherent capability, with less capable models achieving greater gains with an easy-to-hard progression, whereas more competent models excel under a hard-first curriculum. TAROT provides a reproducible method that adaptively tailors curriculum design to a model's capability, thereby consistently improving the functional correctness and robustness of the generated code. All code and data are released to foster reproducibility and advance community research at https://github.com/deep-diver/TAROT.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15449v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.15378v1",
      "title": "Making Large Language Models Speak Tulu: Structured Prompting for an Extremely Low-Resource Language",
      "authors": [
        "Prathamesh Devadiga",
        "Paras Chopra"
      ],
      "abstract": "Can large language models converse in languages virtually absent from their training data? We investigate this question through a case study on Tulu, a Dravidian language with over 2 million speakers but minimal digital presence. Rather than fine-tuning an LLM, we examine whether structured prompts alone can elicit basic conversational ability under controlled prompting. We systematically tackle various challenges posed by absence of training data for Tulu by combining explicit grammar documentation, negative constraints to suppress high-probability tokens from related languages, romanization standardization, and quality-controlled synthetic data generation via self-play. Evaluated on a manually curated held-out set across three LLMs (Gemini 2.0 Flash, GPT-4o, Llama 3.1 70B) and validated by native speakers, our approach reduces vocabulary contamination from 80% to 5% while achieving 85% grammatical accuracy. Cross-model analysis reveals that negative constraints provide consistent improvements (12--18 percentage points), while grammar documentation effects vary by model architecture (8--22 points).",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15378v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.15344v1",
      "title": "ER-MIA: Black-Box Adversarial Memory Injection Attacks on Long-Term Memory-Augmented Large Language Models",
      "authors": [
        "Mitchell Piehl",
        "Zhaohan Xi",
        "Zuobin Xiong",
        "Pan He",
        "Muchao Ye"
      ],
      "abstract": "Large language models (LLMs) are increasingly augmented with long-term memory systems to overcome finite context windows and enable persistent reasoning across interactions. However, recent research finds that LLMs become more vulnerable because memory provides extra attack surfaces. In this paper, we present the first systematic study of black-box adversarial memory injection attacks that target the similarity-based retrieval mechanism in long-term memory-augmented LLMs. We introduce ER-MIA, a unified framework that exposes this vulnerability and formalizes two realistic attack settings: content-based attacks and question-targeted attacks. In these settings, ER-MIA includes an arsenal of composable attack primitives and ensemble attacks that achieve high success rates under minimal attacker assumptions. Extensive experiments across multiple LLMs and long-term memory systems demonstrate that similarity-based retrieval constitutes a fundamental and system-level vulnerability, revealing security risks that persist across memory designs and application scenarios.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15344v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.15338v1",
      "title": "Discovering Implicit Large Language Model Alignment Objectives",
      "authors": [
        "Edward Chen",
        "Sanmi Koyejo",
        "Carlos Guestrin"
      ],
      "abstract": "Large language model (LLM) alignment relies on complex reward signals that often obscure the specific behaviors being incentivized, creating critical risks of misalignment and reward hacking. Existing interpretation methods typically rely on pre-defined rubrics, risking the omission of \"unknown unknowns\", or fail to identify objectives that comprehensively cover and are causal to the model behavior. To address these limitations, we introduce Obj-Disco, a framework that automatically decomposes an alignment reward signal into a sparse, weighted combination of human-interpretable natural language objectives. Our approach utilizes an iterative greedy algorithm to analyze behavioral changes across training checkpoints, identifying and validating candidate objectives that best explain the residual reward signal. Extensive evaluations across diverse tasks, model sizes, and alignment algorithms demonstrate the framework's robustness. Experiments with popular open-source reward models show that the framework consistently captures > 90% of reward behavior, a finding further corroborated by human evaluation. Additionally, a case study on alignment with an open-source reward model reveals that Obj-Disco can successfully identify latent misaligned incentives that emerge alongside intended behaviors. Our work provides a crucial tool for uncovering the implicit objectives in LLM alignment, paving the way for more transparent and safer AI development.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15338v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.15312v1",
      "title": "Extracting Consumer Insight from Text: A Large Language Model Approach to Emotion and Evaluation Measurement",
      "authors": [
        "Stephan Ludwig",
        "Peter J. Danaher",
        "Xiaohao Yang",
        "Yu-Ting Lin",
        "Ehsan Abedin",
        "Dhruv Grewal",
        "Lan Du"
      ],
      "abstract": "Accurately measuring consumer emotions and evaluations from unstructured text remains a core challenge for marketing research and practice. This study introduces the Linguistic eXtractor (LX), a fine-tuned, large language model trained on consumer-authored text that also has been labeled with consumers' self-reported ratings of 16 consumption-related emotions and four evaluation constructs: trust, commitment, recommendation, and sentiment. LX consistently outperforms leading models, including GPT-4 Turbo, RoBERTa, and DeepSeek, achieving 81% macro-F1 accuracy on open-ended survey responses and greater than 95% accuracy on third-party-annotated Amazon and Yelp reviews. An application of LX to online retail data, using seemingly unrelated regression, affirms that review-expressed emotions predict product ratings, which in turn predict purchase behavior. Most emotional effects are mediated by product ratings, though some emotions, such as discontent and peacefulness, influence purchase directly, indicating that emotional tone provides meaningful signals beyond star ratings. To support its use, a no-code, cost-free, LX web application is available, enabling scalable analyses of consumer-authored text. In establishing a new methodological foundation for consumer perception measurement, this research demonstrates new methods for leveraging large language models to advance marketing research and practice, thereby achieving validated detection of marketing constructs from consumer data.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15312v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.15816v1",
      "title": "Developing AI Agents with Simulated Data: Why, what, and how?",
      "authors": [
        "Xiaoran Liu",
        "Istvan David"
      ],
      "abstract": "As insufficient data volume and quality remain the key impediments to the adoption of modern subsymbolic AI, techniques of synthetic data generation are in high demand. Simulation offers an apt, systematic approach to generating diverse synthetic data. This chapter introduces the reader to the key concepts, benefits, and challenges of simulation-based synthetic data generation for AI training purposes, and to a reference framework to describe, design, and analyze digital twin-based AI simulation solutions.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15816v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.15776v1",
      "title": "GlobeDiff: State Diffusion Process for Partial Observability in Multi-Agent Systems",
      "authors": [
        "Yiqin Yang",
        "Xu Yang",
        "Yuhua Jiang",
        "Ni Mu",
        "Hao Hu",
        "Runpeng Xie",
        "Ziyou Zhang",
        "Siyuan Li",
        "Yuan-Hua Ni",
        "Qianchuan Zhao",
        "Bo Xu"
      ],
      "abstract": "In the realm of multi-agent systems, the challenge of \\emph{partial observability} is a critical barrier to effective coordination and decision-making. Existing approaches, such as belief state estimation and inter-agent communication, often fall short. Belief-based methods are limited by their focus on past experiences without fully leveraging global information, while communication methods often lack a robust model to effectively utilize the auxiliary information they provide. To solve this issue, we propose Global State Diffusion Algorithm~(GlobeDiff) to infer the global state based on the local observations. By formulating the state inference process as a multi-modal diffusion process, GlobeDiff overcomes ambiguities in state estimation while simultaneously inferring the global state with high fidelity. We prove that the estimation error of GlobeDiff under both unimodal and multi-modal distributions can be bounded. Extensive experimental results demonstrate that GlobeDiff achieves superior performance and is capable of accurately inferring the global state.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15776v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.15763v1",
      "title": "GLM-5: from Vibe Coding to Agentic Engineering",
      "authors": [
        "GLM-5 Team",
        ":",
        "Aohan Zeng",
        "Xin Lv",
        "Zhenyu Hou",
        "Zhengxiao Du",
        "Qinkai Zheng",
        "Bin Chen",
        "Da Yin",
        "Chendi Ge",
        "Chengxing Xie",
        "Cunxiang Wang",
        "Gengzheng Pan",
        "Hao Zeng",
        "Haoke Zhang",
        "Haoran Wang",
        "Huilong Chen",
        "Jiajie Zhang",
        "Jian Jiao",
        "Jiaqi Guo",
        "Jingsen Wang",
        "Jingzhao Du",
        "Jinzhu Wu",
        "Kedong Wang",
        "Lei Li",
        "Lin Fan",
        "Lucen Zhong",
        "Mingdao Liu",
        "Mingming Zhao",
        "Pengfan Du",
        "Qian Dong",
        "Rui Lu",
        "Shuang-Li",
        "Shulin Cao",
        "Song Liu",
        "Ting Jiang",
        "Xiaodong Chen",
        "Xiaohan Zhang",
        "Xuancheng Huang",
        "Xuezhen Dong",
        "Yabo Xu",
        "Yao Wei",
        "Yifan An",
        "Yilin Niu",
        "Yitong Zhu",
        "Yuanhao Wen",
        "Yukuo Cen",
        "Yushi Bai",
        "Zhongpei Qiao",
        "Zihan Wang",
        "Zikang Wang",
        "Zilin Zhu",
        "Ziqiang Liu",
        "Zixuan Li",
        "Bojie Wang",
        "Bosi Wen",
        "Can Huang",
        "Changpeng Cai",
        "Chao Yu",
        "Chen Li",
        "Chen Li",
        "Chenghua Huang",
        "Chengwei Hu",
        "Chenhui Zhang",
        "Chenzheng Zhu",
        "Congfeng Yin",
        "Daoyan Lin",
        "Dayong Yang",
        "Di Wang",
        "Ding Ai",
        "Erle Zhu",
        "Fangzhou Yi",
        "Feiyu Chen",
        "Guohong Wen",
        "Hailong Sun",
        "Haisha Zhao",
        "Haiyi Hu",
        "Hanchen Zhang",
        "Hanrui Liu",
        "Hanyu Zhang",
        "Hao Peng",
        "Hao Tai",
        "Haobo Zhang",
        "He Liu",
        "Hongwei Wang",
        "Hongxi Yan",
        "Hongyu Ge",
        "Huan Liu",
        "Huan Liu",
        "Huanpeng Chu",
        "Jia'ni Zhao",
        "Jiachen Wang",
        "Jiajing Zhao",
        "Jiamin Ren",
        "Jiapeng Wang",
        "Jiaxin Zhang",
        "Jiayi Gui",
        "Jiayue Zhao",
        "Jijie Li",
        "Jing An",
        "Jing Li",
        "Jingwei Yuan",
        "Jinhua Du",
        "Jinxin Liu",
        "Junkai Zhi",
        "Junwen Duan",
        "Kaiyue Zhou",
        "Kangjian Wei",
        "Ke Wang",
        "Keyun Luo",
        "Laiqiang Zhang",
        "Leigang Sha",
        "Liang Xu",
        "Lindong Wu",
        "Lintao Ding",
        "Lu Chen",
        "Minghao Li",
        "Nianyi Lin",
        "Pan Ta",
        "Qiang Zou",
        "Rongjun Song",
        "Ruiqi Yang",
        "Shangqing Tu",
        "Shangtong Yang",
        "Shaoxiang Wu",
        "Shengyan Zhang",
        "Shijie Li",
        "Shuang Li",
        "Shuyi Fan",
        "Wei Qin",
        "Wei Tian",
        "Weining Zhang",
        "Wenbo Yu",
        "Wenjie Liang",
        "Xiang Kuang",
        "Xiangmeng Cheng",
        "Xiangyang Li",
        "Xiaoquan Yan",
        "Xiaowei Hu",
        "Xiaoying Ling",
        "Xing Fan",
        "Xingye Xia",
        "Xinyuan Zhang",
        "Xinze Zhang",
        "Xirui Pan",
        "Xunkai Zhang",
        "Yandong Wu",
        "Yanfu Li",
        "Yidong Wang",
        "Yifan Zhu",
        "Yijun Tan",
        "Yilin Zhou",
        "Yiming Pan",
        "Ying Zhang",
        "Yinpei Su",
        "Yipeng Geng",
        "Yipeng Geng",
        "Yong Yan",
        "Yonglin Tan",
        "Yuean Bi",
        "Yuhan Shen",
        "Yuhao Yang",
        "Yujiang Li",
        "Yunan Liu",
        "Yunqing Wang",
        "Yuntao Li",
        "Yurong Wu",
        "Yutao Zhang",
        "Yuxi Duan",
        "Yuxuan Zhang",
        "Zezhen Liu",
        "Zhengtao Jiang",
        "Zhenhe Yan",
        "Zheyu Zhang",
        "Zhixiang Wei",
        "Zhuo Chen",
        "Zhuoer Feng",
        "Zijun Yao",
        "Ziwei Chai",
        "Ziyuan Wang",
        "Zuzhou Zhang",
        "Bin Xu",
        "Minlie Huang",
        "Hongning Wang",
        "Juanzi Li",
        "Yuxiao Dong",
        "Jie Tang"
      ],
      "abstract": "We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering. Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benchmarks. Most critically, GLM-5 demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges. Code, models, and more information are available at https://github.com/zai-org/GLM-5.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15763v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.15721v1",
      "title": "Lifelong Scalable Multi-Agent Realistic Testbed and A Comprehensive Study on Design Choices in Lifelong AGV Fleet Management Systems",
      "authors": [
        "Jingtian Yan",
        "Yulun Zhang",
        "Zhenting Liu",
        "Han Zhang",
        "He Jiang",
        "Jingkai Chen",
        "Stephen F. Smith",
        "Jiaoyang Li"
      ],
      "abstract": "We present Lifelong Scalable Multi-Agent Realistic Testbed (LSMART), an open-source simulator to evaluate any Multi-Agent Path Finding (MAPF) algorithm in a Fleet Management System (FMS) with Automated Guided Vehicles (AGVs). MAPF aims to move a group of agents from their corresponding starting locations to their goals. Lifelong MAPF (LMAPF) is a variant of MAPF that continuously assigns new goals for agents to reach. LMAPF applications, such as autonomous warehouses, often require a centralized, lifelong system to coordinate the movement of a fleet of robots, typically AGVs. However, existing works on MAPF and LMAPF often assume simplified kinodynamic models, such as pebble motion, as well as perfect execution and communication for AGVs. Prior work has presented SMART, a software capable of evaluating any MAPF algorithms while considering agent kinodynamics, communication delays, and execution uncertainties. However, SMART is designed for MAPF, not LMAPF. Generalizing SMART to an FMS requires many more design choices. First, an FMS parallelizes planning and execution, raising the question of when to plan. Second, given planners with varying optimality and differing agent-model assumptions, one must decide how to plan. Third, when the planner fails to return valid solutions, the system must determine how to recover. In this paper, we first present LSMART, an open-source simulator that incorporates all these considerations to evaluate any MAPF algorithms in an FMS. We then provide experiment results based on state-of-the-art methods for each design choice, offering guidance on how to effectively design centralized lifelong AGV Fleet Management Systems. LSMART is available at https://smart-mapf.github.io/lifelong-smart.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15721v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.15682v1",
      "title": "The Next Paradigm Is User-Centric Agent, Not Platform-Centric Service",
      "authors": [
        "Luankang Zhang",
        "Hang Lv",
        "Qiushi Pan",
        "Kefen Wang",
        "Yonghao Huang",
        "Xinrui Miao",
        "Yin Xu",
        "Wei Guo",
        "Yong Liu",
        "Hao Wang",
        "Enhong Chen"
      ],
      "abstract": "Modern digital services have evolved into indispensable tools, driving the present large-scale information systems. Yet, the prevailing platform-centric model, where services are optimized for platform-driven metrics such as engagement and conversion, often fails to align with users' true needs. While platform technologies have advanced significantly-especially with the integration of large language models (LLMs)-we argue that improvements in platform service quality do not necessarily translate to genuine user benefit. Instead, platform-centric services prioritize provider objectives over user welfare, resulting in conflicts against user interests. This paper argues that the future of digital services should shift from a platform-centric to a user-centric agent. These user-centric agents prioritize privacy, align with user-defined goals, and grant users control over their preferences and actions. With advancements in LLMs and on-device intelligence, the realization of this vision is now feasible. This paper explores the opportunities and challenges in transitioning to user-centric intelligence, presents a practical device-cloud pipeline for its implementation, and discusses the necessary governance and ecosystem structures for its adoption.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15682v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.15654v1",
      "title": "Zombie Agents: Persistent Control of Self-Evolving LLM Agents via Self-Reinforcing Injections",
      "authors": [
        "Xianglin Yang",
        "Yufei He",
        "Shuo Ji",
        "Bryan Hooi",
        "Jin Song Dong"
      ],
      "abstract": "Self-evolving LLM agents update their internal state across sessions, often by writing and reusing long-term memory. This design improves performance on long-horizon tasks but creates a security risk: untrusted external content observed during a benign session can be stored as memory and later treated as instruction. We study this risk and formalize a persistent attack we call a Zombie Agent, where an attacker covertly implants a payload that survives across sessions, effectively turning the agent into a puppet of the attacker.   We present a black-box attack framework that uses only indirect exposure through attacker-controlled web content. The attack has two phases. During infection, the agent reads a poisoned source while completing a benign task and writes the payload into long-term memory through its normal update process. During trigger, the payload is retrieved or carried forward and causes unauthorized tool behavior. We design mechanism-specific persistence strategies for common memory implementations, including sliding-window and retrieval-augmented memory, to resist truncation and relevance filtering. We evaluate the attack on representative agent setups and tasks, measuring both persistence over time and the ability to induce unauthorized actions while preserving benign task quality. Our results show that memory evolution can convert one-time indirect injection into persistent compromise, which suggests that defenses focused only on per-session prompt filtering are not sufficient for self-evolving agents.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15654v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.15631v1",
      "title": "Meflex: A Multi-agent Scaffolding System for Entrepreneurial Ideation Iteration via Nonlinear Business Plan Writing",
      "authors": [
        "Lan Luo",
        "Dongyijie Primo Pan",
        "Junhua Zhu",
        "Muzhi Zhou",
        "Pan Hui"
      ],
      "abstract": "Business plan (BP) writing plays a key role in entrepreneurship education by helping learners construct, evaluate, and iteratively refine their ideas. However, conventional BP writing remains a rigid, linear process that often fails to reflect the dynamic and recursive nature of entrepreneurial ideation. This mismatch is particularly challenging for novice entrepreneurial students, who struggle with the substantial cognitive demands of developing and refining ideas. While reflection and meta-reflection are critical strategies for fostering divergent and convergent thinking, existing writing tools rarely scaffold these higher-order processes. To address this gap, we present the Meflex System, a large language model (LLM)-based writing tool that integrates BP writing scaffolding with a nonlinear idea canvas to support iterative ideation through reflection and meta-reflection. We report findings from an exploratory user study with 30 participants that examined the system's usability and cognitive impact. Results show that Meflex effectively scaffolds BP writing, promotes divergent thinking through LLM-supported reflection, and enhances meta-reflective awareness while reducing cognitive load during complex idea development. These findings highlight the potential of non-linear LLM-based writing tools to foster deeper and coherent entrepreneurial thinking.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15631v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.15607v1",
      "title": "Agent-based macroeconomics for the UK's Seventh Carbon Budget",
      "authors": [
        "Tom Youngman",
        "Tim Lennox",
        "M. Lopes Alves",
        "Pirta Palola",
        "Brendon Tankwa",
        "Emma Bailey",
        "Emilien Ravigne",
        "Thijs Ter Horst",
        "Benjamin Wagenvoort",
        "Harry Lightfoot Brown",
        "Jose Moran",
        "Doyne Farmer"
      ],
      "abstract": "In June 2026, the UK government will set its carbon budget for the period 2038 to 2042, the seventh such carbon budget (CB7) since the Climate Change Act became law in 2008. For the first time, this carbon budget will be accompanied by a macroeconomic assessment of its impact on growth, employment, inflation and inequality. Researchers from the Institute of New Economic Thinking (INET) Oxford are working in partnership with the Department for Energy Security and Net Zero to deliver this assessment using our data-driven macroeconomic agent-based model (ABM). This extended abstract presents the work in progress towards this pioneering policymaking using our data-driven macroeconomic ABM. We are conducting our work in three work packages. By the time of the workshop, we hope to be able to present preliminary findings from the first two work packages. In WP1, we adapt an existing macro-ABM prototype and build a UK macroeconomic baseline. The main task for this is initialising the model with suitable UK household microdata. We present the options considered and the approach settled upon. In WP2, we conduct preliminary modelling that represents UK decarbonisation as an external shock to financial flows and technical coefficients. In order to present results in time to influence the June 2026 policy decision, this second work package exogenously forces the ABM to follow the CB7 green investment and associated technological change projections provided by the Climate Change Committee. Finally, we will implement more sophisticated social and technological learning packages in WP3, building our own projections of likely decarbonisation pathways that may diverge from UK government plans. For the workshop, we will present the progress of WP1 and WP2.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15607v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.15572v1",
      "title": "Neural Network-Based Parameter Estimation of a Labour Market Agent-Based Model",
      "authors": [
        "M Lopes Alves",
        "Joel Dyer",
        "Doyne Farmer",
        "Michael Wooldridge",
        "Anisoara Calinescu"
      ],
      "abstract": "Agent-based modelling (ABM) is a widespread approach to simulate complex systems. Advancements in computational processing and storage have facilitated the adoption of ABMs across many fields; however, ABMs face challenges that limit their use as decision-support tools. A significant issue is parameter estimation in large-scale ABMs, particularly due to computational constraints on exploring the parameter space. This study evaluates a state-of-the-art simulation-based inference (SBI) framework that uses neural networks (NN) for parameter estimation. This framework is applied to an established labour market ABM based on job transition networks. The ABM is initiated with synthetic datasets and the real U.S. labour market. Next, we compare the effectiveness of summary statistics derived from a list of statistical measures with that learned by an embedded NN. The results demonstrate that the NN-based approach recovers the original parameters when evaluating posterior distributions across various dataset scales and improves efficiency compared to traditional Bayesian methods.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15572v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.15569v1",
      "title": "\"What Are You Doing?\": Effects of Intermediate Feedback from Agentic LLM In-Car Assistants During Multi-Step Processing",
      "authors": [
        "Johannes Kirmayr",
        "Raphael Wennmacher",
        "Khanh Huynh",
        "Lukas Stappen",
        "Elisabeth André",
        "Florian Alt"
      ],
      "abstract": "Agentic AI assistants that autonomously perform multi-step tasks raise open questions for user experience: how should such systems communicate progress and reasoning during extended operations, especially in attention-critical contexts such as driving? We investigate feedback timing and verbosity from agentic LLM-based in-car assistants through a controlled, mixed-methods study (N=45) comparing planned steps and intermediate results feedback against silent operation with final-only response. Using a dual-task paradigm with an in-car voice assistant, we found that intermediate feedback significantly improved perceived speed, trust, and user experience while reducing task load - effects that held across varying task complexities and interaction contexts. Interviews further revealed user preferences for an adaptive approach: high initial transparency to establish trust, followed by progressively reducing verbosity as systems prove reliable, with adjustments based on task stakes and situational context. We translate our empirical findings into design implications for feedback timing and verbosity in agentic assistants, balancing transparency and efficiency.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15569v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.15456v1",
      "title": "In Agents We Trust, but Who Do Agents Trust? Latent Source Preferences Steer LLM Generations",
      "authors": [
        "Mohammad Aflah Khan",
        "Mahsa Amani",
        "Soumi Das",
        "Bishwamittra Ghosh",
        "Qinyuan Wu",
        "Krishna P. Gummadi",
        "Manish Gupta",
        "Abhilasha Ravichander"
      ],
      "abstract": "Agents based on Large Language Models (LLMs) are increasingly being deployed as interfaces to information on online platforms. These agents filter, prioritize, and synthesize information retrieved from the platforms' back-end databases or via web search. In these scenarios, LLM agents govern the information users receive, by drawing users' attention to particular instances of retrieved information at the expense of others. While much prior work has focused on biases in the information LLMs themselves generate, less attention has been paid to the factors that influence what information LLMs select and present to users. We hypothesize that when information is attributed to specific sources (e.g., particular publishers, journals, or platforms), current LLMs exhibit systematic latent source preferences- that is, they prioritize information from some sources over others. Through controlled experiments on twelve LLMs from six model providers, spanning both synthetic and real-world tasks, we find that several models consistently exhibit strong and predictable source preferences. These preferences are sensitive to contextual framing, can outweigh the influence of content itself, and persist despite explicit prompting to avoid them. They also help explain phenomena such as the observed left-leaning skew in news recommendations in prior work. Our findings advocate for deeper investigation into the origins of these preferences, as well as for mechanisms that provide users with transparency and control over the biases guiding LLM-powered agents.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15456v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.15400v1",
      "title": "One Agent to Guide Them All: Empowering MLLMs for Vision-and-Language Navigation via Explicit World Representation",
      "authors": [
        "Zerui Li",
        "Hongpei Zheng",
        "Fangguo Zhao",
        "Aidan Chan",
        "Jian Zhou",
        "Sihao Lin",
        "Shijie Li",
        "Qi Wu"
      ],
      "abstract": "A navigable agent needs to understand both high-level semantic instructions and precise spatial perceptions. Building navigation agents centered on Multimodal Large Language Models (MLLMs) demonstrates a promising solution due to their powerful generalization ability. However, the current tightly coupled design dramatically limits system performance. In this work, we propose a decoupled design that separates low-level spatial state estimation from high-level semantic planning. Unlike previous methods that rely on predefined, oversimplified textual maps, we introduce an interactive metric world representation that maintains rich and consistent information, allowing MLLMs to interact with and reason on it for decision-making. Furthermore, counterfactual reasoning is introduced to further elicit MLLMs' capacity, while the metric world representation ensures the physical validity of the produced actions. We conduct comprehensive experiments in both simulated and real-world environments. Our method establishes a new zero-shot state-of-the-art, achieving 48.8\\% Success Rate (SR) in R2R-CE and 42.2\\% in RxR-CE benchmarks. Furthermore, to validate the versatility of our metric representation, we demonstrate zero-shot sim-to-real transfer across diverse embodiments, including a wheeled TurtleBot 4 and a custom-built aerial drone. These real-world deployments verify that our decoupled framework serves as a robust, domain-invariant interface for embodied Vision-and-Language navigation.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15400v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.15384v1",
      "title": "World-Model-Augmented Web Agents with Action Correction",
      "authors": [
        "Zhouzhou Shen",
        "Xueyu Hu",
        "Xiyun Li",
        "Tianqing Fang",
        "Juncheng Li",
        "Shengyu Zhang"
      ],
      "abstract": "Web agents based on large language models have demonstrated promising capability in automating web tasks. However, current web agents struggle to reason out sensible actions due to the limitations of predicting environment changes, and might not possess comprehensive awareness of execution risks, prematurely performing risky actions that cause losses and lead to task failure. To address these challenges, we propose WAC, a web agent that integrates model collaboration, consequence simulation, and feedback-driven action refinement. To overcome the cognitive isolation of individual models, we introduce a multi-agent collaboration process that enables an action model to consult a world model as a web-environment expert for strategic guidance; the action model then grounds these suggestions into executable actions, leveraging prior knowledge of environmental state transition dynamics to enhance candidate action proposal. To achieve risk-aware resilient task execution, we introduce a two-stage deduction chain. A world model, specialized in environmental state transitions, simulates action outcomes, which a judge model then scrutinizes to trigger action corrective feedback when necessary. Experiments show that WAC achieves absolute gains of 1.8% on VisualWebArena and 1.3% on Online-Mind2Web.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15384v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.15382v1",
      "title": "The Vision Wormhole: Latent-Space Communication in Heterogeneous Multi-Agent Systems",
      "authors": [
        "Xiaoze Liu",
        "Ruowang Zhang",
        "Weichen Yu",
        "Siheng Xiong",
        "Liu He",
        "Feijie Wu",
        "Hoin Jung",
        "Matt Fredrikson",
        "Xiaoqian Wang",
        "Jing Gao"
      ],
      "abstract": "Multi-Agent Systems (MAS) powered by Large Language Models have unlocked advanced collaborative reasoning, yet they remain shackled by the inefficiency of discrete text communication, which imposes significant runtime overhead and information quantization loss. While latent state transfer offers a high-bandwidth alternative, existing approaches either assume homogeneous sender-receiver architectures or rely on pair-specific learned translators, limiting scalability and modularity across diverse model families with disjoint manifolds. In this work, we propose the Vision Wormhole, a novel framework that repurposes the visual interface of Vision-Language Models (VLMs) to enable model-agnostic, text-free communication. By introducing a Universal Visual Codec, we map heterogeneous reasoning traces into a shared continuous latent space and inject them directly into the receiver's visual pathway, effectively treating the vision encoder as a universal port for inter-agent telepathy. Our framework adopts a hub-and-spoke topology to reduce pairwise alignment complexity from O(N^2) to O(N) and leverages a label-free, teacher-student distillation objective to align the high-speed visual channel with the robust reasoning patterns of the text pathway. Extensive experiments across heterogeneous model families (e.g., Qwen-VL, Gemma) demonstrate that the Vision Wormhole reduces end-to-end wall-clock time in controlled comparisons while maintaining reasoning fidelity comparable to standard text-based MAS. Code is available at https://github.com/xz-liu/heterogeneous-latent-mas",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15382v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.15325v1",
      "title": "AgriWorld:A World Tools Protocol Framework for Verifiable Agricultural Reasoning with Code-Executing LLM Agents",
      "authors": [
        "Zhixing Zhang",
        "Jesen Zhang",
        "Hao Liu",
        "Qinhan Lv",
        "Jing Yang",
        "Kaitong Cai",
        "Keze Wang"
      ],
      "abstract": "Foundation models for agriculture are increasingly trained on massive spatiotemporal data (e.g., multi-spectral remote sensing, soil grids, and field-level management logs) and achieve strong performance on forecasting and monitoring. However, these models lack language-based reasoning and interactive capabilities, limiting their usefulness in real-world agronomic workflows. Meanwhile, large language models (LLMs) excel at interpreting and generating text, but cannot directly reason over high-dimensional, heterogeneous agricultural datasets. We bridge this gap with an agentic framework for agricultural science. It provides a Python execution environment, AgriWorld, exposing unified tools for geospatial queries over field parcels, remote-sensing time-series analytics, crop growth simulation, and task-specific predictors (e.g., yield, stress, and disease risk). On top of this environment, we design a multi-turn LLM agent, Agro-Reflective, that iteratively writes code, observes execution results, and refines its analysis via an execute-observe-refine loop. We introduce AgroBench, with scalable data generation for diverse agricultural QA spanning lookups, forecasting, anomaly detection, and counterfactual \"what-if\" analysis. Experiments outperform text-only and direct tool-use baselines, validating execution-driven reflection for reliable agricultural reasoning.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15325v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.14763v1",
      "title": "Unlocking Reasoning Capability on Machine Translation in Large Language Models",
      "authors": [
        "Sara Rajaee",
        "Sebastian Vincent",
        "Alexandre Berard",
        "Marzieh Fadaee",
        "Kelly Marchisio",
        "Tom Kocmi"
      ],
      "abstract": "Reasoning-oriented large language models (RLMs) achieve strong gains on tasks such as mathematics and coding by generating explicit intermediate reasoning. However, their impact on machine translation (MT) remains underexplored. We systematically evaluate several open- and closed-weights RLMs on the WMT24++ benchmark and find that enabling explicit reasoning consistently degrades translation quality across languages and models. Analysis reveals that MT reasoning traces are highly linear, lacking revision, self-correction and exploration of alternative translations, which limits their usefulness. Furthermore, injecting higher-quality reasoning traces from stronger models does not reliably improve weaker models' performance. To address this mismatch, we propose a structured reasoning framework tailored to translation, based on multi-step drafting, adequacy refinement, fluency improvement, and selective iterative revision. We curate a synthetic dataset of dynamic structured reasoning traces and post-train a large reasoning model on this data. Experiments show significant improvements over standard translation fine-tuning and injected generic reasoning baselines. Our findings demonstrate that reasoning must be task-structured to benefit MT.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "pdf_url": "https://arxiv.org/pdf/2602.14763v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.14743v1",
      "title": "LLMStructBench: Benchmarking Large Language Model Structured Data Extraction",
      "authors": [
        "Sönke Tenckhoff",
        "Mario Koddenbrock",
        "Erik Rodner"
      ],
      "abstract": "We present LLMStructBench, a novel benchmark for evaluating Large Language Models (LLMs) on extracting structured data and generating valid JavaScript Object Notation (JSON) outputs from natural-language text. Our open dataset comprises diverse, manually verified parsing scenarios of varying complexity and enables systematic testing across 22 models and five prompting strategies. We further introduce complementary performance metrics that capture both token-level accuracy and document-level validity, facilitating rigorous comparison of model, size, and prompting effects on parsing reliability.   In particular, we show that choosing the right prompting strategy is more important than standard attributes such as model size. This especially ensures structural validity for smaller or less reliable models but increase the number of semantic errors. Our benchmark suite is an step towards future research in the area of LLM applied to parsing or Extract, Transform and Load (ETL) applications.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "pdf_url": "https://arxiv.org/pdf/2602.14743v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.14733v1",
      "title": "More than Decision Support: Exploring Patients' Longitudinal Usage of Large Language Models in Real-World Healthcare-Seeking Journeys",
      "authors": [
        "Yancheng Cao",
        "Yishu Ji",
        "Chris Yue Fu",
        "Sahiti Dharmavaram",
        "Meghan Turchioe",
        "Natalie C Benda",
        "Lena Mamykina",
        "Yuling Sun",
        "Xuhai \"Orson\" Xu"
      ],
      "abstract": "Large language models (LLMs) have been increasingly adopted to support patients' healthcare-seeking in recent years. While prior patient-centered studies have examined the capabilities and experience of LLM-based tools in specific health-related tasks such as information-seeking, diagnosis, or decision-supporting, the inherently longitudinal nature of healthcare in real-world practice has been underexplored. This paper presents a four-week diary study with 25 patients to examine LLMs' roles across healthcare-seeking trajectories. Our analysis reveals that patients integrate LLMs not just as simple decision-support tools, but as dynamic companions that scaffold their journey across behavioral, informational, emotional, and cognitive levels. Meanwhile, patients actively assign diverse socio-technical meanings to LLMs, altering the traditional dynamics of agency, trust, and power in patient-provider relationships. Drawing from these findings, we conceptualize future LLMs as a longitudinal boundary companion that continuously mediates between patients and clinicians throughout longitudinal healthcare-seeking trajectories.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "pdf_url": "https://arxiv.org/pdf/2602.14733v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.07905v1",
      "title": "MedCoG: Maximizing LLM Inference Density in Medical Reasoning via Meta-Cognitive Regulation",
      "authors": [
        "Yu Zhao",
        "Hao Guan",
        "Yongcheng Jing",
        "Ying Zhang",
        "Dacheng Tao"
      ],
      "abstract": "Large Language Models (LLMs) have shown strong potential in complex medical reasoning yet face diminishing gains under inference scaling laws. While existing studies augment LLMs with various knowledge types, it remains unclear how effectively the additional costs translate into accuracy. In this paper, we explore how meta-cognition of LLMs, i.e., their self-awareness of their own knowledge states, can regulate the reasoning process. Specifically, we propose MedCoG, a Medical Meta-Cognition Agent with Knowledge Graph, where the meta-cognitive assessments of task complexity, familiarity, and knowledge density dynamically regulate utilization of procedural, episodic, and factual knowledge. The LLM-centric on-demand reasoning aims to mitigate scaling laws by (1) reducing costs via avoiding indiscriminate scaling, (2) improving accuracy via filtering out distractive knowledge. To validate this, we empirically characterize the scaling curve and introduce inference density to quantify inference efficiency, defined as the ratio of theoretically effective cost to actual cost. Experiments demonstrate the effectiveness and efficiency of MedCoG on five hard sets of medical benchmarks, yielding 5.5x inference density. Furthermore, the Oracle study highlights the significant potential of meta-cognitive regulation.",
      "published": "2026-02-08",
      "updated": "2026-02-08",
      "pdf_url": "https://arxiv.org/pdf/2602.07905v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2602.07529v2",
      "title": "MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution",
      "authors": [
        "Jianwen Chen",
        "Xinyu Yang",
        "Peng Xia",
        "Arian Azarang",
        "Yueh Z Lee",
        "Gang Li",
        "Hongtu Zhu",
        "Yun Li",
        "Beidi Chen",
        "Huaxiu Yao"
      ],
      "abstract": "Large language models (LLMs) have demonstrated strong performance and rapid progress in a wide range of medical reasoning tasks. However, their sequential autoregressive decoding forces inherently parallel clinical reasoning, such as differential diagnosis, into a single linear reasoning path, limiting both efficiency and reliability for complex medical problems. To address this, we propose MedVerse, a reasoning framework for complex medical inference that reformulates medical reasoning as a parallelizable directed acyclic graph (DAG) process based on Petri net theory. The framework adopts a full-stack design across data, model architecture, and system execution. For data creation, we introduce the MedVerse Curator, an automated pipeline that synthesizes knowledge-grounded medical reasoning paths and transforms them into Petri net-structured representations. At the architectural level, we propose a topology-aware attention mechanism with adaptive position indices that supports parallel reasoning while preserving logical consistency. Systematically, we develop a customized inference engine that supports parallel execution without additional overhead. Empirical evaluations show that MedVerse improves strong general-purpose LLMs by up to 8.9%. Compared to specialized medical LLMs, MedVerse achieves comparable performance while delivering a 1.3x reduction in inference latency and a 1.7x increase in generation throughput, enabled by its parallel decoding capability. Code is available at https://github.com/aiming-lab/MedVerse.",
      "published": "2026-02-07",
      "updated": "2026-02-10",
      "pdf_url": "https://arxiv.org/pdf/2602.07529v2",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2601.20221v1",
      "title": "Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning",
      "authors": [
        "Hang Zhang",
        "Ruheng Wang",
        "Yuelyu Ji",
        "Mingu Kwak",
        "Xizhi Wu",
        "Chenyu Li",
        "Li Zhang",
        "Wenqi Shi",
        "Yifan Peng",
        "Yanshan Wang"
      ],
      "abstract": "Large language models have achieved strong performance on medical reasoning benchmarks, yet their deployment in clinical settings demands rigorous verification to ensure factual accuracy. While reward models offer a scalable approach for reasoning trace verification, existing methods face two limitations: they produce only scalar reward values without explicit justification, and they rely on single-pass retrieval that precludes adaptive knowledge access as verification unfolds. We introduce $\\method$, an agentic framework that addresses these limitations by training medical reasoning verifiers to iteratively query external medical corpora during evaluation. Our approach combines tool-augmented verification with an iterative reinforcement learning paradigm that requires only trace-level supervision, alongside an adaptive curriculum mechanism that dynamically adjusts training data distribution. Across four medical reasoning benchmarks, $\\method$ achieves substantial gains over existing methods, improving MedQA accuracy by 23.5% and MedXpertQA by 32.0% relative to the base generator in particular. Crucially, $\\method$ demonstrates an $\\mathbf{8\\times}$ reduction in sampling budget requirement compared to prior reward model baselines. These findings establish that grounding verification in dynamically retrieved evidence offers a principled path toward more reliable medical reasoning systems.",
      "published": "2026-01-28",
      "updated": "2026-01-28",
      "pdf_url": "https://arxiv.org/pdf/2601.20221v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2601.13262v1",
      "title": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning",
      "authors": [
        "Eric Onyame",
        "Akash Ghosh",
        "Subhadip Baidya",
        "Sriparna Saha",
        "Xiuying Chen",
        "Chirag Agarwal"
      ],
      "abstract": "While large language models (LLMs) have shown to perform well on monolingual mathematical and commonsense reasoning, they remain unreliable for multilingual medical reasoning applications, hindering their deployment in multilingual healthcare settings. We address this by first introducing CUREMED-BENCH, a high-quality multilingual medical reasoning dataset with open-ended reasoning queries with a single verifiable answer, spanning thirteen languages, including underrepresented languages such as Amharic, Yoruba, and Swahili. Building on this dataset, we propose CURE-MED, a curriculum-informed reinforcement learning framework that integrates code-switching-aware supervised fine-tuning and Group Relative Policy Optimization to jointly improve logical correctness and language stability. Across thirteen languages, our approach consistently outperforms strong baselines and scales effectively, achieving 85.21% language consistency and 54.35% logical correctness at 7B parameters, and 94.96% language consistency and 70.04% logical correctness at 32B parameters. These results support reliable and equitable multilingual medical reasoning in LLMs. The code and dataset are available at https://cure-med.github.io/",
      "published": "2026-01-19",
      "updated": "2026-01-19",
      "pdf_url": "https://arxiv.org/pdf/2601.13262v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2601.08267v2",
      "title": "Med-CoReasoner: Reducing Language Disparities in Medical Reasoning via Language-Informed Co-Reasoning",
      "authors": [
        "Fan Gao",
        "Sherry T. Tong",
        "Jiwoong Sohn",
        "Jiahao Huang",
        "Junfeng Jiang",
        "Ding Xia",
        "Piyalitt Ittichaiwong",
        "Kanyakorn Veerakanjana",
        "Hyunjae Kim",
        "Qingyu Chen",
        "Edison Marrese Taylor",
        "Kazuma Kobayashi",
        "Akkiko Aizawa",
        "Irene Li"
      ],
      "abstract": "While reasoning-enhanced large language models perform strongly on English medical tasks, a persistent multilingual gap remains, with substantially weaker reasoning in local languages, limiting equitable global medical deployment. To bridge this gap, we introduce Med-CoReasoner, a language-informed co-reasoning framework that elicits parallel English and local-language reasoning, abstracts them into structured concepts, and integrates local clinical knowledge into an English logical scaffold via concept-level alignment and retrieval. This design combines the structural robustness of English reasoning with the practice-grounded expertise encoded in local languages. To evaluate multilingual medical reasoning beyond multiple-choice settings, we construct MultiMed-X, a benchmark covering seven languages with expert-annotated long-form question answering and natural language inference tasks, comprising 350 instances per language. Experiments across three benchmarks show that Med-CoReasoner improves multilingual reasoning performance by an average of 5%, with particularly substantial gains in low-resource languages. Moreover, model distillation and expert evaluation analysis further confirm that Med-CoReasoner produces clinically sound and culturally grounded reasoning traces.",
      "published": "2026-01-13",
      "updated": "2026-01-19",
      "pdf_url": "https://arxiv.org/pdf/2601.08267v2",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2512.13510v1",
      "title": "MedCEG: Reinforcing Verifiable Medical Reasoning with Critical Evidence Graph",
      "authors": [
        "Linjie Mu",
        "Yannian Gu",
        "Zhongzhen Huang",
        "Yakun Zhu",
        "Shaoting Zhang",
        "Xiaofan Zhang"
      ],
      "abstract": "Large language models with reasoning capabilities have demonstrated impressive performance across a wide range of domains. In clinical applications, a transparent, step-by-step reasoning process provides physicians with strong evidence to support decision-making. While reinforcement learning has effectively enhanced reasoning performance in medical contexts, the clinical reliability of these reasoning processes remains limited because their accuracy and validity are often overlooked during training. To address this gap, we propose MedCEG, a framework that augments medical language models with clinically valid reasoning pathways by explicitly supervising the reasoning process through a Critical Evidence Graph (CEG). We curate a dataset of challenging clinical cases and algorithmically construct a CEG for each sample to represent a high-quality verifiable reasoning pathway. To guide the reasoning process, we introduce a Clinical Reasoning Procedure Reward, which evaluates Node Coverage, Structural Correctness, and Chain Completeness, thereby providing a holistic assessment of reasoning quality. Experimental results show that MedCEG surpasses existing methods in performance while producing clinically valid reasoning chains, representing a solid advancement in reliable medical AI reasoning. The code and models are available at https://github.com/LinjieMu/MedCEG.",
      "published": "2025-12-15",
      "updated": "2025-12-15",
      "pdf_url": "https://arxiv.org/pdf/2512.13510v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2512.13742v1",
      "title": "DL$^3$M: A Vision-to-Language Framework for Expert-Level Medical Reasoning through Deep Learning and Large Language Models",
      "authors": [
        "Md. Najib Hasan",
        "Imran Ahmad",
        "Sourav Basak Shuvo",
        "Md. Mahadi Hasan Ankon",
        "Sunanda Das",
        "Nazmul Siddique",
        "Hui Wang"
      ],
      "abstract": "Medical image classifiers detect gastrointestinal diseases well, but they do not explain their decisions. Large language models can generate clinical text, yet they struggle with visual reasoning and often produce unstable or incorrect explanations. This leaves a gap between what a model sees and the type of reasoning a clinician expects. We introduce a framework that links image classification with structured clinical reasoning. A new hybrid model, MobileCoAtNet, is designed for endoscopic images and achieves high accuracy across eight stomach-related classes. Its outputs are then used to drive reasoning by several LLMs. To judge this reasoning, we build two expert-verified benchmarks covering causes, symptoms, treatment, lifestyle, and follow-up care. Thirty-two LLMs are evaluated against these gold standards. Strong classification improves the quality of their explanations, but none of the models reach human-level stability. Even the best LLMs change their reasoning when prompts vary. Our study shows that combining DL with LLMs can produce useful clinical narratives, but current LLMs remain unreliable for high-stakes medical decisions. The framework provides a clearer view of their limits and a path for building safer reasoning systems. The complete source code and datasets used in this study are available at https://github.com/souravbasakshuvo/DL3M.",
      "published": "2025-12-14",
      "updated": "2025-12-14",
      "pdf_url": "https://arxiv.org/pdf/2512.13742v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2512.05658v1",
      "title": "Grounded Multilingual Medical Reasoning for Question Answering with Large Language Models",
      "authors": [
        "Pietro Ferrazzi",
        "Aitor Soroa",
        "Rodrigo Agerri"
      ],
      "abstract": "Large Language Models (LLMs) with reasoning capabilities have recently demonstrated strong potential in medical Question Answering (QA). Existing approaches are largely English-focused and primarily rely on distillation from general-purpose LLMs, raising concerns about the reliability of their medical knowledge. In this work, we present a method to generate multilingual reasoning traces grounded in factual medical knowledge. We produce 500k traces in English, Italian, and Spanish, using a retrievalaugmented generation approach over medical information from Wikipedia. The traces are generated to solve medical questions drawn from MedQA and MedMCQA, which we extend to Italian and Spanish. We test our pipeline in both in-domain and outof-domain settings across Medical QA benchmarks, and demonstrate that our reasoning traces improve performance both when utilized via in-context learning (few-shot) and supervised fine-tuning, yielding state-of-the-art results among 8B-parameter LLMs. We believe that these resources can support the development of safer, more transparent clinical decision-support tools in multilingual settings. We release the full suite of resources: reasoning traces, translated QA datasets, Medical-Wikipedia, and fine-tuned models.",
      "published": "2025-12-05",
      "updated": "2025-12-05",
      "pdf_url": "https://arxiv.org/pdf/2512.05658v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2511.23269v1",
      "title": "OctoMed: Data Recipes for State-of-the-Art Multimodal Medical Reasoning",
      "authors": [
        "Timothy Ossowski",
        "Sheng Zhang",
        "Qianchu Liu",
        "Guanghui Qin",
        "Reuben Tan",
        "Tristan Naumann",
        "Junjie Hu",
        "Hoifung Poon"
      ],
      "abstract": "High-quality and carefully curated data is a cornerstone of training medical large language models, as it directly impacts both generalization and robustness to unseen clinical tasks. We investigate strategies for training and data curation to develop a robust multimodal reasoning model in the medical domain. Our work focuses on supervised fine-tuning (SFT) and explores data recipes that leverage structured reasoning traces. Using our proposed data recipe, we scale experiments to a dataset of over 8 million examples and 6.8 billion response tokens, achieving state-of-the-art performance among open-source models across diverse out-of-distribution medical benchmark tasks. Our results further indicate that curating a high-quality, diverse training dataset with varying structured reasoning trace lengths enables the fine-tuned model to self-calibrate its reasoning trajectory lengths based on the downstream task, without explicit supervision. We present key insights, describe the data curation strategy, and outline next steps toward developing robust medical vision-language reasoning system.",
      "published": "2025-11-28",
      "updated": "2025-11-28",
      "pdf_url": "https://arxiv.org/pdf/2511.23269v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2511.00421v1",
      "title": "MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts",
      "authors": [
        "Naoto Iwase",
        "Hiroki Okuyama",
        "Junichiro Iwasawa"
      ],
      "abstract": "Large language models (LLMs) show increasing promise in medical applications, but their ability to detect and correct errors in clinical texts -- a prerequisite for safe deployment -- remains under-evaluated, particularly beyond English. We introduce MedRECT, a cross-lingual benchmark (Japanese/English) that formulates medical error handling as three subtasks: error detection, error localization (sentence extraction), and error correction. MedRECT is built with a scalable, automated pipeline from the Japanese Medical Licensing Examinations (JMLE) and a curated English counterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with comparable error/no-error balance. We evaluate 9 contemporary LLMs spanning proprietary, open-weight, and reasoning families. Key findings: (i) reasoning models substantially outperform standard architectures, with up to 13.5% relative improvement in error detection and 51.0% in sentence extraction; (ii) cross-lingual evaluation reveals 5-10% performance gaps from English to Japanese, with smaller disparities for reasoning models; (iii) targeted LoRA fine-tuning yields asymmetric improvements in error correction performance (Japanese: +0.078, English: +0.168) while preserving reasoning capabilities; and (iv) our fine-tuned model exceeds human expert performance on structured medical error correction tasks. To our knowledge, MedRECT is the first comprehensive cross-lingual benchmark for medical error correction, providing a reproducible framework and resources for developing safer medical LLMs across languages.",
      "published": "2025-11-01",
      "updated": "2025-11-01",
      "pdf_url": "https://arxiv.org/pdf/2511.00421v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2510.03536v2",
      "title": "Triplet-Structured Knowledge Integration for Multi-Turn Medical Reasoning",
      "authors": [
        "Zhaohan Meng",
        "Zaiqiao Meng",
        "Siwei Liu",
        "Iadh Ounis"
      ],
      "abstract": "Large Language Models (LLMs) have shown strong performance on static medical Question Answering (QA) tasks, yet their reasoning often deteriorates in multi-turn clinical dialogues where patient information is scattered across turns. This paper introduces TriMediQ, a triplet-structured approach that enhances the reasoning reliability of LLMs through explicit knowledge integration. TriMediQ first employs a frozen triplet extraction LLM to convert patient responses into clinically grounded triplets, ensuring factual precision via constrained prompting. These triplets are incorporated into a patient-specific Knowledge Graph (KG), from which a trainable projection module consisting of a graph encoder and a projector captures relational dependencies while keeping all LLM parameters frozen. During inference, the projection module guides multi-hop reasoning over the KG, enabling coherent clinical dialogue understanding. Experiments on two interactive medical QA benchmarks show that TriMediQ achieves up to 10.4\\% improvement in accuracy over five existing baselines on the iMedQA dataset. These results demonstrate that structuring patient information as triplets can effectively improve the reasoning capability of LLMs in multi-turn medical QA.",
      "published": "2025-10-03",
      "updated": "2025-10-14",
      "pdf_url": "https://arxiv.org/pdf/2510.03536v2",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2509.23725v2",
      "title": "MedLA: A Logic-Driven Multi-Agent Framework for Complex Medical Reasoning with Large Language Models",
      "authors": [
        "Siqi Ma",
        "Jiajie Huang",
        "Fan Zhang",
        "Jinlin Wu",
        "Yue Shen",
        "Guohui Fan",
        "Zhu Zhang",
        "Zelin Zang"
      ],
      "abstract": "Answering complex medical questions requires not only domain expertise and patient-specific information, but also structured and multi-perspective reasoning. Existing multi-agent approaches often rely on fixed roles or shallow interaction prompts, limiting their ability to detect and resolve fine-grained logical inconsistencies. To address this, we propose \\textsc{MedLA}, a logic-driven multi-agent framework built on large language models. Each agent organizes its reasoning process into an explicit logical tree based on syllogistic triads (major premise, minor premise, and conclusion), enabling transparent inference and premise-level alignment. Agents engage in a multi-round, graph-guided discussion to compare and iteratively refine their logic trees, achieving consensus through error correction and contradiction resolution. We demonstrate that \\textsc{MedLA} consistently outperforms both static role-based systems and single-agent baselines on challenging benchmarks such as MedDDx and standard medical QA tasks. Furthermore, \\textsc{MedLA} scales effectively across both open-source and commercial LLM backbones, achieving state-of-the-art performance and offering a generalizable paradigm for trustworthy medical reasoning.",
      "published": "2025-09-28",
      "updated": "2025-11-19",
      "pdf_url": "https://arxiv.org/pdf/2509.23725v2",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2509.23368v1",
      "title": "MedCritical: Enhancing Medical Reasoning in Small Language Models via Self-Collaborative Correction",
      "authors": [
        "Xinchun Su",
        "Chunxu Luo",
        "Yixuan Li",
        "Weidong Yang",
        "Lipeng Ma"
      ],
      "abstract": "In the field of medicine, complex reasoning tasks such as clinical diagnosis, treatment planning, and medical knowledge integration pose significant challenges, where small language models often underperform compared to large language models like GPT-4 and Deepseek. Recent knowledge distillation-based methods aim to address these issues through teacher-guided error correction, but this LLM as judge approach remains challenging in terms of cost, time, and efficiency. To circumvent this issue, we propose a novel two-stage framework, MedCritical, which uses a small language model fine-tuned by a large teacher model to play against itself. In the first stage, we extract high-level and detailed long-chain thought templates from the teacher model to guide the student model to generate more complex reasoning thoughts. In the second stage, we introduce direct preference optimization (DPO) through model self-iteration collaboration to enhance the reasoning ability of the student model by playing against the correction trajectory of the fine-tuned model during training. This model self-learning DPO approach teaches the student model to use its own error-driven insights to consolidate its skills and knowledge to solve complex problems, and achieves comparable results to traditional knowledge distillation methods using teacher models at a lower cost. Notably, our MedCritical 7B model outperforms the Taiyi and Huatuo-o1-7B models by 3.04\\% and 10.12\\% respectively on the CMExam benchmark, achieving new SOTA performance among 7B-class small models.",
      "published": "2025-09-27",
      "updated": "2025-09-27",
      "pdf_url": "https://arxiv.org/pdf/2509.23368v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2509.22713v1",
      "title": "RAR$^2$: Retrieval-Augmented Medical Reasoning via Thought-Driven Retrieval",
      "authors": [
        "Kaishuai Xu",
        "Wenjun Hou",
        "Yi Cheng",
        "Wenjie Li"
      ],
      "abstract": "Large Language Models (LLMs) have shown promising performance on diverse medical benchmarks, highlighting their potential in supporting real-world clinical tasks. Retrieval-Augmented Generation (RAG) has emerged as a key approach for mitigating knowledge gaps and hallucinations by incorporating external medical information. However, RAG still struggles with complex medical questions that require intensive reasoning, as surface-level input often fails to reflect the true knowledge needs of the task. Existing methods typically focus on refining queries without explicitly modeling the reasoning process, limiting their ability to retrieve and integrate clinically relevant knowledge. In this work, we propose RAR$^2$, a joint learning framework that improves both Reasoning-Augmented Retrieval and Retrieval-Augmented Reasoning. RAR$^2$ constructs a thought process to uncover implicit knowledge requirements and uses it to guide retrieval and answer generation. We build a training dataset of mixed preference pairs and apply Direct Preference Optimization (DPO) to train the model. Moreover, we design two test-time scaling strategies to explore the boundaries of our framework. Experiments demonstrate the effectiveness of RAR$^2$ across several biomedical question answering datasets, outperforming RAG baselines with or without fine-tuning.",
      "published": "2025-09-24",
      "updated": "2025-09-24",
      "pdf_url": "https://arxiv.org/pdf/2509.22713v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2509.15279v1",
      "title": "Fleming-R1: Toward Expert-Level Medical Reasoning via Reinforcement Learning",
      "authors": [
        "Chi Liu",
        "Derek Li",
        "Yan Shu",
        "Robin Chen",
        "Derek Duan",
        "Teng Fang",
        "Bryan Dai"
      ],
      "abstract": "While large language models show promise in medical applications, achieving expert-level clinical reasoning remains challenging due to the need for both accurate answers and transparent reasoning processes. To address this challenge, we introduce Fleming-R1, a model designed for verifiable medical reasoning through three complementary innovations. First, our Reasoning-Oriented Data Strategy (RODS) combines curated medical QA datasets with knowledge-graph-guided synthesis to improve coverage of underrepresented diseases, drugs, and multi-hop reasoning chains. Second, we employ Chain-of-Thought (CoT) cold start to distill high-quality reasoning trajectories from teacher models, establishing robust inference priors. Third, we implement a two-stage Reinforcement Learning from Verifiable Rewards (RLVR) framework using Group Relative Policy Optimization, which consolidates core reasoning skills while targeting persistent failure modes through adaptive hard-sample mining. Across diverse medical benchmarks, Fleming-R1 delivers substantial parameter-efficient improvements: the 7B variant surpasses much larger baselines, while the 32B model achieves near-parity with GPT-4o and consistently outperforms strong open-source alternatives. These results demonstrate that structured data design, reasoning-oriented initialization, and verifiable reinforcement learning can advance clinical reasoning beyond simple accuracy optimization. We release Fleming-R1 publicly to promote transparent, reproducible, and auditable progress in medical AI, enabling safer deployment in high-stakes clinical environments.",
      "published": "2025-09-18",
      "updated": "2025-09-18",
      "pdf_url": "https://arxiv.org/pdf/2509.15279v1",
      "keyword": "Medical reasoning LLM"
    }
  ]
}