{
  "last_updated": "2026-02-20 01:47 UTC",
  "keywords": [
    "Medical reasoning LLM",
    "Large language model",
    "Agentic AI"
  ],
  "papers": [
    {
      "id": "http://arxiv.org/abs/2602.16639v1",
      "title": "AREG: Adversarial Resource Extraction Game for Evaluating Persuasion and Resistance in Large Language Models",
      "authors": [
        "Adib Sakhawat",
        "Fardeen Sadab"
      ],
      "abstract": "Evaluating the social intelligence of Large Language Models (LLMs) increasingly requires moving beyond static text generation toward dynamic, adversarial interaction. We introduce the Adversarial Resource Extraction Game (AREG), a benchmark that operationalizes persuasion and resistance as a multi-turn, zero-sum negotiation over financial resources. Using a round-robin tournament across frontier models, AREG enables joint evaluation of offensive (persuasion) and defensive (resistance) capabilities within a single interactional framework. Our analysis provides evidence that these capabilities are weakly correlated ($ρ= 0.33$) and empirically dissociated: strong persuasive performance does not reliably predict strong resistance, and vice versa. Across all evaluated models, resistance scores exceed persuasion scores, indicating a systematic defensive advantage in adversarial dialogue settings. Further linguistic analysis suggests that interaction structure plays a central role in these outcomes. Incremental commitment-seeking strategies are associated with higher extraction success, while verification-seeking responses are more prevalent in successful defenses than explicit refusal. Together, these findings indicate that social influence in LLMs is not a monolithic capability and that evaluation frameworks focusing on persuasion alone may overlook asymmetric behavioral vulnerabilities.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "pdf_url": "https://arxiv.org/pdf/2602.16639v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.16551v1",
      "title": "Automated Extraction of Mechanical Constitutive Models from Scientific Literature using Large Language Models: Applications in Cultural Heritage Conservation",
      "authors": [
        "Rui Hu",
        "Yue Wu",
        "Tianhao Su",
        "Yin Wang",
        "Shunbo Hu",
        "Jizhong Huang"
      ],
      "abstract": "The preservation of cultural heritage is increasingly transitioning towards data-driven predictive maintenance and \"Digital Twin\" construction. However, the mechanical constitutive models required for high-fidelity simulations remain fragmented across decades of unstructured scientific literature, creating a \"Data Silo\" that hinders conservation engineering. To address this, we present an automated, two-stage agentic framework leveraging Large Language Models (LLMs) to extract mechanical constitutive equations, calibrated parameters, and metadata from PDF documents. The workflow employs a resource-efficient \"Gatekeeper\" agent for relevance filtering and a high-capability \"Analyst\" agent for fine-grained extraction, featuring a novel Context-Aware Symbolic Grounding mechanism to resolve mathematical ambiguities. Applied to a corpus of over 2,000 research papers, the system successfully isolated 113 core documents and constructed a structured database containing 185 constitutive model instances and over 450 calibrated parameters. The extraction precision reached 80.4\\%, establishing a highly efficient \"Human-in-the-loop\" workflow that reduces manual data curation time by approximately 90\\%. We demonstrate the system's utility through a web-based Knowledge Retrieval Platform, which enables rapid parameter discovery for computational modeling. This work transforms scattered literature into a queryable digital asset, laying the data foundation for the \"Digital Material Twin\" of built heritage.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "pdf_url": "https://arxiv.org/pdf/2602.16551v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.16481v1",
      "title": "Leveraging Large Language Models for Causal Discovery: a Constraint-based, Argumentation-driven Approach",
      "authors": [
        "Zihao Li",
        "Fabrizio Russo"
      ],
      "abstract": "Causal discovery seeks to uncover causal relations from data, typically represented as causal graphs, and is essential for predicting the effects of interventions. While expert knowledge is required to construct principled causal graphs, many statistical methods have been proposed to leverage observational data with varying formal guarantees. Causal Assumption-based Argumentation (ABA) is a framework that uses symbolic reasoning to ensure correspondence between input constraints and output graphs, while offering a principled way to combine data and expertise. We explore the use of large language models (LLMs) as imperfect experts for Causal ABA, eliciting semantic structural priors from variable names and descriptions and integrating them with conditional-independence evidence. Experiments on standard benchmarks and semantically grounded synthetic graphs demonstrate state-of-the-art performance, and we additionally introduce an evaluation protocol to mitigate memorisation bias when assessing LLMs for causal discovery.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "pdf_url": "https://arxiv.org/pdf/2602.16481v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.16467v1",
      "title": "IndicEval: A Bilingual Indian Educational Evaluation Framework for Large Language Models",
      "authors": [
        "Saurabh Bharti",
        "Gaurav Azad",
        "Abhinaw Jagtap",
        "Nachiket Tapas"
      ],
      "abstract": "The rapid advancement of large language models (LLMs) necessitates evaluation frameworks that reflect real-world academic rigor and multilingual complexity. This paper introduces IndicEval, a scalable benchmarking platform designed to assess LLM performance using authentic high-stakes examination questions from UPSC, JEE, and NEET across STEM and humanities domains in both English and Hindi. Unlike synthetic benchmarks, IndicEval grounds evaluation in real examination standards, enabling realistic measurement of reasoning, domain knowledge, and bilingual adaptability. The framework automates assessment using Zero-Shot, Few-Shot, and Chain-of-Thought (CoT) prompting strategies and supports modular integration of new models and languages. Experiments conducted on Gemini 2.0 Flash, GPT-4, Claude, and LLaMA 3-70B reveal three major findings. First, CoT prompting consistently improves reasoning accuracy, with substantial gains across subjects and languages. Second, significant cross-model performance disparities persist, particularly in high-complexity examinations. Third, multilingual degradation remains a critical challenge, with marked accuracy drops in Hindi compared to English, especially under Zero-Shot conditions. These results highlight persistent gaps in bilingual reasoning and domain transfer. Overall, IndicEval provides a practice-oriented, extensible foundation for rigorous, equitable evaluation of LLMs in multilingual educational settings and offers actionable insights for improving reasoning robustness and language adaptability.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "pdf_url": "https://arxiv.org/pdf/2602.16467v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.16412v1",
      "title": "ReMoRa: Multimodal Large Language Model based on Refined Motion Representation for Long-Video Understanding",
      "authors": [
        "Daichi Yashima",
        "Shuhei Kurita",
        "Yusuke Oda",
        "Komei Sugiura"
      ],
      "abstract": "While multimodal large language models (MLLMs) have shown remarkable success across a wide range of tasks, long-form video understanding remains a significant challenge. In this study, we focus on video understanding by MLLMs. This task is challenging because processing a full stream of RGB frames is computationally intractable and highly redundant, as self-attention have quadratic complexity with sequence length. In this paper, we propose ReMoRa, a video MLLM that processes videos by operating directly on their compressed representations. A sparse set of RGB keyframes is retained for appearance, while temporal dynamics are encoded as a motion representation, removing the need for sequential RGB frames. These motion representations act as a compact proxy for optical flow, capturing temporal dynamics without full frame decoding. To refine the noise and low fidelity of block-based motions, we introduce a module to denoise and generate a fine-grained motion representation. Furthermore, our model compresses these features in a way that scales linearly with sequence length. We demonstrate the effectiveness of ReMoRa through extensive experiments across a comprehensive suite of long-video understanding benchmarks. ReMoRa outperformed baseline methods on multiple challenging benchmarks, including LongVideoBench, NExT-QA, and MLVU.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "pdf_url": "https://arxiv.org/pdf/2602.16412v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.16201v1",
      "title": "Long-Tail Knowledge in Large Language Models: Taxonomy, Mechanisms, Interventions and Implications",
      "authors": [
        "Sanket Badhe",
        "Deep Shah",
        "Nehal Kathrotia"
      ],
      "abstract": "Large language models (LLMs) are trained on web-scale corpora that exhibit steep power-law distributions, in which the distribution of knowledge is highly long-tailed, with most appearing infrequently. While scaling has improved average-case performance, persistent failures on low-frequency, domain-specific, cultural, and temporal knowledge remain poorly characterized. This paper develops a structured taxonomy and analysis of long-Tail Knowledge in large language models, synthesizing prior work across technical and sociotechnical perspectives.   We introduce a structured analytical framework that synthesizes prior work across four complementary axes: how long-Tail Knowledge is defined, the mechanisms by which it is lost or distorted during training and inference, the technical interventions proposed to mitigate these failures, and the implications of these failures for fairness, accountability, transparency, and user trust. We further examine how existing evaluation practices obscure tail behavior and complicate accountability for rare but consequential failures. The paper concludes by identifying open challenges related to privacy, sustainability, and governance that constrain long-Tail Knowledge representation. Taken together, this paper provides a unifying conceptual framework for understanding how long-Tail Knowledge is defined, lost, evaluated, and manifested in deployed language model systems.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "pdf_url": "https://arxiv.org/pdf/2602.16201v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.16165v1",
      "title": "HiPER: Hierarchical Reinforcement Learning with Explicit Credit Assignment for Large Language Model Agents",
      "authors": [
        "Jiangweizhi Peng",
        "Yuanxin Liu",
        "Ruida Zhou",
        "Charles Fleming",
        "Zhaoran Wang",
        "Alfredo Garcia",
        "Mingyi Hong"
      ],
      "abstract": "Training LLMs as interactive agents for multi-turn decision-making remains challenging, particularly in long-horizon tasks with sparse and delayed rewards, where agents must execute extended sequences of actions before receiving meaningful feedback. Most existing reinforcement learning (RL) approaches model LLM agents as flat policies operating at a single time scale, selecting one action at each turn. In sparse-reward settings, such flat policies must propagate credit across the entire trajectory without explicit temporal abstraction, which often leads to unstable optimization and inefficient credit assignment.   We propose HiPER, a novel Hierarchical Plan-Execute RL framework that explicitly separates high-level planning from low-level execution. HiPER factorizes the policy into a high-level planner that proposes subgoals and a low-level executor that carries them out over multiple action steps. To align optimization with this structure, we introduce a key technique called hierarchical advantage estimation (HAE), which carefully assigns credit at both the planning and execution levels. By aggregating returns over the execution of each subgoal and coordinating updates across the two levels, HAE provides an unbiased gradient estimator and provably reduces variance compared to flat generalized advantage estimation.   Empirically, HiPER achieves state-of-the-art performance on challenging interactive benchmarks, reaching 97.4\\% success on ALFWorld and 83.3\\% on WebShop with Qwen2.5-7B-Instruct (+6.6\\% and +8.3\\% over the best prior method), with especially large gains on long-horizon tasks requiring multiple dependent subtasks. These results highlight the importance of explicit hierarchical decomposition for scalable RL training of multi-turn LLM agents.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "pdf_url": "https://arxiv.org/pdf/2602.16165v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.16140v1",
      "title": "Human-AI Collaboration in Large Language Model-Integrated Building Energy Management Systems: The Role of User Domain Knowledge and AI Literacy",
      "authors": [
        "Wooyoung Jung",
        "Kahyun Jeon",
        "Prosper Babon-Ayeng"
      ],
      "abstract": "This study aimed to comprehend how user domain knowledge and artificial intelligence (AI) literacy impact the effective use of human-AI interactive building energy management system (BEMS). While prior studies have investigated the potential of integrating large language models (LLMs) into BEMS or building energy modeling, very few studies have examined how user interact with such systems. We conducted a systematic role-playing experiment, where 85 human subjects interacted with an advanced generative pre-trained transformer (OpenAI GPT-4o). Participants were tasked with identifying the top five behavioral changes that could reduce home energy use with the GPT model that functioned as an LLM-integrated BEMS. Then, the collected prompt-response data and participant conclusions were analyzed using an analytical framework that hierarchically assessed and scored human-AI interactions and their home energy analysis approaches. Also, participants were classified into four groups based on their self-evaluated domain knowledge of building energy use and AI literacy, and Kruskal-Wallis H tests with post-hoc pairwise comparisons were conducted across 20 quantifiable metrics. Key takeaways include: most participants employed concise prompts (median: 16.2 words) and relied heavily on GPT's analytical capabilities; and notably, only 1 of 20 metrics, appliance identification rate, showed statistically significant group differences (p=0.037), driven by AI literacy rather than domain knowledge, suggesting an equalizing effect of LLMs across expertise levels. This study provides foundational insights into human-AI collaboration dynamics and promising development directions in the context of LLM-integrated BEMS and contributes to realizing human-centric LLM-integrated energy systems.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "pdf_url": "https://arxiv.org/pdf/2602.16140v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.16105v1",
      "title": "GPSBench: Do Large Language Models Understand GPS Coordinates?",
      "authors": [
        "Thinh Hung Truong",
        "Jey Han Lau",
        "Jianzhong Qi"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed in applications that interact with the physical world, such as navigation, robotics, or mapping, making robust geospatial reasoning a critical capability. Despite that, LLMs' ability to reason about GPS coordinates and real-world geography remains underexplored. We introduce GPSBench, a dataset of 57,800 samples across 17 tasks for evaluating geospatial reasoning in LLMs, spanning geometric coordinate operations (e.g., distance and bearing computation) and reasoning that integrates coordinates with world knowledge. Focusing on intrinsic model capabilities rather than tool use, we evaluate 14 state-of-the-art LLMs and find that GPS reasoning remains challenging, with substantial variation across tasks: models are generally more reliable at real-world geographic reasoning than at geometric computations. Geographic knowledge degrades hierarchically, with strong country-level performance but weak city-level localization, while robustness to coordinate noise suggests genuine coordinate understanding rather than memorization. We further show that GPS-coordinate augmentation can improve in downstream geospatial tasks, and that finetuning induces trade-offs between gains in geometric computation and degradation in world knowledge. Our dataset and reproducible code are available at https://github.com/joey234/gpsbench",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "pdf_url": "https://arxiv.org/pdf/2602.16105v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.16708v1",
      "title": "Policy Compiler for Secure Agentic Systems",
      "authors": [
        "Nils Palumbo",
        "Sarthak Choudhary",
        "Jihye Choi",
        "Prasad Chalasani",
        "Mihai Christodorescu",
        "Somesh Jha"
      ],
      "abstract": "LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement.   Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture. Instead, PCAS models the agentic system state as a dependency graph capturing causal relationships among events such as tool calls, tool results, and messages. Policies are expressed in a Datalog-derived language, as declarative rules that account for transitive information flow and cross-agent provenance. A reference monitor intercepts all actions and blocks violations before execution, providing deterministic enforcement independent of model reasoning.   PCAS takes an existing agent implementation and a policy specification, and compiles them into an instrumented system that is policy-compliant by construction, with no security-specific restructuring required. We evaluate PCAS on three case studies: information flow policies for prompt injection defense, approval workflows in a multi-agent pharmacovigilance system, and organizational policies for customer service. On customer service tasks, PCAS improves policy compliance from 48% to 93% across frontier models, with zero policy violations in instrumented runs.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "pdf_url": "https://arxiv.org/pdf/2602.16708v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.16699v1",
      "title": "Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents",
      "authors": [
        "Wenxuan Ding",
        "Nicholas Tomlin",
        "Greg Durrett"
      ],
      "abstract": "LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "pdf_url": "https://arxiv.org/pdf/2602.16699v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.16666v1",
      "title": "Towards a Science of AI Agent Reliability",
      "authors": [
        "Stephan Rabanser",
        "Sayash Kapoor",
        "Peter Kirgis",
        "Kangheng Liu",
        "Saiteja Utpala",
        "Arvind Narayanan"
      ],
      "abstract": "AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "pdf_url": "https://arxiv.org/pdf/2602.16666v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.16662v1",
      "title": "Evaluating Collective Behaviour of Hundreds of LLM Agents",
      "authors": [
        "Richard Willis",
        "Jianing Zhao",
        "Yali Du",
        "Joel Z. Leibo"
      ],
      "abstract": "As autonomous agents powered by LLM are increasingly deployed in society, understanding their collective behaviour in social dilemmas becomes critical. We introduce an evaluation framework where LLMs generate strategies encoded as algorithms, enabling inspection prior to deployment and scaling to populations of hundreds of agents -- substantially larger than in previous work. We find that more recent models tend to produce worse societal outcomes compared to older models when agents prioritise individual gain over collective benefits. Using cultural evolution to model user selection of agents, our simulations reveal a significant risk of convergence to poor societal equilibria, particularly when the relative benefit of cooperation diminishes and population sizes increase. We release our code as an evaluation suite for developers to assess the emergent collective behaviour of their models.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "pdf_url": "https://arxiv.org/pdf/2602.16662v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.16653v1",
      "title": "Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments",
      "authors": [
        "Yangjie Xu",
        "Lujun Li",
        "Lama Sleem",
        "Niccolo Gentile",
        "Yewei Song",
        "Yiqun Wang",
        "Siming Ji",
        "Wenbo Wu",
        "Radu State"
      ],
      "abstract": "Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigation is conducted to determine whether the Agent Skill paradigm provides similar benefits to small language models (SLMs). This question matters in industrial scenarios where continuous reliance on public APIs is infeasible due to data-security and budget constraints requirements, and where SLMs often show limited generalization in highly customized scenarios. This work introduces a formal mathematical definition of the Agent Skill process, followed by a systematic evaluation of language models of varying sizes across multiple use cases. The evaluation encompasses two open-source tasks and a real-world insurance claims data set. The results show that tiny models struggle with reliable skill selection, while moderately sized SLMs (approximately 12B - 30B) parameters) benefit substantially from the Agent Skill approach. Moreover, code-specialized variants at around 80B parameters achieve performance comparable to closed-source baselines while improving GPU efficiency. Collectively, these findings provide a comprehensive and nuanced characterization of the capabilities and constraints of the framework, while providing actionable insights for the effective deployment of Agent Skills in SLM-centered environments.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "pdf_url": "https://arxiv.org/pdf/2602.16653v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.16585v1",
      "title": "DataJoint 2.0: A Computational Substrate for Agentic Scientific Workflows",
      "authors": [
        "Dimitri Yatsenko",
        "Thinh T. Nguyen"
      ],
      "abstract": "Operational rigor determines whether human-agent collaboration succeeds or fails. Scientific data pipelines need the equivalent of DevOps -- SciOps -- yet common approaches fragment provenance across disconnected systems without transactional guarantees. DataJoint 2.0 addresses this gap through the relational workflow model: tables represent workflow steps, rows represent artifacts, foreign keys prescribe execution order. The schema specifies not only what data exists but how it is derived -- a single formal system where data structure, computational dependencies, and integrity constraints are all queryable, enforceable, and machine-readable. Four technical innovations extend this foundation: object-augmented schemas integrating relational metadata with scalable object storage, semantic matching using attribute lineage to prevent erroneous joins, an extensible type system for domain-specific formats, and distributed job coordination designed for composability with external orchestration. By unifying data structure, data, and computational transformations, DataJoint creates a substrate for SciOps where agents can participate in scientific workflows without risking data corruption.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "pdf_url": "https://arxiv.org/pdf/2602.16585v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.16554v1",
      "title": "MerLean: An Agentic Framework for Autoformalization in Quantum Computation",
      "authors": [
        "Yuanjie Ren",
        "Jinzheng Li",
        "Yidi Qi"
      ],
      "abstract": "We introduce MerLean, a fully automated agentic framework for autoformalization in quantum computation. MerLean extracts mathematical statements from \\LaTeX{} source files, formalizes them into verified Lean~4 code built on Mathlib, and translates the result back into human-readable \\LaTeX{} for semantic review. We evaluate MerLean on three theoretical quantum computing papers producing 2,050 Lean declarations from 114 statements in total. MerLean achieves end-to-end formalization on all three papers, reducing the verification burden to only the newly introduced definitions and axioms. Our results demonstrate that agentic autoformalization can scale to frontier research, offering both a practical tool for machine-verified peer review and a scalable engine for mining high-quality synthetic data to train future reasoning models. Our approach can also be generalized to any other rigorous research in mathematics and theoretical physics.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "pdf_url": "https://arxiv.org/pdf/2602.16554v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.16553v1",
      "title": "Agentic AI, Medical Morality, and the Transformation of the Patient-Physician Relationship",
      "authors": [
        "Robert Ranisch",
        "Sabine Salloch"
      ],
      "abstract": "The emergence of agentic AI marks a new phase in the digital transformation of healthcare. Distinct from conventional generative AI, agentic AI systems are capable of autonomous, goal-directed actions and complex task coordination. They promise to support or even collaborate with clinicians and patients in increasingly independent ways. While agentic AI raises familiar moral concerns regarding safety, accountability, and bias, this article focuses on a less explored dimension: its capacity to transform the moral fabric of healthcare itself. Drawing on the framework of techno-moral change and the three domains of decision, relation and perception, we investigate how agentic AI might reshape the patient-physician relationship and reconfigure core concepts of medical morality. We argue that these shifts, while not fully predictable, demand ethical attention before widespread deployment. Ultimately, the paper calls for integrating ethical foresight into the design and use of agentic AI.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "pdf_url": "https://arxiv.org/pdf/2602.16553v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.16520v1",
      "title": "Recursive language models for jailbreak detection: a procedural defense for tool-augmented agents",
      "authors": [
        "Doron Shavit"
      ],
      "abstract": "Jailbreak prompts are a practical and evolving threat to large language models (LLMs), particularly in agentic systems that execute tools over untrusted content. Many attacks exploit long-context hiding, semantic camouflage, and lightweight obfuscations that can evade single-pass guardrails. We present RLM-JB, an end-to-end jailbreak detection framework built on Recursive Language Models (RLMs), in which a root model orchestrates a bounded analysis program that transforms the input, queries worker models over covered segments, and aggregates evidence into an auditable decision. RLM-JB treats detection as a procedure rather than a one-shot classification: it normalizes and de-obfuscates suspicious inputs, chunks text to reduce context dilution and guarantee coverage, performs parallel chunk screening, and composes cross-chunk signals to recover split-payload attacks. On AutoDAN-style adversarial inputs, RLM-JB achieves high detection effectiveness across three LLM backends (ASR/Recall 92.5-98.0%) while maintaining very high precision (98.99-100%) and low false positive rates (0.0-2.0%), highlighting a practical sensitivity-specificity trade-off as the screening backend changes.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "pdf_url": "https://arxiv.org/pdf/2602.16520v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.16493v1",
      "title": "MMA: Multimodal Memory Agent",
      "authors": [
        "Yihao Lu",
        "Wanru Cheng",
        "Zeyu Zhang",
        "Hao Tang"
      ],
      "abstract": "Long-horizon multimodal agents depend on external memory; however, similarity-based retrieval often surfaces stale, low-credibility, or conflicting items, which can trigger overconfident errors. We propose Multimodal Memory Agent (MMA), which assigns each retrieved memory item a dynamic reliability score by combining source credibility, temporal decay, and conflict-aware network consensus, and uses this signal to reweight evidence and abstain when support is insufficient. We also introduce MMA-Bench, a programmatically generated benchmark for belief dynamics with controlled speaker reliability and structured text-vision contradictions. Using this framework, we uncover the \"Visual Placebo Effect\", revealing how RAG-based agents inherit latent visual biases from foundation models. On FEVER, MMA matches baseline accuracy while reducing variance by 35.2% and improving selective utility; on LoCoMo, a safety-oriented configuration improves actionable accuracy and reduces wrong answers; on MMA-Bench, MMA reaches 41.18% Type-B accuracy in Vision mode, while the baseline collapses to 0.0% under the same protocol. Code: https://github.com/AIGeeksGroup/MMA.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "pdf_url": "https://arxiv.org/pdf/2602.16493v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.16485v1",
      "title": "Team of Thoughts: Efficient Test-time Scaling of Agentic Systems through Orchestrated Tool Calling",
      "authors": [
        "Jeffrey T. H. Wong",
        "Zixi Zhang",
        "Junyi Liu",
        "Yiren Zhao"
      ],
      "abstract": "Existing Multi-Agent Systems (MAS) typically rely on static, homogeneous model configurations, limiting their ability to exploit the distinct strengths of differently post-trained models. To address this, we introduce Team-of-Thoughts, a novel MAS architecture that leverages the complementary capabilities of heterogeneous agents via an orchestrator-tool paradigm. Our framework introduces two key mechanisms to optimize performance: (1) an orchestrator calibration scheme that identifies models with superior coordination capabilities, and (2) a self-assessment protocol where tool agents profile their own domain expertise to account for variations in post-training skills. During inference, the orchestrator dynamically activates the most suitable tool agents based on these proficiency profiles. Experiments on five reasoning and code generation benchmarks show that Team-of-Thoughts delivers consistently superior task performance. Notably, on AIME24 and LiveCodeBench, our approach achieves accuracies of 96.67% and 72.53%, respectively, substantially outperforming homogeneous role-play baselines, which score 80% and 65.93%.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "pdf_url": "https://arxiv.org/pdf/2602.16485v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.16444v1",
      "title": "RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation",
      "authors": [
        "Yixue Zhang",
        "Kun Wu",
        "Zhi Gao",
        "Zhen Zhao",
        "Pei Ren",
        "Zhiyuan Xu",
        "Fei Liao",
        "Xinhua Wang",
        "Shichao Fan",
        "Di Wu",
        "Qiuxuan Feng",
        "Meng Li",
        "Zhengping Che",
        "Chang Liu",
        "Jian Tang"
      ],
      "abstract": "The pursuit of general-purpose robotic manipulation is hindered by the scarcity of diverse, real-world interaction data. Unlike data collection from web in vision or language, robotic data collection is an active process incurring prohibitive physical costs. Consequently, automated task curation to maximize data value remains a critical yet under-explored challenge. Existing manual methods are unscalable and biased toward common tasks, while off-the-shelf foundation models often hallucinate physically infeasible instructions. To address this, we introduce RoboGene, an agentic framework designed to automate the generation of diverse, physically plausible manipulation tasks across single-arm, dual-arm, and mobile robots. RoboGene integrates three core components: diversity-driven sampling for broad task coverage, self-reflection mechanisms to enforce physical constraints, and human-in-the-loop refinement for continuous improvement. We conduct extensive quantitative analysis and large-scale real-world experiments, collecting datasets of 18k trajectories and introducing novel metrics to assess task quality, feasibility, and diversity. Results demonstrate that RoboGene significantly outperforms state-of-the-art foundation models (e.g., GPT-4o, Gemini 2.5 Pro). Furthermore, real-world experiments show that VLA models pre-trained with RoboGene achieve higher success rates and superior generalization, underscoring the importance of high-quality task generation. Our project is available at https://robogene-boost-vla.github.io.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "pdf_url": "https://arxiv.org/pdf/2602.16444v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.16435v1",
      "title": "Causally-Guided Automated Feature Engineering with Multi-Agent Reinforcement Learning",
      "authors": [
        "Arun Vignesh Malarkkan",
        "Wangyang Ying",
        "Yanjie Fu"
      ],
      "abstract": "Automated feature engineering (AFE) enables AI systems to autonomously construct high-utility representations from raw tabular data. However, existing AFE methods rely on statistical heuristics, yielding brittle features that fail under distribution shift. We introduce CAFE, a framework that reformulates AFE as a causally-guided sequential decision process, bridging causal discovery with reinforcement learning-driven feature construction. Phase I learns a sparse directed acyclic graph over features and the target to obtain soft causal priors, grouping features as direct, indirect, or other based on their causal influence with respect to the target. Phase II uses a cascading multi-agent deep Q-learning architecture to select causal groups and transformation operators, with hierarchical reward shaping and causal group-level exploration strategies that favor causally plausible transformations while controlling feature complexity. Across 15 public benchmarks (classification with macro-F1; regression with inverse relative absolute error), CAFE achieves up to 7% improvement over strong AFE baselines, reduces episodes-to-convergence, and delivers competitive time-to-target. Under controlled covariate shifts, CAFE reduces performance drop by ~4x relative to a non-causal multi-agent baseline, and produces more compact feature sets with more stable post-hoc attributions. These findings underscore that causal structure, used as a soft inductive prior rather than a rigid constraint, can substantially improve the robustness and efficiency of automated feature engineering.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "pdf_url": "https://arxiv.org/pdf/2602.16435v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.16429v1",
      "title": "TabAgent: A Framework for Replacing Agentic Generative Components with Tabular-Textual Classifiers",
      "authors": [
        "Ido Levy",
        "Eilam Shapira",
        "Yinon Goldshtein",
        "Avi Yaeli",
        "Nir Mashkif",
        "Segev Shlomov"
      ],
      "abstract": "Agentic systems, AI architectures that autonomously execute multi-step workflows to achieve complex goals, are often built using repeated large language model (LLM) calls for closed-set decision tasks such as routing, shortlisting, gating, and verification. While convenient, this design makes deployments slow and expensive due to cumulative latency and token usage. We propose TabAgent, a framework for replacing generative decision components in closed-set selection tasks with a compact textual-tabular classifier trained on execution traces. TabAgent (i) extracts structured schema, state, and dependency features from trajectories (TabSchema), (ii) augments coverage with schema-aligned synthetic supervision (TabSynth), and (iii) scores candidates with a lightweight classifier (TabHead). On the long-horizon AppWorld benchmark, TabAgent maintains task-level success while eliminating shortlist-time LLM calls, reducing latency by approximately 95% and inference cost by 85-91%. Beyond tool shortlisting, TabAgent generalizes to other agentic decision heads, establishing a paradigm for learned discriminative replacements of generative bottlenecks in production agent architectures.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "pdf_url": "https://arxiv.org/pdf/2602.16429v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.16424v1",
      "title": "Verifiable Semantics for Agent-to-Agent Communication",
      "authors": [
        "Philipp Schoenegger",
        "Matt Carlson",
        "Chris Schneider",
        "Chris Daly"
      ],
      "abstract": "Multiagent AI systems require consistent communication, but we lack methods to verify that agents share the same understanding of the terms used. Natural language is interpretable but vulnerable to semantic drift, while learned protocols are efficient but opaque. We propose a certification protocol based on the stimulus-meaning model, where agents are tested on shared observable events and terms are certified if empirical disagreement falls below a statistical threshold. In this protocol, agents restricting their reasoning to certified terms (\"core-guarded reasoning\") achieve provably bounded disagreement. We also outline mechanisms for detecting drift (recertification) and recovering shared vocabulary (renegotiation). In simulations with varying degrees of semantic divergence, core-guarding reduces disagreement by 72-96%. In a validation with fine-tuned language models, disagreement is reduced by 51%. Our framework provides a first step towards verifiable agent-to-agent communication.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "pdf_url": "https://arxiv.org/pdf/2602.16424v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.15951v1",
      "title": "MadEvolve: Evolutionary Optimization of Cosmological Algorithms with Large Language Models",
      "authors": [
        "Tianyi Li",
        "Shihui Zang",
        "Moritz Münchmeyer"
      ],
      "abstract": "We develop a general framework to discover scientific algorithms and apply it to three problems in computational cosmology. Our code, MadEvolve, is similar to Google's AlphaEvolve, but places a stronger emphasis on free parameters and their optimization. Our code starts with a baseline human algorithm implementation, and then optimizes its performance metrics by making iterative changes to its code. As a further convenient feature, MadEvolve automatically generates a report that compares the input algorithm with the evolved algorithm, describes the algorithmic innovations and lists the free parameters and their function. Our code supports both auto-differentiable, gradient-based parameter optimization and gradient-free optimization methods. We apply MadEvolve to the reconstruction of cosmological initial conditions, 21cm foreground contamination reconstruction and effective baryonic physics in N-body simulations. In all cases, we find substantial improvements over the base algorithm. We make MadEvolve and our three tasks publicly available at madevolve.org.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15951v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.15791v1",
      "title": "Enhancing Building Semantics Preservation in AI Model Training with Large Language Model Encodings",
      "authors": [
        "Suhyung Jang",
        "Ghang Lee",
        "Jaekun Lee",
        "Hyunjun Lee"
      ],
      "abstract": "Accurate representation of building semantics, encompassing both generic object types and specific subtypes, is essential for effective AI model training in the architecture, engineering, construction, and operation (AECO) industry. Conventional encoding methods (e.g., one-hot) often fail to convey the nuanced relationships among closely related subtypes, limiting AI's semantic comprehension. To address this limitation, this study proposes a novel training approach that employs large language model (LLM) embeddings (e.g., OpenAI GPT and Meta LLaMA) as encodings to preserve finer distinctions in building semantics. We evaluated the proposed method by training GraphSAGE models to classify 42 building object subtypes across five high-rise residential building information models (BIMs). Various embedding dimensions were tested, including original high-dimensional LLM embeddings (1,536, 3,072, or 4,096) and 1,024-dimensional compacted embeddings generated via the Matryoshka representation model. Experimental results demonstrated that LLM encodings outperformed the conventional one-hot baseline, with the llama-3 (compacted) embedding achieving a weighted average F1-score of 0.8766, compared to 0.8475 for one-hot encoding. The results underscore the promise of leveraging LLM-based encodings to enhance AI's ability to interpret complex, domain-specific building semantics. As the capabilities of LLMs and dimensionality reduction techniques continue to evolve, this approach holds considerable potential for broad application in semantic elaboration tasks throughout the AECO industry.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15791v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.15769v1",
      "title": "ViTaB-A: Evaluating Multimodal Large Language Models on Visual Table Attribution",
      "authors": [
        "Yahia Alqurnawi",
        "Preetom Biswas",
        "Anmol Rao",
        "Tejas Anvekar",
        "Chitta Baral",
        "Vivek Gupta"
      ],
      "abstract": "Multimodal Large Language Models (mLLMs) are often used to answer questions in structured data such as tables in Markdown, JSON, and images. While these models can often give correct answers, users also need to know where those answers come from. In this work, we study structured data attribution/citation, which is the ability of the models to point to the specific rows and columns that support an answer. We evaluate several mLLMs across different table formats and prompting strategies. Our results show a clear gap between question answering and evidence attribution. Although question answering accuracy remains moderate, attribution accuracy is much lower, near random for JSON inputs, across all models. We also find that models are more reliable at citing rows than columns, and struggle more with textual formats than images. Finally, we observe notable differences across model families. Overall, our findings show that current mLLMs are unreliable at providing fine-grained, trustworthy attribution for structured data, which limits their usage in applications requiring transparency and traceability.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15769v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.15725v1",
      "title": "Recursive Concept Evolution for Compositional Reasoning in Large Language Models",
      "authors": [
        "Sarim Chaudhry"
      ],
      "abstract": "Large language models achieve strong performance on many complex reasoning tasks, yet their accuracy degrades sharply on benchmarks that require compositional reasoning, including ARC-AGI-2, GPQA, MATH, BBH, and HLE. Existing methods improve reasoning by expanding token-level search through chain-of-thought prompting, self-consistency, or reinforcement learning, but they leave the model's latent representation space fixed. When the required abstraction is not already encoded in this space, performance collapses. We propose Recursive Concept Evolution (RCE), a framework that enables pretrained language models to modify their internal representation geometry during inference. RCE introduces dynamically generated low-rank concept subspaces that are spawned when representational inadequacy is detected, selected through a minimum description length criterion, merged when synergistic, and consolidated via constrained optimization to preserve stability. This process allows the model to construct new abstractions rather than recombining existing ones. We integrate RCE with Mistral-7B and evaluate it across compositional reasoning benchmarks. RCE yields 12-18 point gains on ARC-AGI-2, 8-14 point improvements on GPQA and BBH, and consistent reductions in depth-induced error on MATH and HLE.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15725v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.15689v2",
      "title": "A Content-Based Framework for Cybersecurity Refusal Decisions in Large Language Models",
      "authors": [
        "Noa Linder",
        "Meirav Segal",
        "Omer Antverg",
        "Gil Gekker",
        "Tomer Fichman",
        "Omri Bodenheimer",
        "Edan Maor",
        "Omer Nevo"
      ],
      "abstract": "Large language models and LLM-based agents are increasingly used for cybersecurity tasks that are inherently dual-use. Existing approaches to refusal, spanning academic policy frameworks and commercially deployed systems, often rely on broad topic-based bans or offensive-focused taxonomies. As a result, they can yield inconsistent decisions, over-restrict legitimate defenders, and behave brittlely under obfuscation or request segmentation. We argue that effective refusal requires explicitly modeling the trade-off between offensive risk and defensive benefit, rather than relying solely on intent or offensive classification. In this paper, we introduce a content-based framework for designing and auditing cyber refusal policies that makes offense-defense tradeoffs explicit. The framework characterizes requests along five dimensions: Offensive Action Contribution, Offensive Risk, Technical Complexity, Defensive Benefit, and Expected Frequency for Legitimate Users, grounded in the technical substance of the request rather than stated intent. We demonstrate that this content-grounded approach resolves inconsistencies in current frontier model behavior and allows organizations to construct tunable, risk-aware refusal policies.",
      "published": "2026-02-17",
      "updated": "2026-02-18",
      "pdf_url": "https://arxiv.org/pdf/2602.15689v2",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.15678v1",
      "title": "Revisiting Northrop Frye's Four Myths Theory with Large Language Models",
      "authors": [
        "Edirlei Soares de Lima",
        "Marco A. Casanova",
        "Antonio L. Furtado"
      ],
      "abstract": "Northrop Frye's theory of four fundamental narrative genres (comedy, romance, tragedy, satire) has profoundly influenced literary criticism, yet computational approaches to his framework have focused primarily on narrative patterns rather than character functions. In this paper, we present a new character function framework that complements pattern-based analysis by examining how archetypal roles manifest differently across Frye's genres. Drawing on Jungian archetype theory, we derive four universal character functions (protagonist, mentor, antagonist, companion) by mapping them to Jung's psychic structure components. These functions are then specialized into sixteen genre-specific roles based on prototypical works. To validate this framework, we conducted a multi-model study using six state-of-the-art Large Language Models (LLMs) to evaluate character-role correspondences across 40 narrative works. The validation employed both positive samples (160 valid correspondences) and negative samples (30 invalid correspondences) to evaluate whether models both recognize valid correspondences and reject invalid ones. LLMs achieved substantial performance (mean balanced accuracy of 82.5%) with strong inter-model agreement (Fleiss' $κ$ = 0.600), demonstrating that the proposed correspondences capture systematic structural patterns. Performance varied by genre (ranging from 72.7% to 89.9%) and role (52.5% to 99.2%), with qualitative analysis revealing that variations reflect genuine narrative properties, including functional distribution in romance and deliberate archetypal subversion in satire. This character-based approach demonstrates the potential of LLM-supported methods for computational narratology and provides a foundation for future development of narrative generation methods and interactive storytelling applications.",
      "published": "2026-02-17",
      "updated": "2026-02-17",
      "pdf_url": "https://arxiv.org/pdf/2602.15678v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.07905v1",
      "title": "MedCoG: Maximizing LLM Inference Density in Medical Reasoning via Meta-Cognitive Regulation",
      "authors": [
        "Yu Zhao",
        "Hao Guan",
        "Yongcheng Jing",
        "Ying Zhang",
        "Dacheng Tao"
      ],
      "abstract": "Large Language Models (LLMs) have shown strong potential in complex medical reasoning yet face diminishing gains under inference scaling laws. While existing studies augment LLMs with various knowledge types, it remains unclear how effectively the additional costs translate into accuracy. In this paper, we explore how meta-cognition of LLMs, i.e., their self-awareness of their own knowledge states, can regulate the reasoning process. Specifically, we propose MedCoG, a Medical Meta-Cognition Agent with Knowledge Graph, where the meta-cognitive assessments of task complexity, familiarity, and knowledge density dynamically regulate utilization of procedural, episodic, and factual knowledge. The LLM-centric on-demand reasoning aims to mitigate scaling laws by (1) reducing costs via avoiding indiscriminate scaling, (2) improving accuracy via filtering out distractive knowledge. To validate this, we empirically characterize the scaling curve and introduce inference density to quantify inference efficiency, defined as the ratio of theoretically effective cost to actual cost. Experiments demonstrate the effectiveness and efficiency of MedCoG on five hard sets of medical benchmarks, yielding 5.5x inference density. Furthermore, the Oracle study highlights the significant potential of meta-cognitive regulation.",
      "published": "2026-02-08",
      "updated": "2026-02-08",
      "pdf_url": "https://arxiv.org/pdf/2602.07905v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2602.07529v2",
      "title": "MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution",
      "authors": [
        "Jianwen Chen",
        "Xinyu Yang",
        "Peng Xia",
        "Arian Azarang",
        "Yueh Z Lee",
        "Gang Li",
        "Hongtu Zhu",
        "Yun Li",
        "Beidi Chen",
        "Huaxiu Yao"
      ],
      "abstract": "Large language models (LLMs) have demonstrated strong performance and rapid progress in a wide range of medical reasoning tasks. However, their sequential autoregressive decoding forces inherently parallel clinical reasoning, such as differential diagnosis, into a single linear reasoning path, limiting both efficiency and reliability for complex medical problems. To address this, we propose MedVerse, a reasoning framework for complex medical inference that reformulates medical reasoning as a parallelizable directed acyclic graph (DAG) process based on Petri net theory. The framework adopts a full-stack design across data, model architecture, and system execution. For data creation, we introduce the MedVerse Curator, an automated pipeline that synthesizes knowledge-grounded medical reasoning paths and transforms them into Petri net-structured representations. At the architectural level, we propose a topology-aware attention mechanism with adaptive position indices that supports parallel reasoning while preserving logical consistency. Systematically, we develop a customized inference engine that supports parallel execution without additional overhead. Empirical evaluations show that MedVerse improves strong general-purpose LLMs by up to 8.9%. Compared to specialized medical LLMs, MedVerse achieves comparable performance while delivering a 1.3x reduction in inference latency and a 1.7x increase in generation throughput, enabled by its parallel decoding capability. Code is available at https://github.com/aiming-lab/MedVerse.",
      "published": "2026-02-07",
      "updated": "2026-02-10",
      "pdf_url": "https://arxiv.org/pdf/2602.07529v2",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2601.20221v1",
      "title": "Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning",
      "authors": [
        "Hang Zhang",
        "Ruheng Wang",
        "Yuelyu Ji",
        "Mingu Kwak",
        "Xizhi Wu",
        "Chenyu Li",
        "Li Zhang",
        "Wenqi Shi",
        "Yifan Peng",
        "Yanshan Wang"
      ],
      "abstract": "Large language models have achieved strong performance on medical reasoning benchmarks, yet their deployment in clinical settings demands rigorous verification to ensure factual accuracy. While reward models offer a scalable approach for reasoning trace verification, existing methods face two limitations: they produce only scalar reward values without explicit justification, and they rely on single-pass retrieval that precludes adaptive knowledge access as verification unfolds. We introduce $\\method$, an agentic framework that addresses these limitations by training medical reasoning verifiers to iteratively query external medical corpora during evaluation. Our approach combines tool-augmented verification with an iterative reinforcement learning paradigm that requires only trace-level supervision, alongside an adaptive curriculum mechanism that dynamically adjusts training data distribution. Across four medical reasoning benchmarks, $\\method$ achieves substantial gains over existing methods, improving MedQA accuracy by 23.5% and MedXpertQA by 32.0% relative to the base generator in particular. Crucially, $\\method$ demonstrates an $\\mathbf{8\\times}$ reduction in sampling budget requirement compared to prior reward model baselines. These findings establish that grounding verification in dynamically retrieved evidence offers a principled path toward more reliable medical reasoning systems.",
      "published": "2026-01-28",
      "updated": "2026-01-28",
      "pdf_url": "https://arxiv.org/pdf/2601.20221v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2601.13262v1",
      "title": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning",
      "authors": [
        "Eric Onyame",
        "Akash Ghosh",
        "Subhadip Baidya",
        "Sriparna Saha",
        "Xiuying Chen",
        "Chirag Agarwal"
      ],
      "abstract": "While large language models (LLMs) have shown to perform well on monolingual mathematical and commonsense reasoning, they remain unreliable for multilingual medical reasoning applications, hindering their deployment in multilingual healthcare settings. We address this by first introducing CUREMED-BENCH, a high-quality multilingual medical reasoning dataset with open-ended reasoning queries with a single verifiable answer, spanning thirteen languages, including underrepresented languages such as Amharic, Yoruba, and Swahili. Building on this dataset, we propose CURE-MED, a curriculum-informed reinforcement learning framework that integrates code-switching-aware supervised fine-tuning and Group Relative Policy Optimization to jointly improve logical correctness and language stability. Across thirteen languages, our approach consistently outperforms strong baselines and scales effectively, achieving 85.21% language consistency and 54.35% logical correctness at 7B parameters, and 94.96% language consistency and 70.04% logical correctness at 32B parameters. These results support reliable and equitable multilingual medical reasoning in LLMs. The code and dataset are available at https://cure-med.github.io/",
      "published": "2026-01-19",
      "updated": "2026-01-19",
      "pdf_url": "https://arxiv.org/pdf/2601.13262v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2601.08267v2",
      "title": "Med-CoReasoner: Reducing Language Disparities in Medical Reasoning via Language-Informed Co-Reasoning",
      "authors": [
        "Fan Gao",
        "Sherry T. Tong",
        "Jiwoong Sohn",
        "Jiahao Huang",
        "Junfeng Jiang",
        "Ding Xia",
        "Piyalitt Ittichaiwong",
        "Kanyakorn Veerakanjana",
        "Hyunjae Kim",
        "Qingyu Chen",
        "Edison Marrese Taylor",
        "Kazuma Kobayashi",
        "Akkiko Aizawa",
        "Irene Li"
      ],
      "abstract": "While reasoning-enhanced large language models perform strongly on English medical tasks, a persistent multilingual gap remains, with substantially weaker reasoning in local languages, limiting equitable global medical deployment. To bridge this gap, we introduce Med-CoReasoner, a language-informed co-reasoning framework that elicits parallel English and local-language reasoning, abstracts them into structured concepts, and integrates local clinical knowledge into an English logical scaffold via concept-level alignment and retrieval. This design combines the structural robustness of English reasoning with the practice-grounded expertise encoded in local languages. To evaluate multilingual medical reasoning beyond multiple-choice settings, we construct MultiMed-X, a benchmark covering seven languages with expert-annotated long-form question answering and natural language inference tasks, comprising 350 instances per language. Experiments across three benchmarks show that Med-CoReasoner improves multilingual reasoning performance by an average of 5%, with particularly substantial gains in low-resource languages. Moreover, model distillation and expert evaluation analysis further confirm that Med-CoReasoner produces clinically sound and culturally grounded reasoning traces.",
      "published": "2026-01-13",
      "updated": "2026-01-19",
      "pdf_url": "https://arxiv.org/pdf/2601.08267v2",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2512.13510v1",
      "title": "MedCEG: Reinforcing Verifiable Medical Reasoning with Critical Evidence Graph",
      "authors": [
        "Linjie Mu",
        "Yannian Gu",
        "Zhongzhen Huang",
        "Yakun Zhu",
        "Shaoting Zhang",
        "Xiaofan Zhang"
      ],
      "abstract": "Large language models with reasoning capabilities have demonstrated impressive performance across a wide range of domains. In clinical applications, a transparent, step-by-step reasoning process provides physicians with strong evidence to support decision-making. While reinforcement learning has effectively enhanced reasoning performance in medical contexts, the clinical reliability of these reasoning processes remains limited because their accuracy and validity are often overlooked during training. To address this gap, we propose MedCEG, a framework that augments medical language models with clinically valid reasoning pathways by explicitly supervising the reasoning process through a Critical Evidence Graph (CEG). We curate a dataset of challenging clinical cases and algorithmically construct a CEG for each sample to represent a high-quality verifiable reasoning pathway. To guide the reasoning process, we introduce a Clinical Reasoning Procedure Reward, which evaluates Node Coverage, Structural Correctness, and Chain Completeness, thereby providing a holistic assessment of reasoning quality. Experimental results show that MedCEG surpasses existing methods in performance while producing clinically valid reasoning chains, representing a solid advancement in reliable medical AI reasoning. The code and models are available at https://github.com/LinjieMu/MedCEG.",
      "published": "2025-12-15",
      "updated": "2025-12-15",
      "pdf_url": "https://arxiv.org/pdf/2512.13510v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2512.13742v1",
      "title": "DL$^3$M: A Vision-to-Language Framework for Expert-Level Medical Reasoning through Deep Learning and Large Language Models",
      "authors": [
        "Md. Najib Hasan",
        "Imran Ahmad",
        "Sourav Basak Shuvo",
        "Md. Mahadi Hasan Ankon",
        "Sunanda Das",
        "Nazmul Siddique",
        "Hui Wang"
      ],
      "abstract": "Medical image classifiers detect gastrointestinal diseases well, but they do not explain their decisions. Large language models can generate clinical text, yet they struggle with visual reasoning and often produce unstable or incorrect explanations. This leaves a gap between what a model sees and the type of reasoning a clinician expects. We introduce a framework that links image classification with structured clinical reasoning. A new hybrid model, MobileCoAtNet, is designed for endoscopic images and achieves high accuracy across eight stomach-related classes. Its outputs are then used to drive reasoning by several LLMs. To judge this reasoning, we build two expert-verified benchmarks covering causes, symptoms, treatment, lifestyle, and follow-up care. Thirty-two LLMs are evaluated against these gold standards. Strong classification improves the quality of their explanations, but none of the models reach human-level stability. Even the best LLMs change their reasoning when prompts vary. Our study shows that combining DL with LLMs can produce useful clinical narratives, but current LLMs remain unreliable for high-stakes medical decisions. The framework provides a clearer view of their limits and a path for building safer reasoning systems. The complete source code and datasets used in this study are available at https://github.com/souravbasakshuvo/DL3M.",
      "published": "2025-12-14",
      "updated": "2025-12-14",
      "pdf_url": "https://arxiv.org/pdf/2512.13742v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2512.05658v1",
      "title": "Grounded Multilingual Medical Reasoning for Question Answering with Large Language Models",
      "authors": [
        "Pietro Ferrazzi",
        "Aitor Soroa",
        "Rodrigo Agerri"
      ],
      "abstract": "Large Language Models (LLMs) with reasoning capabilities have recently demonstrated strong potential in medical Question Answering (QA). Existing approaches are largely English-focused and primarily rely on distillation from general-purpose LLMs, raising concerns about the reliability of their medical knowledge. In this work, we present a method to generate multilingual reasoning traces grounded in factual medical knowledge. We produce 500k traces in English, Italian, and Spanish, using a retrievalaugmented generation approach over medical information from Wikipedia. The traces are generated to solve medical questions drawn from MedQA and MedMCQA, which we extend to Italian and Spanish. We test our pipeline in both in-domain and outof-domain settings across Medical QA benchmarks, and demonstrate that our reasoning traces improve performance both when utilized via in-context learning (few-shot) and supervised fine-tuning, yielding state-of-the-art results among 8B-parameter LLMs. We believe that these resources can support the development of safer, more transparent clinical decision-support tools in multilingual settings. We release the full suite of resources: reasoning traces, translated QA datasets, Medical-Wikipedia, and fine-tuned models.",
      "published": "2025-12-05",
      "updated": "2025-12-05",
      "pdf_url": "https://arxiv.org/pdf/2512.05658v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2511.23269v1",
      "title": "OctoMed: Data Recipes for State-of-the-Art Multimodal Medical Reasoning",
      "authors": [
        "Timothy Ossowski",
        "Sheng Zhang",
        "Qianchu Liu",
        "Guanghui Qin",
        "Reuben Tan",
        "Tristan Naumann",
        "Junjie Hu",
        "Hoifung Poon"
      ],
      "abstract": "High-quality and carefully curated data is a cornerstone of training medical large language models, as it directly impacts both generalization and robustness to unseen clinical tasks. We investigate strategies for training and data curation to develop a robust multimodal reasoning model in the medical domain. Our work focuses on supervised fine-tuning (SFT) and explores data recipes that leverage structured reasoning traces. Using our proposed data recipe, we scale experiments to a dataset of over 8 million examples and 6.8 billion response tokens, achieving state-of-the-art performance among open-source models across diverse out-of-distribution medical benchmark tasks. Our results further indicate that curating a high-quality, diverse training dataset with varying structured reasoning trace lengths enables the fine-tuned model to self-calibrate its reasoning trajectory lengths based on the downstream task, without explicit supervision. We present key insights, describe the data curation strategy, and outline next steps toward developing robust medical vision-language reasoning system.",
      "published": "2025-11-28",
      "updated": "2025-11-28",
      "pdf_url": "https://arxiv.org/pdf/2511.23269v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2511.00421v1",
      "title": "MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts",
      "authors": [
        "Naoto Iwase",
        "Hiroki Okuyama",
        "Junichiro Iwasawa"
      ],
      "abstract": "Large language models (LLMs) show increasing promise in medical applications, but their ability to detect and correct errors in clinical texts -- a prerequisite for safe deployment -- remains under-evaluated, particularly beyond English. We introduce MedRECT, a cross-lingual benchmark (Japanese/English) that formulates medical error handling as three subtasks: error detection, error localization (sentence extraction), and error correction. MedRECT is built with a scalable, automated pipeline from the Japanese Medical Licensing Examinations (JMLE) and a curated English counterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with comparable error/no-error balance. We evaluate 9 contemporary LLMs spanning proprietary, open-weight, and reasoning families. Key findings: (i) reasoning models substantially outperform standard architectures, with up to 13.5% relative improvement in error detection and 51.0% in sentence extraction; (ii) cross-lingual evaluation reveals 5-10% performance gaps from English to Japanese, with smaller disparities for reasoning models; (iii) targeted LoRA fine-tuning yields asymmetric improvements in error correction performance (Japanese: +0.078, English: +0.168) while preserving reasoning capabilities; and (iv) our fine-tuned model exceeds human expert performance on structured medical error correction tasks. To our knowledge, MedRECT is the first comprehensive cross-lingual benchmark for medical error correction, providing a reproducible framework and resources for developing safer medical LLMs across languages.",
      "published": "2025-11-01",
      "updated": "2025-11-01",
      "pdf_url": "https://arxiv.org/pdf/2511.00421v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2510.03536v2",
      "title": "Triplet-Structured Knowledge Integration for Multi-Turn Medical Reasoning",
      "authors": [
        "Zhaohan Meng",
        "Zaiqiao Meng",
        "Siwei Liu",
        "Iadh Ounis"
      ],
      "abstract": "Large Language Models (LLMs) have shown strong performance on static medical Question Answering (QA) tasks, yet their reasoning often deteriorates in multi-turn clinical dialogues where patient information is scattered across turns. This paper introduces TriMediQ, a triplet-structured approach that enhances the reasoning reliability of LLMs through explicit knowledge integration. TriMediQ first employs a frozen triplet extraction LLM to convert patient responses into clinically grounded triplets, ensuring factual precision via constrained prompting. These triplets are incorporated into a patient-specific Knowledge Graph (KG), from which a trainable projection module consisting of a graph encoder and a projector captures relational dependencies while keeping all LLM parameters frozen. During inference, the projection module guides multi-hop reasoning over the KG, enabling coherent clinical dialogue understanding. Experiments on two interactive medical QA benchmarks show that TriMediQ achieves up to 10.4\\% improvement in accuracy over five existing baselines on the iMedQA dataset. These results demonstrate that structuring patient information as triplets can effectively improve the reasoning capability of LLMs in multi-turn medical QA.",
      "published": "2025-10-03",
      "updated": "2025-10-14",
      "pdf_url": "https://arxiv.org/pdf/2510.03536v2",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2509.23725v2",
      "title": "MedLA: A Logic-Driven Multi-Agent Framework for Complex Medical Reasoning with Large Language Models",
      "authors": [
        "Siqi Ma",
        "Jiajie Huang",
        "Fan Zhang",
        "Jinlin Wu",
        "Yue Shen",
        "Guohui Fan",
        "Zhu Zhang",
        "Zelin Zang"
      ],
      "abstract": "Answering complex medical questions requires not only domain expertise and patient-specific information, but also structured and multi-perspective reasoning. Existing multi-agent approaches often rely on fixed roles or shallow interaction prompts, limiting their ability to detect and resolve fine-grained logical inconsistencies. To address this, we propose \\textsc{MedLA}, a logic-driven multi-agent framework built on large language models. Each agent organizes its reasoning process into an explicit logical tree based on syllogistic triads (major premise, minor premise, and conclusion), enabling transparent inference and premise-level alignment. Agents engage in a multi-round, graph-guided discussion to compare and iteratively refine their logic trees, achieving consensus through error correction and contradiction resolution. We demonstrate that \\textsc{MedLA} consistently outperforms both static role-based systems and single-agent baselines on challenging benchmarks such as MedDDx and standard medical QA tasks. Furthermore, \\textsc{MedLA} scales effectively across both open-source and commercial LLM backbones, achieving state-of-the-art performance and offering a generalizable paradigm for trustworthy medical reasoning.",
      "published": "2025-09-28",
      "updated": "2025-11-19",
      "pdf_url": "https://arxiv.org/pdf/2509.23725v2",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2509.23368v1",
      "title": "MedCritical: Enhancing Medical Reasoning in Small Language Models via Self-Collaborative Correction",
      "authors": [
        "Xinchun Su",
        "Chunxu Luo",
        "Yixuan Li",
        "Weidong Yang",
        "Lipeng Ma"
      ],
      "abstract": "In the field of medicine, complex reasoning tasks such as clinical diagnosis, treatment planning, and medical knowledge integration pose significant challenges, where small language models often underperform compared to large language models like GPT-4 and Deepseek. Recent knowledge distillation-based methods aim to address these issues through teacher-guided error correction, but this LLM as judge approach remains challenging in terms of cost, time, and efficiency. To circumvent this issue, we propose a novel two-stage framework, MedCritical, which uses a small language model fine-tuned by a large teacher model to play against itself. In the first stage, we extract high-level and detailed long-chain thought templates from the teacher model to guide the student model to generate more complex reasoning thoughts. In the second stage, we introduce direct preference optimization (DPO) through model self-iteration collaboration to enhance the reasoning ability of the student model by playing against the correction trajectory of the fine-tuned model during training. This model self-learning DPO approach teaches the student model to use its own error-driven insights to consolidate its skills and knowledge to solve complex problems, and achieves comparable results to traditional knowledge distillation methods using teacher models at a lower cost. Notably, our MedCritical 7B model outperforms the Taiyi and Huatuo-o1-7B models by 3.04\\% and 10.12\\% respectively on the CMExam benchmark, achieving new SOTA performance among 7B-class small models.",
      "published": "2025-09-27",
      "updated": "2025-09-27",
      "pdf_url": "https://arxiv.org/pdf/2509.23368v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2509.22713v1",
      "title": "RAR$^2$: Retrieval-Augmented Medical Reasoning via Thought-Driven Retrieval",
      "authors": [
        "Kaishuai Xu",
        "Wenjun Hou",
        "Yi Cheng",
        "Wenjie Li"
      ],
      "abstract": "Large Language Models (LLMs) have shown promising performance on diverse medical benchmarks, highlighting their potential in supporting real-world clinical tasks. Retrieval-Augmented Generation (RAG) has emerged as a key approach for mitigating knowledge gaps and hallucinations by incorporating external medical information. However, RAG still struggles with complex medical questions that require intensive reasoning, as surface-level input often fails to reflect the true knowledge needs of the task. Existing methods typically focus on refining queries without explicitly modeling the reasoning process, limiting their ability to retrieve and integrate clinically relevant knowledge. In this work, we propose RAR$^2$, a joint learning framework that improves both Reasoning-Augmented Retrieval and Retrieval-Augmented Reasoning. RAR$^2$ constructs a thought process to uncover implicit knowledge requirements and uses it to guide retrieval and answer generation. We build a training dataset of mixed preference pairs and apply Direct Preference Optimization (DPO) to train the model. Moreover, we design two test-time scaling strategies to explore the boundaries of our framework. Experiments demonstrate the effectiveness of RAR$^2$ across several biomedical question answering datasets, outperforming RAG baselines with or without fine-tuning.",
      "published": "2025-09-24",
      "updated": "2025-09-24",
      "pdf_url": "https://arxiv.org/pdf/2509.22713v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2509.15279v1",
      "title": "Fleming-R1: Toward Expert-Level Medical Reasoning via Reinforcement Learning",
      "authors": [
        "Chi Liu",
        "Derek Li",
        "Yan Shu",
        "Robin Chen",
        "Derek Duan",
        "Teng Fang",
        "Bryan Dai"
      ],
      "abstract": "While large language models show promise in medical applications, achieving expert-level clinical reasoning remains challenging due to the need for both accurate answers and transparent reasoning processes. To address this challenge, we introduce Fleming-R1, a model designed for verifiable medical reasoning through three complementary innovations. First, our Reasoning-Oriented Data Strategy (RODS) combines curated medical QA datasets with knowledge-graph-guided synthesis to improve coverage of underrepresented diseases, drugs, and multi-hop reasoning chains. Second, we employ Chain-of-Thought (CoT) cold start to distill high-quality reasoning trajectories from teacher models, establishing robust inference priors. Third, we implement a two-stage Reinforcement Learning from Verifiable Rewards (RLVR) framework using Group Relative Policy Optimization, which consolidates core reasoning skills while targeting persistent failure modes through adaptive hard-sample mining. Across diverse medical benchmarks, Fleming-R1 delivers substantial parameter-efficient improvements: the 7B variant surpasses much larger baselines, while the 32B model achieves near-parity with GPT-4o and consistently outperforms strong open-source alternatives. These results demonstrate that structured data design, reasoning-oriented initialization, and verifiable reinforcement learning can advance clinical reasoning beyond simple accuracy optimization. We release Fleming-R1 publicly to promote transparent, reproducible, and auditable progress in medical AI, enabling safer deployment in high-stakes clinical environments.",
      "published": "2025-09-18",
      "updated": "2025-09-18",
      "pdf_url": "https://arxiv.org/pdf/2509.15279v1",
      "keyword": "Medical reasoning LLM"
    }
  ]
}