{
  "last_updated": "2026-02-22 01:50 UTC",
  "keywords": [
    "Medical reasoning LLM",
    "Large language model",
    "Agentic AI"
  ],
  "papers": [
    {
      "id": "http://arxiv.org/abs/2602.17529v1",
      "title": "Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation",
      "authors": [
        "Dun Yuan",
        "Hao Zhou",
        "Xue Liu",
        "Hao Chen",
        "Yan Xin",
        "Jianzhong",
        "Zhang"
      ],
      "abstract": "Large language models (LLMs) have shown strong potential across a variety of tasks, but their application in the telecom field remains challenging due to domain complexity, evolving standards, and specialized terminology. Therefore, general-domain LLMs may struggle to provide accurate and reliable outputs in this context, leading to increased hallucinations and reduced utility in telecom operations.To address these limitations, this work introduces KG-RAG-a novel framework that integrates knowledge graphs (KGs) with retrieval-augmented generation (RAG) to enhance LLMs for telecom-specific tasks. In particular, the KG provides a structured representation of domain knowledge derived from telecom standards and technical documents, while RAG enables dynamic retrieval of relevant facts to ground the model's outputs. Such a combination improves factual accuracy, reduces hallucination, and ensures compliance with telecom specifications.Experimental results across benchmark datasets demonstrate that KG-RAG outperforms both LLM-only and standard RAG baselines, e.g., KG-RAG achieves an average accuracy improvement of 14.3% over RAG and 21.6% over LLM-only models. These results highlight KG-RAG's effectiveness in producing accurate, reliable, and explainable outputs in complex telecom scenarios.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "pdf_url": "https://arxiv.org/pdf/2602.17529v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.17497v1",
      "title": "Retrospective In-Context Learning for Temporal Credit Assignment with Large Language Models",
      "authors": [
        "Wen-Tse Chen",
        "Jiayu Chen",
        "Fahim Tajwar",
        "Hao Zhu",
        "Xintong Duan",
        "Ruslan Salakhutdinov",
        "Jeff Schneider"
      ],
      "abstract": "Learning from self-sampled data and sparse environmental feedback remains a fundamental challenge in training self-evolving agents. Temporal credit assignment mitigates this issue by transforming sparse feedback into dense supervision signals. However, previous approaches typically depend on learning task-specific value functions for credit assignment, which suffer from poor sample efficiency and limited generalization. In this work, we propose to leverage pretrained knowledge from large language models (LLMs) to transform sparse rewards into dense training signals (i.e., the advantage function) through retrospective in-context learning (RICL). We further propose an online learning framework, RICOL, which iteratively refines the policy based on the credit assignment results from RICL. We empirically demonstrate that RICL can accurately estimate the advantage function with limited samples and effectively identify critical states in the environment for temporal credit assignment. Extended evaluation on four BabyAI scenarios show that RICOL achieves comparable convergent performance with traditional online RL algorithms with significantly higher sample efficiency. Our findings highlight the potential of leveraging LLMs for temporal credit assignment, paving the way for more sample-efficient and generalizable RL paradigms.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "pdf_url": "https://arxiv.org/pdf/2602.17497v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.17433v1",
      "title": "Preserving Historical Truth: Detecting Historical Revisionism in Large Language Models",
      "authors": [
        "Francesco Ortu",
        "Joeun Yook",
        "Punya Syon Pandey",
        "Keenan Samway",
        "Bernhard Schölkopf",
        "Alberto Cazzaniga",
        "Rada Mihalcea",
        "Zhijing Jin"
      ],
      "abstract": "Large language models (LLMs) are increasingly used as sources of historical information, motivating the need for scalable audits on contested events and politically charged narratives in settings that mirror real user interactions. We introduce \\textsc{\\texttt{HistoricalMisinfo}}, a curated dataset of $500$ contested events from $45$ countries, each paired with a factual reference narrative and a documented revisionist reference narrative. To approximate real-world usage, we instantiate each event in $11$ prompt scenarios that reflect common communication settings (e.g., questions, textbooks, social posts, policy briefs). Using an LLM-as-a-judge protocol that compares model outputs to the two references, we evaluate LLMs varying across model architectures in two conditions: (i) neutral user prompts that ask for factually accurate information, and (ii) robustness prompts in which the user explicitly requests the revisionist version of the event. Under neutral prompts, models are generally closer to factual references, though the resulting scores should be interpreted as reference-alignment signals rather than definitive evidence of human-interpretable revisionism. Robustness prompting yields a strong and consistent effect: when the user requests the revisionist narrative, all evaluated models show sharply higher revisionism scores, indicating limited resistance or self-correction. \\textsc{\\texttt{HistoricalMisinfo}} provides a practical foundation for benchmarking robustness to revisionist framing and for guiding future work on more precise automatic evaluation of contested historical claims to ensure a sustainable integration of AI systems within society.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "pdf_url": "https://arxiv.org/pdf/2602.17433v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.17419v1",
      "title": "EAGLE: Expert-Augmented Attention Guidance for Tuning-Free Industrial Anomaly Detection in Multimodal Large Language Models",
      "authors": [
        "Xiaomeng Peng",
        "Xilang Huang",
        "Seon Han Choi"
      ],
      "abstract": "Industrial anomaly detection is important for smart manufacturing, but many deep learning approaches produce only binary decisions and provide limited semantic explanations. Multimodal large language models (MLLMs) can potentially generate fine-grained, language-based analyses, yet existing methods often require costly fine-tuning and do not consistently improve anomaly detection accuracy compared to lightweight specialist detectors. We propose expert-augmented attention guidance for industrial anomaly detection in MLLMs (EAGLE), a tuning-free framework that integrates outputs from expert model to guide MLLMs toward both accurate detection and interpretable anomaly descriptions. We further study how EAGLE affects MLLMs internals by examining the attention distribution of MLLMs to the anomalous image regions in the intermediate layers. We observe that successful anomaly detection is associated with increased attention concentration on anomalous regions, and EAGLE tends to encourage this alignment. Experiments on MVTec-AD and VisA show that EAGLE improves anomaly detection performance across multiple MLLMs without any parameter updates, achieving results comparable to fine-tuning based methods. Code is available at \\href{https://github.com/shengtun/Eagle}{https://github.com/shengtun/Eagle}",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "pdf_url": "https://arxiv.org/pdf/2602.17419v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.17418v1",
      "title": "A Privacy by Design Framework for Large Language Model-Based Applications for Children",
      "authors": [
        "Diana Addae",
        "Diana Rogachova",
        "Nafiseh Kahani",
        "Masoud Barati",
        "Michael Christensen",
        "Chen Zhou"
      ],
      "abstract": "Children are increasingly using technologies powered by Artificial Intelligence (AI). However, there are growing concerns about privacy risks, particularly for children. Although existing privacy regulations require companies and organizations to implement protections, doing so can be challenging in practice. To address this challenge, this article proposes a framework based on Privacy-by-Design (PbD), which guides designers and developers to take on a proactive and risk-averse approach to technology design. Our framework includes principles from several privacy regulations, such as the General Data Protection Regulation (GDPR) from the European Union, the Personal Information Protection and Electronic Documents Act (PIPEDA) from Canada, and the Children's Online Privacy Protection Act (COPPA) from the United States. We map these principles to various stages of applications that use Large Language Models (LLMs), including data collection, model training, operational monitoring, and ongoing validation. For each stage, we discuss the operational controls found in the recent academic literature to help AI service providers and developers reduce privacy risks while meeting legal standards. In addition, the framework includes design guidelines for children, drawing from the United Nations Convention on the Rights of the Child (UNCRC), the UK's Age-Appropriate Design Code (AADC), and recent academic research. To demonstrate how this framework can be applied in practice, we present a case study of an LLM-based educational tutor for children under 13. Through our analysis and the case study, we show that by using data protection strategies such as technical and organizational controls and making age-appropriate design decisions throughout the LLM life cycle, we can support the development of AI applications for children that provide privacy protections and comply with legal requirements.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "pdf_url": "https://arxiv.org/pdf/2602.17418v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.17196v1",
      "title": "EntropyPrune: Matrix Entropy Guided Visual Token Pruning for Multimodal Large Language Models",
      "authors": [
        "Yahong Wang",
        "Juncheng Wu",
        "Zhangkai Ni",
        "Chengmei Yang",
        "Yihang Liu",
        "Longzhen Yang",
        "Yuyin Zhou",
        "Ying Wen",
        "Lianghua He"
      ],
      "abstract": "Multimodal large language models (MLLMs) incur substantial inference cost due to the processing of hundreds of visual tokens per image. Although token pruning has proven effective for accelerating inference, determining when and where to prune remains largely heuristic. Existing approaches typically rely on static, empirically selected layers, which limit interpretability and transferability across models. In this work, we introduce a matrix-entropy perspective and identify an \"Entropy Collapse Layer\" (ECL), where the information content of visual representations exhibits a sharp and consistent drop, which provides a principled criterion for selecting the pruning stage. Building on this observation, we propose EntropyPrune, a novel matrix-entropy-guided token pruning framework that quantifies the information value of individual visual tokens and prunes redundant ones without relying on attention maps. Moreover, to enable efficient computation, we exploit the spectral equivalence of dual Gram matrices, reducing the complexity of entropy computation and yielding up to a 64x theoretical speedup. Extensive experiments on diverse multimodal benchmarks demonstrate that EntropyPrune consistently outperforms state-of-the-art pruning methods in both accuracy and efficiency. On LLaVA-1.5-7B, our method achieves a 68.2% reduction in FLOPs while preserving 96.0% of the original performance. Furthermore, EntropyPrune generalizes effectively to high-resolution and video-based models, highlighting the strong robustness and scalability in practical MLLM acceleration. The code will be publicly available at https://github.com/YahongWang1/EntropyPrune.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "pdf_url": "https://arxiv.org/pdf/2602.17196v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.17183v1",
      "title": "Robustness and Reasoning Fidelity of Large Language Models in Long-Context Code Question Answering",
      "authors": [
        "Kishan Maharaj",
        "Nandakishore Menon",
        "Ashita Saxena",
        "Srikanth Tamilselvam"
      ],
      "abstract": "Large language models (LLMs) increasingly assist software engineering tasks that require reasoning over long code contexts, yet their robustness under varying input conditions remains unclear. We conduct a systematic study of long-context code question answering using controlled ablations that test sensitivity to answer format, distractors, and context scale. Extending LongCodeBench Python dataset with new COBOL and Java question-answer sets, we evaluate state-of-the-art models under three settings: (i) shuffled multiple-choice options, (ii) open-ended questions and (iii) needle-in-a-haystack contexts containing relevant and adversarially irrelevant information. Results show substantial performance drops in both shuffled multiple-choice options and open-ended questions, and brittle behavior in the presence of irrelevant cues. Our findings highlight limitations of current long-context evaluations and provide a broader benchmark for assessing code reasoning in both legacy and modern systems.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "pdf_url": "https://arxiv.org/pdf/2602.17183v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.17169v1",
      "title": "SimulatorCoder: DNN Accelerator Simulator Code Generation and Optimization via Large Language Models",
      "authors": [
        "Yuhuan Xia",
        "Tun Li",
        "Hongji Zhou",
        "Xianfa Zhou",
        "Chong Chen",
        "Ruiyu Zhang"
      ],
      "abstract": "This paper presents SimulatorCoder, an agent powered by large language models (LLMs), designed to generate and optimize deep neural network (DNN) accelerator simulators based on natural language descriptions. By integrating domain-specific prompt engineering including In-Context Learning (ICL), Chain-of-Thought (CoT) reasoning, and a multi-round feedback-verification flow, SimulatorCoder systematically transforms high-level functional requirements into efficient, executable, and architecture-aligned simulator code. Experiments based on the customized SCALE-Sim benchmark demonstrate that structured prompting and feedback mechanisms substantially improve both code generation accuracy and simulator performance. The resulting simulators not only maintain cycle-level fidelity with less than 1% error compared to manually implemented counterparts, but also consistently achieve lower simulation runtimes, highlighting the effectiveness of LLM-based methods in accelerating simulator development. Our code is available at https://github.com/xiayuhuan/SimulatorCoder.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "pdf_url": "https://arxiv.org/pdf/2602.17169v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.17045v1",
      "title": "Large Language Models Persuade Without Planning Theory of Mind",
      "authors": [
        "Jared Moore",
        "Rasmus Overmark",
        "Ned Cooper",
        "Beba Cibralic",
        "Nick Haber",
        "Cameron R. Jones"
      ],
      "abstract": "A growing body of work attempts to evaluate the theory of mind (ToM) abilities of humans and large language models (LLMs) using static, non-interactive question-and-answer benchmarks. However, theoretical work in the field suggests that first-personal interaction is a crucial part of ToM and that such predictive, spectatorial tasks may fail to evaluate it. We address this gap with a novel ToM task that requires an agent to persuade a target to choose one of three policy proposals by strategically revealing information. Success depends on a persuader's sensitivity to a given target's knowledge states (what the target knows about the policies) and motivational states (how much the target values different outcomes). We varied whether these states were Revealed to persuaders or Hidden, in which case persuaders had to inquire about or infer them. In Experiment 1, participants persuaded a bot programmed to make only rational inferences. LLMs excelled in the Revealed condition but performed below chance in the Hidden condition, suggesting difficulty with the multi-step planning required to elicit and use mental state information. Humans performed moderately well in both conditions, indicating an ability to engage such planning. In Experiment 2, where a human target role-played the bot, and in Experiment 3, where we measured whether human targets' real beliefs changed, LLMs outperformed human persuaders across all conditions. These results suggest that effective persuasion can occur without explicit ToM reasoning (e.g., through rhetorical strategies) and that LLMs excel at this form of persuasion. Overall, our results caution against attributing human-like ToM to LLMs while highlighting LLMs' potential to influence people's beliefs and behavior.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "pdf_url": "https://arxiv.org/pdf/2602.17045v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.16977v1",
      "title": "Fail-Closed Alignment for Large Language Models",
      "authors": [
        "Zachary Coalson",
        "Beth Sohler",
        "Aiden Gabriel",
        "Sanghyun Hong"
      ],
      "abstract": "We identify a structural weakness in current large language model (LLM) alignment: modern refusal mechanisms are fail-open. While existing approaches encode refusal behaviors across multiple latent features, suppressing a single dominant feature$-$via prompt-based jailbreaks$-$can cause alignment to collapse, leading to unsafe generation. Motivated by this, we propose fail-closed alignment as a design principle for robust LLM safety: refusal mechanisms should remain effective even under partial failures via redundant, independent causal pathways. We present a concrete instantiation of this principle: a progressive alignment framework that iteratively identifies and ablates previously learned refusal directions, forcing the model to reconstruct safety along new, independent subspaces. Across four jailbreak attacks, we achieve the strongest overall robustness while mitigating over-refusal and preserving generation quality, with small computational overhead. Our mechanistic analyses confirm that models trained with our method encode multiple, causally independent refusal directions that prompt-based jailbreaks cannot suppress simultaneously, providing empirical support for fail-closed alignment as a principled foundation for robust LLM safety.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "pdf_url": "https://arxiv.org/pdf/2602.16977v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.17665v1",
      "title": "OpenEarthAgent: A Unified Framework for Tool-Augmented Geospatial Agents",
      "authors": [
        "Akashah Shabbir",
        "Muhammad Umer Sheikh",
        "Muhammad Akhtar Munir",
        "Hiyam Debary",
        "Mustansar Fiaz",
        "Muhammad Zaigham Zaheer",
        "Paolo Fraccaro",
        "Fahad Shahbaz Khan",
        "Muhammad Haris Khan",
        "Xiao Xiang Zhu",
        "Salman Khan"
      ],
      "abstract": "Recent progress in multimodal reasoning has enabled agents that can interpret imagery, connect it with language, and perform structured analytical tasks. Extending such capabilities to the remote sensing domain remains challenging, as models must reason over spatial scale, geographic structures, and multispectral indices while maintaining coherent multi-step logic. To bridge this gap, OpenEarthAgent introduces a unified framework for developing tool-augmented geospatial agents trained on satellite imagery, natural-language queries, and detailed reasoning traces. The training pipeline relies on supervised fine-tuning over structured reasoning trajectories, aligning the model with verified multistep tool interactions across diverse analytical contexts. The accompanying corpus comprises 14,538 training and 1,169 evaluation instances, with more than 100K reasoning steps in the training split and over 7K reasoning steps in the evaluation split. It spans urban, environmental, disaster, and infrastructure domains, and incorporates GIS-based operations alongside index analyses such as NDVI, NBR, and NDBI. Grounded in explicit reasoning traces, the learned agent demonstrates structured reasoning, stable spatial understanding, and interpretable behaviour through tool-driven geospatial interactions across diverse conditions. We report consistent improvements over a strong baseline and competitive performance relative to recent open and closed-source models.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "pdf_url": "https://arxiv.org/pdf/2602.17665v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.17641v1",
      "title": "FAMOSE: A ReAct Approach to Automated Feature Discovery",
      "authors": [
        "Keith Burghardt",
        "Jienan Liu",
        "Sadman Sakib",
        "Yuning Hao",
        "Bo Li"
      ],
      "abstract": "Feature engineering remains a critical yet challenging bottleneck in machine learning, particularly for tabular data, as identifying optimal features from an exponentially large feature space traditionally demands substantial domain expertise. To address this challenge, we introduce FAMOSE (Feature AugMentation and Optimal Selection agEnt), a novel framework that leverages the ReAct paradigm to autonomously explore, generate, and refine features while integrating feature selection and evaluation tools within an agent architecture. To our knowledge, FAMOSE represents the first application of an agentic ReAct framework to automated feature engineering, especially for both regression and classification tasks. Extensive experiments demonstrate that FAMOSE is at or near the state-of-the-art on classification tasks (especially tasks with more than 10K instances, where ROC-AUC increases 0.23% on average), and achieves the state-of-the-art for regression tasks by reducing RMSE by 2.0% on average, while remaining more robust to errors than other algorithms. We hypothesize that FAMOSE's strong performance is because ReAct allows the LLM context window to record (via iterative feature discovery and evaluation steps) what features did or did not work. This is similar to a few-shot prompt and guides the LLM to invent better, more innovative features. Our work offers evidence that AI agents are remarkably effective in solving problems that require highly inventive solutions, such as feature engineering.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "pdf_url": "https://arxiv.org/pdf/2602.17641v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.17622v1",
      "title": "What Makes a Good LLM Agent for Real-world Penetration Testing?",
      "authors": [
        "Gelei Deng",
        "Yi Liu",
        "Yuekang Li",
        "Ruozhao Yang",
        "Xiaofei Xie",
        "Jie Zhang",
        "Han Qiu",
        "Tianwei Zhang"
      ],
      "abstract": "LLM-based agents show promise for automating penetration testing, yet reported performance varies widely across systems and benchmarks. We analyze 28 LLM-based penetration testing systems and evaluate five representative implementations across three benchmarks of increasing complexity. Our analysis reveals two distinct failure modes: Type A failures stem from capability gaps (missing tools, inadequate prompts) that engineering readily addresses, while Type B failures persist regardless of tooling due to planning and state management limitations. We show that Type B failures share a root cause that is largely invariant to the underlying LLM: agents lack real-time task difficulty estimation. As a result, agents misallocate effort, over-commit to low-value branches, and exhaust context before completing attack chains.   Based on this insight, we present Excalibur, a penetration testing agent that couples strong tooling with difficulty-aware planning. A Tool and Skill Layer eliminates Type A failures through typed interfaces and retrieval-augmented knowledge. A Task Difficulty Assessment (TDA) mechanism addresses Type B failures by estimating tractability through four measurable dimensions (horizon estimation, evidence confidence, context load, and historical success) and uses these estimates to guide exploration-exploitation decisions within an Evidence-Guided Attack Tree Search (EGATS) framework. Excalibur achieves up to 91% task completion on CTF benchmarks with frontier models (39 to 49% relative improvement over baselines) and compromises 4 of 5 hosts on the GOAD Active Directory environment versus 2 by prior systems. These results show that difficulty-aware planning yields consistent end-to-end gains across models and addresses a limitation that model scaling alone does not eliminate.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "pdf_url": "https://arxiv.org/pdf/2602.17622v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.17607v1",
      "title": "AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing",
      "authors": [
        "Jianda Du",
        "Youran Sun",
        "Haizhao Yang"
      ],
      "abstract": "PDEs are central to scientific and engineering modeling, yet designing accurate numerical solvers typically requires substantial mathematical expertise and manual tuning. Recent neural network-based approaches improve flexibility but often demand high computational cost and suffer from limited interpretability. We introduce \\texttt{AutoNumerics}, a multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for general PDEs directly from natural language descriptions. Unlike black-box neural solvers, our framework generates transparent solvers grounded in classical numerical analysis. We introduce a coarse-to-fine execution strategy and a residual-based self-verification mechanism. Experiments on 24 canonical and real-world PDE problems demonstrate that \\texttt{AutoNumerics} achieves competitive or superior accuracy compared to existing neural and LLM-based baselines, and correctly selects numerical schemes based on PDE structural properties, suggesting its viability as an accessible paradigm for automated PDE solving.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "pdf_url": "https://arxiv.org/pdf/2602.17607v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.17588v1",
      "title": "Modeling Distinct Human Interaction in Web Agents",
      "authors": [
        "Faria Huq",
        "Zora Zhiruo Wang",
        "Zhanqiu Guo",
        "Venu Arvind Arangarajan",
        "Tianyue Ou",
        "Frank Xu",
        "Shuyan Zhou",
        "Graham Neubig",
        "Jeffrey P. Bigham"
      ],
      "abstract": "Despite rapid progress in autonomous web agents, human involvement remains essential for shaping preferences and correcting agent behavior as tasks unfold. However, current agentic systems lack a principled understanding of when and why humans intervene, often proceeding autonomously past critical decision points or requesting unnecessary confirmation. In this work, we introduce the task of modeling human intervention to support collaborative web task execution. We collect CowCorpus, a dataset of 400 real-user web navigation trajectories containing over 4,200 interleaved human and agent actions. We identify four distinct patterns of user interaction with agents -- hands-off supervision, hands-on oversight, collaborative task-solving, and full user takeover. Leveraging these insights, we train language models (LMs) to anticipate when users are likely to intervene based on their interaction styles, yielding a 61.4-63.4% improvement in intervention prediction accuracy over base LMs. Finally, we deploy these intervention-aware models in live web navigation agents and evaluate them in a user study, finding a 26.5% increase in user-rated agent usefulness. Together, our results show structured modeling of human intervention leads to more adaptive, collaborative agents.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "pdf_url": "https://arxiv.org/pdf/2602.17588v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.17558v1",
      "title": "RetouchIQ: MLLM Agents for Instruction-Based Image Retouching with Generalist Reward",
      "authors": [
        "Qiucheng Wu",
        "Jing Shi",
        "Simon Jenni",
        "Kushal Kafle",
        "Tianyu Wang",
        "Shiyu Chang",
        "Handong Zhao"
      ],
      "abstract": "Recent advances in multimodal large language models (MLLMs) have shown great potential for extending vision-language reasoning to professional tool-based image editing, enabling intuitive and creative editing. A promising direction is to use reinforcement learning (RL) to enable MLLMs to reason about and execute optimal tool-use plans within professional image-editing software. However, training remains challenging due to the lack of reliable, verifiable reward signals that can reflect the inherently subjective nature of creative editing. In this work, we introduce RetouchIQ, a framework that performs instruction-based executable image editing through MLLM agents guided by a generalist reward model. RetouchIQ interprets user-specified editing intentions and generates corresponding, executable image adjustments, bridging high-level aesthetic goals with precise parameter control. To move beyond conventional, rule-based rewards that compute similarity against a fixed reference image using handcrafted metrics, we propose a generalist reward model, an RL fine-tuned MLLM that evaluates edited results through a set of generated metrics on a case-by-case basis. Then, the reward model provides scalar feedback through multimodal reasoning, enabling reinforcement learning with high-quality, instruction-consistent gradients. We curate an extended dataset with 190k instruction-reasoning pairs and establish a new benchmark for instruction-based image editing. Experiments show that RetouchIQ substantially improves both semantic consistency and perceptual quality over previous MLLM-based and diffusion-based editing systems. Our findings demonstrate the potential of generalist reward-driven MLLM agents as flexible, explainable, and executable assistants for professional image editing.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "pdf_url": "https://arxiv.org/pdf/2602.17558v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.17547v1",
      "title": "KLong: Training LLM Agent for Extremely Long-horizon Tasks",
      "authors": [
        "Yue Liu",
        "Zhiyuan Hu",
        "Flood Sung",
        "Jiaheng Zhang",
        "Bryan Hooi"
      ],
      "abstract": "This paper introduces KLong, an open-source LLM agent trained to solve extremely long-horizon tasks. The principle is to first cold-start the model via trajectory-splitting SFT, then scale it via progressive RL training. Specifically, we first activate basic agentic abilities of a base model with a comprehensive SFT recipe. Then, we introduce Research-Factory, an automated pipeline that generates high-quality training data by collecting research papers and constructing evaluation rubrics. Using this pipeline, we build thousands of long-horizon trajectories distilled from Claude 4.5 Sonnet (Thinking). To train with these extremely long trajectories, we propose a new trajectory-splitting SFT, which preserves early context, progressively truncates later context, and maintains overlap between sub-trajectories. In addition, to further improve long-horizon task-solving capability, we propose a novel progressive RL, which schedules training into multiple stages with progressively extended timeouts. Experiments demonstrate the superiority and generalization of KLong, as shown in Figure 1. Notably, our proposed KLong (106B) surpasses Kimi K2 Thinking (1T) by 11.28% on PaperBench, and the performance improvement generalizes to other coding benchmarks like SWE-bench Verified and MLE-bench.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "pdf_url": "https://arxiv.org/pdf/2602.17547v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.17518v1",
      "title": "A Picture of Agentic Search",
      "authors": [
        "Francesca Pezzuti",
        "Ophir Frieder",
        "Fabrizio Silvestri",
        "Sean MacAvaney",
        "Nicola Tonellotto"
      ],
      "abstract": "With automated systems increasingly issuing search queries alongside humans, Information Retrieval (IR) faces a major shift. Yet IR remains human-centred, with systems, evaluation metrics, user models, and datasets designed around human queries and behaviours. Consequently, IR operates under assumptions that no longer hold in practice, with changes to workload volumes, predictability, and querying behaviours. This misalignment affects system performance and optimisation: caching may lose effectiveness, query pre-processing may add overhead without improving results, and standard metrics may mismeasure satisfaction. Without adaptation, retrieval models risk satisfying neither humans, nor the emerging user segment of agents. However, datasets capturing agent search behaviour are lacking, which is a critical gap given IR's historical reliance on data-driven evaluation and optimisation. We develop a methodology for collecting all the data produced and consumed by agentic retrieval-augmented systems when answering queries, and we release the Agentic Search Queryset (ASQ) dataset. ASQ contains reasoning-induced queries, retrieved documents, and thoughts for queries in HotpotQA, Researchy Questions, and MS MARCO, for 3 diverse agents and 2 retrieval pipelines. The accompanying toolkit enables ASQ to be extended to new agents, retrievers, and datasets.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "pdf_url": "https://arxiv.org/pdf/2602.17518v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.17434v1",
      "title": "Multi-Agent Temporal Logic Planning via Penalty Functions and Block-Coordinate Optimization",
      "authors": [
        "Eleftherios E. Vlahakis",
        "Arash Bahari Kordabad",
        "Lars Lindemann",
        "Pantelis Sopasakis",
        "Sadegh Soudjani",
        "Dimos V. Dimarogonas"
      ],
      "abstract": "Multi-agent planning under Signal Temporal Logic (STL) is often hindered by collaborative tasks that lead to computational challenges due to the inherent high-dimensionality of the problem, preventing scalable synthesis with satisfaction guarantees. To address this, we formulate STL planning as an optimization program under arbitrary multi-agent constraints and introduce a penalty-based unconstrained relaxation that can be efficiently solved via a Block-Coordinate Gradient Descent (BCGD) method, where each block corresponds to a single agent's decision variables, thereby mitigating complexity. By utilizing a quadratic penalty function defined via smooth STL semantics, we show that BCGD iterations converge to a stationary point of the penalized problem under standard regularity assumptions. To enforce feasibility, the BCGD solver is embedded within a two-layer optimization scheme: inner BCGD updates are performed for a fixed penalty parameter, which is then increased in an outer loop to progressively improve multi-agent STL robustness. The proposed framework enables scalable computations and is validated through various complex multi-robot planning scenarios.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "pdf_url": "https://arxiv.org/pdf/2602.17434v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.17308v1",
      "title": "MedClarify: An information-seeking AI agent for medical diagnosis with case-specific follow-up questions",
      "authors": [
        "Hui Min Wong",
        "Philip Heesen",
        "Pascal Janetzky",
        "Martin Bendszus",
        "Stefan Feuerriegel"
      ],
      "abstract": "Large language models (LLMs) are increasingly used for diagnostic tasks in medicine. In clinical practice, the correct diagnosis can rarely be immediately inferred from the initial patient presentation alone. Rather, reaching a diagnosis often involves systematic history taking, during which clinicians reason over multiple potential conditions through iterative questioning to resolve uncertainty. This process requires considering differential diagnoses and actively excluding emergencies that demand immediate intervention. Yet, the ability of medical LLMs to generate informative follow-up questions and thus reason over differential diagnoses remains underexplored. Here, we introduce MedClarify, an AI agent for information-seeking that can generate follow-up questions for iterative reasoning to support diagnostic decision-making. Specifically, MedClarify computes a list of candidate diagnoses analogous to a differential diagnosis, and then proactively generates follow-up questions aimed at reducing diagnostic uncertainty. By selecting the question with the highest expected information gain, MedClarify enables targeted, uncertainty-aware reasoning to improve diagnostic performance. In our experiments, we first demonstrate the limitations of current LLMs in medical reasoning, which often yield multiple, similarly likely diagnoses, especially when patient cases are incomplete or relevant information for diagnosis is missing. We then show that our information-theoretic reasoning approach can generate effective follow-up questioning and thereby reduces diagnostic errors by ~27 percentage points (p.p.) compared to a standard single-shot LLM baseline. Altogether, MedClarify offers a path to improve medical LLMs through agentic information-seeking and to thus promote effective dialogues with medical LLMs that reflect the iterative and uncertain nature of real-world clinical reasoning.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "pdf_url": "https://arxiv.org/pdf/2602.17308v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.17282v1",
      "title": "Visual Insights into Agentic Optimization of Pervasive Stream Processing Services",
      "authors": [
        "Boris Sedlak",
        "Víctor Casamayor Pujol",
        "Schahram Dustdar"
      ],
      "abstract": "Processing sensory data close to the data source, often involving Edge devices, promises low latency for pervasive applications, like smart cities. This commonly involves a multitude of processing services, executed with limited resources; this setup faces three problems: first, the application demand and the resource availability fluctuate, so the service execution must scale dynamically to sustain processing requirements (e.g., latency); second, each service permits different actions to adjust its operation, so they require individual scaling policies; third, without a higher-level mediator, services would cannibalize any resources of services co-located on the same device. This demo first presents a platform for context-aware autoscaling of stream processing services that allows developers to monitor and adjust the service execution across multiple service-specific parameters. We then connect a scaling agent to these interfaces that gradually builds an understanding of the processing environment by exploring each service's action space; the agent then optimizes the service execution according to this knowledge. Participants can revisit the demo contents as video summary and introductory poster, or build a custom agent by extending the artifact repository.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "pdf_url": "https://arxiv.org/pdf/2602.17282v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.17245v1",
      "title": "Web Verbs: Typed Abstractions for Reliable Task Composition on the Agentic Web",
      "authors": [
        "Linxi Jiang",
        "Rui Xi",
        "Zhijie Liu",
        "Shuo Chen",
        "Zhiqiang Lin",
        "Suman Nath"
      ],
      "abstract": "The Web is evolving from a medium that humans browse to an environment where software agents act on behalf of users. Advances in large language models (LLMs) make natural language a practical interface for goal-directed tasks, yet most current web agents operate on low-level primitives such as clicks and keystrokes. These operations are brittle, inefficient, and difficult to verify. Complementing content-oriented efforts such as NLWeb's semantic layer for retrieval, we argue that the agentic web also requires a semantic layer for web actions. We propose \\textbf{Web Verbs}, a web-scale set of typed, semantically documented functions that expose site capabilities through a uniform interface, whether implemented through APIs or robust client-side workflows. These verbs serve as stable and composable units that agents can discover, select, and synthesize into concise programs. This abstraction unifies API-based and browser-based paradigms, enabling LLMs to synthesize reliable and auditable workflows with explicit control and data flow. Verbs can carry preconditions, postconditions, policy tags, and logging support, which improves \\textbf{reliability} by providing stable interfaces, \\textbf{efficiency} by reducing dozens of steps into a few function calls, and \\textbf{verifiability} through typed contracts and checkable traces. We present our vision, a proof-of-concept implementation, and representative case studies that demonstrate concise and robust execution compared to existing agents. Finally, we outline a roadmap for standardization to make verbs deployable and trustworthy at web scale.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "pdf_url": "https://arxiv.org/pdf/2602.17245v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.17221v1",
      "title": "From Labor to Collaboration: A Methodological Experiment Using AI Agents to Augment Research Perspectives in Taiwan's Humanities and Social Sciences",
      "authors": [
        "Yi-Chih Huang"
      ],
      "abstract": "Generative AI is reshaping knowledge work, yet existing research focuses predominantly on software engineering and the natural sciences, with limited methodological exploration for the humanities and social sciences. Positioned as a \"methodological experiment,\" this study proposes an AI Agent-based collaborative research workflow (Agentic Workflow) for humanities and social science research. Taiwan's Claude.ai usage data (N = 7,729 conversations, November 2025) from the Anthropic Economic Index (AEI) serves as the empirical vehicle for validating the feasibility of this methodology.   This study operates on two levels: the primary level is the design and validation of a methodological framework - a seven-stage modular workflow grounded in three principles: task modularization, human-AI division of labor, and verifiability, with each stage delineating clear roles for human researchers (research judgment and ethical decisions) and AI Agents (information retrieval and text generation); the secondary level is the empirical analysis of AEI Taiwan data - serving as an operational demonstration of the workflow's application to secondary data research, showcasing both the process and output quality (see Appendix A).   This study contributes by proposing a replicable AI collaboration framework for humanities and social science researchers, and identifying three operational modes of human-AI collaboration - direct execution, iterative refinement, and human-led - through reflexive documentation of the operational process. This taxonomy reveals the irreplaceability of human judgment in research question formulation, theoretical interpretation, contextualized reasoning, and ethical reflection. Limitations including single-platform data, cross-sectional design, and AI reliability risks are acknowledged.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "pdf_url": "https://arxiv.org/pdf/2602.17221v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.17185v1",
      "title": "The Bots of Persuasion: Examining How Conversational Agents' Linguistic Expressions of Personality Affect User Perceptions and Decisions",
      "authors": [
        "Uğur Genç",
        "Heng Gu",
        "Chadha Degachi",
        "Evangelos Niforatos",
        "Senthil Chandrasegaran",
        "Himanshu Verma"
      ],
      "abstract": "Large Language Model-powered conversational agents (CAs) are increasingly capable of projecting sophisticated personalities through language, but how these projections affect users is unclear. We thus examine how CA personalities expressed linguistically affect user decisions and perceptions in the context of charitable giving. In a crowdsourced study, 360 participants interacted with one of eight CAs, each projecting a personality composed of three linguistic aspects: attitude (optimistic/pessimistic), authority (authoritative/submissive), and reasoning (emotional/rational). While the CA's composite personality did not affect participants' decisions, it did affect their perceptions and emotional responses. Particularly, participants interacting with pessimistic CAs felt lower emotional state and lower affinity towards the cause, perceived the CA as less trustworthy and less competent, and yet tended to donate more toward the charity. Perceptions of trust, competence, and situational empathy significantly predicted donation decisions. Our findings emphasize the risks CAs pose as instruments of manipulation, subtly influencing user perceptions and decisions.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "pdf_url": "https://arxiv.org/pdf/2602.17185v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.17103v1",
      "title": "Online Learning with Improving Agents: Multiclass, Budgeted Agents and Bandit Learners",
      "authors": [
        "Sajad Ashkezari",
        "Shai Ben-David"
      ],
      "abstract": "We investigate the recently introduced model of learning with improvements, where agents are allowed to make small changes to their feature values to be warranted a more desirable label. We extensively extend previously published results by providing combinatorial dimensions that characterize online learnability in this model, by analyzing the multiclass setup, learnability in a bandit feedback setup, modeling agents' cost for making improvements and more.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "pdf_url": "https://arxiv.org/pdf/2602.17103v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.16928v1",
      "title": "Discovering Multiagent Learning Algorithms with Large Language Models",
      "authors": [
        "Zun Li",
        "John Schultz",
        "Daniel Hennes",
        "Marc Lanctot"
      ],
      "abstract": "Much of the advancement of Multi-Agent Reinforcement Learning (MARL) in imperfect-information games has historically depended on manual iterative refinement of baselines. While foundational families like Counterfactual Regret Minimization (CFR) and Policy Space Response Oracles (PSRO) rest on solid theoretical ground, the design of their most effective variants often relies on human intuition to navigate a vast algorithmic design space. In this work, we propose the use of AlphaEvolve, an evolutionary coding agent powered by large language models, to automatically discover new multiagent learning algorithms. We demonstrate the generality of this framework by evolving novel variants for two distinct paradigms of game-theoretic learning. First, in the domain of iterative regret minimization, we evolve the logic governing regret accumulation and policy derivation, discovering a new algorithm, Volatility-Adaptive Discounted (VAD-)CFR. VAD-CFR employs novel, non-intuitive mechanisms-including volatility-sensitive discounting, consistency-enforced optimism, and a hard warm-start policy accumulation schedule-to outperform state-of-the-art baselines like Discounted Predictive CFR+. Second, in the regime of population based training algorithms, we evolve training-time and evaluation-time meta strategy solvers for PSRO, discovering a new variant, Smoothed Hybrid Optimistic Regret (SHOR-)PSRO. SHOR-PSRO introduces a hybrid meta-solver that linearly blends Optimistic Regret Matching with a smoothed, temperature-controlled distribution over best pure strategies. By dynamically annealing this blending factor and diversity bonuses during training, the algorithm automates the transition from population diversity to rigorous equilibrium finding, yielding superior empirical convergence compared to standard static meta-solvers.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "pdf_url": "https://arxiv.org/pdf/2602.16928v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.16926v1",
      "title": "BEMEval-Doc2Schema: Benchmarking Large Language Models for Structured Data Extraction in Building Energy Modeling",
      "authors": [
        "Yiyuan Jia",
        "Xiaoqin Fu",
        "Liang Zhang"
      ],
      "abstract": "Recent advances in foundation models, including large language models (LLMs), have created new opportunities to automate building energy modeling (BEM). However, systematic evaluation has remained challenging due to the absence of publicly available, task-specific datasets and standardized performance metrics. We present BEMEval, a benchmark framework designed to assess foundation models' performance across BEM tasks. The first benchmark in this suite, BEMEval-Doc2Schema, focuses on structured data extraction from building documentation, a foundational step toward automated BEM processes. BEMEval-Doc2Schema introduces the Key-Value Overlap Rate (KVOR), a metric that quantifies the alignment between LLM-generated structured outputs and ground-truth schema references. Using this framework, we evaluate two leading models (GPT-5 and Gemini 2.5) under zero-shot and few-shot prompting strategies across three datasets: HERS L100, NREL iUnit, and NIST NZERTF. Results show that Gemini 2.5 consistently outperforms GPT-5, and that few-shot prompts improve accuracy for both models. Performance also varies by schema: the EPC schema yields significantly higher KVOR scores than HPXML, reflecting its simpler and reduced hierarchical depth. By combining curated datasets, reproducible metrics, and cross-model comparisons, BEMEval-Doc2Schema establishes the first community-driven benchmark for evaluating LLMs in performing building energy modeling tasks, laying the groundwork for future research on AI-assisted BEM workflows.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "pdf_url": "https://arxiv.org/pdf/2602.16926v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.16852v1",
      "title": "Meenz bleibt Meenz, but Large Language Models Do Not Speak Its Dialect",
      "authors": [
        "Minh Duc Bui",
        "Manuel Mager",
        "Peter Herbert Kann",
        "Katharina von der Wense"
      ],
      "abstract": "Meenzerisch, the dialect spoken in the German city of Mainz, is also the traditional language of the Mainz carnival, a yearly celebration well known throughout Germany. However, Meenzerisch is on the verge of dying out-a fate it shares with many other German dialects. Natural language processing (NLP) has the potential to help with the preservation and revival efforts of languages and dialects. However, so far no NLP research has looked at Meenzerisch. This work presents the first research in the field of NLP that is explicitly focused on the dialect of Mainz. We introduce a digital dictionary-an NLP-ready dataset derived from an existing resource (Schramm, 1966)-to support researchers in modeling and benchmarking the language. It contains 2,351 words in the dialect paired with their meanings described in Standard German. We then use this dataset to answer the following research questions: (1) Can state-of-the-art large language models (LLMs) generate definitions for dialect words? (2) Can LLMs generate words in Meenzerisch, given their definitions? Our experiments show that LLMs can do neither: the best model for definitions reaches only 6.27% accuracy and the best word generation model's accuracy is 1.51%. We then conduct two additional experiments in order to see if accuracy is improved by few-shot learning and by extracting rules from the training set, which are then passed to the LLM. While those approaches are able to improve the results, accuracy remains below 10%. This highlights that additional resources and an intensification of research efforts focused on German dialects are desperately needed.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "pdf_url": "https://arxiv.org/pdf/2602.16852v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.16836v1",
      "title": "Claim Automation using Large Language Model",
      "authors": [
        "Zhengda Mo",
        "Zhiyu Quan",
        "Eli O'Donohue",
        "Kaiwen Zhong"
      ],
      "abstract": "While Large Language Models (LLMs) have achieved strong performance on general-purpose language tasks, their deployment in regulated and data-sensitive domains, including insurance, remains limited. Leveraging millions of historical warranty claims, we propose a locally deployed governance-aware language modeling component that generates structured corrective-action recommendations from unstructured claim narratives. We fine-tune pretrained LLMs using Low-Rank Adaptation (LoRA), scoping the model to an initial decision module within the claim processing pipeline to speed up claim adjusters' decisions. We assess this module using a multi-dimensional evaluation framework that combines automated semantic similarity metrics with human evaluation, enabling a rigorous examination of both practical utility and predictive accuracy. Our results show that domain-specific fine-tuning substantially outperforms commercial general-purpose and prompt-based LLMs, with approximately 80% of the evaluated cases achieving near-identical matches to ground-truth corrective actions. Overall, this study provides both theoretical and empirical evidence to prove that domain-adaptive fine-tuning can align model output distributions more closely with real-world operational data, demonstrating its promise as a reliable and governable building block for insurance applications.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "pdf_url": "https://arxiv.org/pdf/2602.16836v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.16811v1",
      "title": "Evaluating Monolingual and Multilingual Large Language Models for Greek Question Answering: The DemosQA Benchmark",
      "authors": [
        "Charalampos Mastrokostas",
        "Nikolaos Giarelis",
        "Nikos Karacapilidis"
      ],
      "abstract": "Recent advancements in Natural Language Processing and Deep Learning have enabled the development of Large Language Models (LLMs), which have significantly advanced the state-of-the-art across a wide range of tasks, including Question Answering (QA). Despite these advancements, research on LLMs has primarily targeted high-resourced languages (e.g., English), and only recently has attention shifted toward multilingual models. However, these models demonstrate a training data bias towards a small number of popular languages or rely on transfer learning from high- to under-resourced languages; this may lead to a misrepresentation of social, cultural, and historical aspects. To address this challenge, monolingual LLMs have been developed for under-resourced languages; however, their effectiveness remains less studied when compared to multilingual counterparts on language-specific tasks. In this study, we address this research gap in Greek QA by contributing: (i) DemosQA, a novel dataset, which is constructed using social media user questions and community-reviewed answers to better capture the Greek social and cultural zeitgeist; (ii) a memory-efficient LLM evaluation framework adaptable to diverse QA datasets and languages; and (iii) an extensive evaluation of 11 monolingual and multilingual LLMs on 6 human-curated Greek QA datasets using 3 different prompting strategies. We release our code and data to facilitate reproducibility.",
      "published": "2026-02-18",
      "updated": "2026-02-18",
      "pdf_url": "https://arxiv.org/pdf/2602.16811v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.07905v1",
      "title": "MedCoG: Maximizing LLM Inference Density in Medical Reasoning via Meta-Cognitive Regulation",
      "authors": [
        "Yu Zhao",
        "Hao Guan",
        "Yongcheng Jing",
        "Ying Zhang",
        "Dacheng Tao"
      ],
      "abstract": "Large Language Models (LLMs) have shown strong potential in complex medical reasoning yet face diminishing gains under inference scaling laws. While existing studies augment LLMs with various knowledge types, it remains unclear how effectively the additional costs translate into accuracy. In this paper, we explore how meta-cognition of LLMs, i.e., their self-awareness of their own knowledge states, can regulate the reasoning process. Specifically, we propose MedCoG, a Medical Meta-Cognition Agent with Knowledge Graph, where the meta-cognitive assessments of task complexity, familiarity, and knowledge density dynamically regulate utilization of procedural, episodic, and factual knowledge. The LLM-centric on-demand reasoning aims to mitigate scaling laws by (1) reducing costs via avoiding indiscriminate scaling, (2) improving accuracy via filtering out distractive knowledge. To validate this, we empirically characterize the scaling curve and introduce inference density to quantify inference efficiency, defined as the ratio of theoretically effective cost to actual cost. Experiments demonstrate the effectiveness and efficiency of MedCoG on five hard sets of medical benchmarks, yielding 5.5x inference density. Furthermore, the Oracle study highlights the significant potential of meta-cognitive regulation.",
      "published": "2026-02-08",
      "updated": "2026-02-08",
      "pdf_url": "https://arxiv.org/pdf/2602.07905v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2602.07529v2",
      "title": "MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution",
      "authors": [
        "Jianwen Chen",
        "Xinyu Yang",
        "Peng Xia",
        "Arian Azarang",
        "Yueh Z Lee",
        "Gang Li",
        "Hongtu Zhu",
        "Yun Li",
        "Beidi Chen",
        "Huaxiu Yao"
      ],
      "abstract": "Large language models (LLMs) have demonstrated strong performance and rapid progress in a wide range of medical reasoning tasks. However, their sequential autoregressive decoding forces inherently parallel clinical reasoning, such as differential diagnosis, into a single linear reasoning path, limiting both efficiency and reliability for complex medical problems. To address this, we propose MedVerse, a reasoning framework for complex medical inference that reformulates medical reasoning as a parallelizable directed acyclic graph (DAG) process based on Petri net theory. The framework adopts a full-stack design across data, model architecture, and system execution. For data creation, we introduce the MedVerse Curator, an automated pipeline that synthesizes knowledge-grounded medical reasoning paths and transforms them into Petri net-structured representations. At the architectural level, we propose a topology-aware attention mechanism with adaptive position indices that supports parallel reasoning while preserving logical consistency. Systematically, we develop a customized inference engine that supports parallel execution without additional overhead. Empirical evaluations show that MedVerse improves strong general-purpose LLMs by up to 8.9%. Compared to specialized medical LLMs, MedVerse achieves comparable performance while delivering a 1.3x reduction in inference latency and a 1.7x increase in generation throughput, enabled by its parallel decoding capability. Code is available at https://github.com/aiming-lab/MedVerse.",
      "published": "2026-02-07",
      "updated": "2026-02-10",
      "pdf_url": "https://arxiv.org/pdf/2602.07529v2",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2601.20221v1",
      "title": "Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning",
      "authors": [
        "Hang Zhang",
        "Ruheng Wang",
        "Yuelyu Ji",
        "Mingu Kwak",
        "Xizhi Wu",
        "Chenyu Li",
        "Li Zhang",
        "Wenqi Shi",
        "Yifan Peng",
        "Yanshan Wang"
      ],
      "abstract": "Large language models have achieved strong performance on medical reasoning benchmarks, yet their deployment in clinical settings demands rigorous verification to ensure factual accuracy. While reward models offer a scalable approach for reasoning trace verification, existing methods face two limitations: they produce only scalar reward values without explicit justification, and they rely on single-pass retrieval that precludes adaptive knowledge access as verification unfolds. We introduce $\\method$, an agentic framework that addresses these limitations by training medical reasoning verifiers to iteratively query external medical corpora during evaluation. Our approach combines tool-augmented verification with an iterative reinforcement learning paradigm that requires only trace-level supervision, alongside an adaptive curriculum mechanism that dynamically adjusts training data distribution. Across four medical reasoning benchmarks, $\\method$ achieves substantial gains over existing methods, improving MedQA accuracy by 23.5% and MedXpertQA by 32.0% relative to the base generator in particular. Crucially, $\\method$ demonstrates an $\\mathbf{8\\times}$ reduction in sampling budget requirement compared to prior reward model baselines. These findings establish that grounding verification in dynamically retrieved evidence offers a principled path toward more reliable medical reasoning systems.",
      "published": "2026-01-28",
      "updated": "2026-01-28",
      "pdf_url": "https://arxiv.org/pdf/2601.20221v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2601.13262v1",
      "title": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning",
      "authors": [
        "Eric Onyame",
        "Akash Ghosh",
        "Subhadip Baidya",
        "Sriparna Saha",
        "Xiuying Chen",
        "Chirag Agarwal"
      ],
      "abstract": "While large language models (LLMs) have shown to perform well on monolingual mathematical and commonsense reasoning, they remain unreliable for multilingual medical reasoning applications, hindering their deployment in multilingual healthcare settings. We address this by first introducing CUREMED-BENCH, a high-quality multilingual medical reasoning dataset with open-ended reasoning queries with a single verifiable answer, spanning thirteen languages, including underrepresented languages such as Amharic, Yoruba, and Swahili. Building on this dataset, we propose CURE-MED, a curriculum-informed reinforcement learning framework that integrates code-switching-aware supervised fine-tuning and Group Relative Policy Optimization to jointly improve logical correctness and language stability. Across thirteen languages, our approach consistently outperforms strong baselines and scales effectively, achieving 85.21% language consistency and 54.35% logical correctness at 7B parameters, and 94.96% language consistency and 70.04% logical correctness at 32B parameters. These results support reliable and equitable multilingual medical reasoning in LLMs. The code and dataset are available at https://cure-med.github.io/",
      "published": "2026-01-19",
      "updated": "2026-01-19",
      "pdf_url": "https://arxiv.org/pdf/2601.13262v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2601.08267v2",
      "title": "Med-CoReasoner: Reducing Language Disparities in Medical Reasoning via Language-Informed Co-Reasoning",
      "authors": [
        "Fan Gao",
        "Sherry T. Tong",
        "Jiwoong Sohn",
        "Jiahao Huang",
        "Junfeng Jiang",
        "Ding Xia",
        "Piyalitt Ittichaiwong",
        "Kanyakorn Veerakanjana",
        "Hyunjae Kim",
        "Qingyu Chen",
        "Edison Marrese Taylor",
        "Kazuma Kobayashi",
        "Akkiko Aizawa",
        "Irene Li"
      ],
      "abstract": "While reasoning-enhanced large language models perform strongly on English medical tasks, a persistent multilingual gap remains, with substantially weaker reasoning in local languages, limiting equitable global medical deployment. To bridge this gap, we introduce Med-CoReasoner, a language-informed co-reasoning framework that elicits parallel English and local-language reasoning, abstracts them into structured concepts, and integrates local clinical knowledge into an English logical scaffold via concept-level alignment and retrieval. This design combines the structural robustness of English reasoning with the practice-grounded expertise encoded in local languages. To evaluate multilingual medical reasoning beyond multiple-choice settings, we construct MultiMed-X, a benchmark covering seven languages with expert-annotated long-form question answering and natural language inference tasks, comprising 350 instances per language. Experiments across three benchmarks show that Med-CoReasoner improves multilingual reasoning performance by an average of 5%, with particularly substantial gains in low-resource languages. Moreover, model distillation and expert evaluation analysis further confirm that Med-CoReasoner produces clinically sound and culturally grounded reasoning traces.",
      "published": "2026-01-13",
      "updated": "2026-01-19",
      "pdf_url": "https://arxiv.org/pdf/2601.08267v2",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2512.13510v1",
      "title": "MedCEG: Reinforcing Verifiable Medical Reasoning with Critical Evidence Graph",
      "authors": [
        "Linjie Mu",
        "Yannian Gu",
        "Zhongzhen Huang",
        "Yakun Zhu",
        "Shaoting Zhang",
        "Xiaofan Zhang"
      ],
      "abstract": "Large language models with reasoning capabilities have demonstrated impressive performance across a wide range of domains. In clinical applications, a transparent, step-by-step reasoning process provides physicians with strong evidence to support decision-making. While reinforcement learning has effectively enhanced reasoning performance in medical contexts, the clinical reliability of these reasoning processes remains limited because their accuracy and validity are often overlooked during training. To address this gap, we propose MedCEG, a framework that augments medical language models with clinically valid reasoning pathways by explicitly supervising the reasoning process through a Critical Evidence Graph (CEG). We curate a dataset of challenging clinical cases and algorithmically construct a CEG for each sample to represent a high-quality verifiable reasoning pathway. To guide the reasoning process, we introduce a Clinical Reasoning Procedure Reward, which evaluates Node Coverage, Structural Correctness, and Chain Completeness, thereby providing a holistic assessment of reasoning quality. Experimental results show that MedCEG surpasses existing methods in performance while producing clinically valid reasoning chains, representing a solid advancement in reliable medical AI reasoning. The code and models are available at https://github.com/LinjieMu/MedCEG.",
      "published": "2025-12-15",
      "updated": "2025-12-15",
      "pdf_url": "https://arxiv.org/pdf/2512.13510v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2512.13742v1",
      "title": "DL$^3$M: A Vision-to-Language Framework for Expert-Level Medical Reasoning through Deep Learning and Large Language Models",
      "authors": [
        "Md. Najib Hasan",
        "Imran Ahmad",
        "Sourav Basak Shuvo",
        "Md. Mahadi Hasan Ankon",
        "Sunanda Das",
        "Nazmul Siddique",
        "Hui Wang"
      ],
      "abstract": "Medical image classifiers detect gastrointestinal diseases well, but they do not explain their decisions. Large language models can generate clinical text, yet they struggle with visual reasoning and often produce unstable or incorrect explanations. This leaves a gap between what a model sees and the type of reasoning a clinician expects. We introduce a framework that links image classification with structured clinical reasoning. A new hybrid model, MobileCoAtNet, is designed for endoscopic images and achieves high accuracy across eight stomach-related classes. Its outputs are then used to drive reasoning by several LLMs. To judge this reasoning, we build two expert-verified benchmarks covering causes, symptoms, treatment, lifestyle, and follow-up care. Thirty-two LLMs are evaluated against these gold standards. Strong classification improves the quality of their explanations, but none of the models reach human-level stability. Even the best LLMs change their reasoning when prompts vary. Our study shows that combining DL with LLMs can produce useful clinical narratives, but current LLMs remain unreliable for high-stakes medical decisions. The framework provides a clearer view of their limits and a path for building safer reasoning systems. The complete source code and datasets used in this study are available at https://github.com/souravbasakshuvo/DL3M.",
      "published": "2025-12-14",
      "updated": "2025-12-14",
      "pdf_url": "https://arxiv.org/pdf/2512.13742v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2512.05658v1",
      "title": "Grounded Multilingual Medical Reasoning for Question Answering with Large Language Models",
      "authors": [
        "Pietro Ferrazzi",
        "Aitor Soroa",
        "Rodrigo Agerri"
      ],
      "abstract": "Large Language Models (LLMs) with reasoning capabilities have recently demonstrated strong potential in medical Question Answering (QA). Existing approaches are largely English-focused and primarily rely on distillation from general-purpose LLMs, raising concerns about the reliability of their medical knowledge. In this work, we present a method to generate multilingual reasoning traces grounded in factual medical knowledge. We produce 500k traces in English, Italian, and Spanish, using a retrievalaugmented generation approach over medical information from Wikipedia. The traces are generated to solve medical questions drawn from MedQA and MedMCQA, which we extend to Italian and Spanish. We test our pipeline in both in-domain and outof-domain settings across Medical QA benchmarks, and demonstrate that our reasoning traces improve performance both when utilized via in-context learning (few-shot) and supervised fine-tuning, yielding state-of-the-art results among 8B-parameter LLMs. We believe that these resources can support the development of safer, more transparent clinical decision-support tools in multilingual settings. We release the full suite of resources: reasoning traces, translated QA datasets, Medical-Wikipedia, and fine-tuned models.",
      "published": "2025-12-05",
      "updated": "2025-12-05",
      "pdf_url": "https://arxiv.org/pdf/2512.05658v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2511.23269v1",
      "title": "OctoMed: Data Recipes for State-of-the-Art Multimodal Medical Reasoning",
      "authors": [
        "Timothy Ossowski",
        "Sheng Zhang",
        "Qianchu Liu",
        "Guanghui Qin",
        "Reuben Tan",
        "Tristan Naumann",
        "Junjie Hu",
        "Hoifung Poon"
      ],
      "abstract": "High-quality and carefully curated data is a cornerstone of training medical large language models, as it directly impacts both generalization and robustness to unseen clinical tasks. We investigate strategies for training and data curation to develop a robust multimodal reasoning model in the medical domain. Our work focuses on supervised fine-tuning (SFT) and explores data recipes that leverage structured reasoning traces. Using our proposed data recipe, we scale experiments to a dataset of over 8 million examples and 6.8 billion response tokens, achieving state-of-the-art performance among open-source models across diverse out-of-distribution medical benchmark tasks. Our results further indicate that curating a high-quality, diverse training dataset with varying structured reasoning trace lengths enables the fine-tuned model to self-calibrate its reasoning trajectory lengths based on the downstream task, without explicit supervision. We present key insights, describe the data curation strategy, and outline next steps toward developing robust medical vision-language reasoning system.",
      "published": "2025-11-28",
      "updated": "2025-11-28",
      "pdf_url": "https://arxiv.org/pdf/2511.23269v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2511.00421v1",
      "title": "MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts",
      "authors": [
        "Naoto Iwase",
        "Hiroki Okuyama",
        "Junichiro Iwasawa"
      ],
      "abstract": "Large language models (LLMs) show increasing promise in medical applications, but their ability to detect and correct errors in clinical texts -- a prerequisite for safe deployment -- remains under-evaluated, particularly beyond English. We introduce MedRECT, a cross-lingual benchmark (Japanese/English) that formulates medical error handling as three subtasks: error detection, error localization (sentence extraction), and error correction. MedRECT is built with a scalable, automated pipeline from the Japanese Medical Licensing Examinations (JMLE) and a curated English counterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with comparable error/no-error balance. We evaluate 9 contemporary LLMs spanning proprietary, open-weight, and reasoning families. Key findings: (i) reasoning models substantially outperform standard architectures, with up to 13.5% relative improvement in error detection and 51.0% in sentence extraction; (ii) cross-lingual evaluation reveals 5-10% performance gaps from English to Japanese, with smaller disparities for reasoning models; (iii) targeted LoRA fine-tuning yields asymmetric improvements in error correction performance (Japanese: +0.078, English: +0.168) while preserving reasoning capabilities; and (iv) our fine-tuned model exceeds human expert performance on structured medical error correction tasks. To our knowledge, MedRECT is the first comprehensive cross-lingual benchmark for medical error correction, providing a reproducible framework and resources for developing safer medical LLMs across languages.",
      "published": "2025-11-01",
      "updated": "2025-11-01",
      "pdf_url": "https://arxiv.org/pdf/2511.00421v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2510.03536v2",
      "title": "Triplet-Structured Knowledge Integration for Multi-Turn Medical Reasoning",
      "authors": [
        "Zhaohan Meng",
        "Zaiqiao Meng",
        "Siwei Liu",
        "Iadh Ounis"
      ],
      "abstract": "Large Language Models (LLMs) have shown strong performance on static medical Question Answering (QA) tasks, yet their reasoning often deteriorates in multi-turn clinical dialogues where patient information is scattered across turns. This paper introduces TriMediQ, a triplet-structured approach that enhances the reasoning reliability of LLMs through explicit knowledge integration. TriMediQ first employs a frozen triplet extraction LLM to convert patient responses into clinically grounded triplets, ensuring factual precision via constrained prompting. These triplets are incorporated into a patient-specific Knowledge Graph (KG), from which a trainable projection module consisting of a graph encoder and a projector captures relational dependencies while keeping all LLM parameters frozen. During inference, the projection module guides multi-hop reasoning over the KG, enabling coherent clinical dialogue understanding. Experiments on two interactive medical QA benchmarks show that TriMediQ achieves up to 10.4\\% improvement in accuracy over five existing baselines on the iMedQA dataset. These results demonstrate that structuring patient information as triplets can effectively improve the reasoning capability of LLMs in multi-turn medical QA.",
      "published": "2025-10-03",
      "updated": "2025-10-14",
      "pdf_url": "https://arxiv.org/pdf/2510.03536v2",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2509.23725v2",
      "title": "MedLA: A Logic-Driven Multi-Agent Framework for Complex Medical Reasoning with Large Language Models",
      "authors": [
        "Siqi Ma",
        "Jiajie Huang",
        "Fan Zhang",
        "Jinlin Wu",
        "Yue Shen",
        "Guohui Fan",
        "Zhu Zhang",
        "Zelin Zang"
      ],
      "abstract": "Answering complex medical questions requires not only domain expertise and patient-specific information, but also structured and multi-perspective reasoning. Existing multi-agent approaches often rely on fixed roles or shallow interaction prompts, limiting their ability to detect and resolve fine-grained logical inconsistencies. To address this, we propose \\textsc{MedLA}, a logic-driven multi-agent framework built on large language models. Each agent organizes its reasoning process into an explicit logical tree based on syllogistic triads (major premise, minor premise, and conclusion), enabling transparent inference and premise-level alignment. Agents engage in a multi-round, graph-guided discussion to compare and iteratively refine their logic trees, achieving consensus through error correction and contradiction resolution. We demonstrate that \\textsc{MedLA} consistently outperforms both static role-based systems and single-agent baselines on challenging benchmarks such as MedDDx and standard medical QA tasks. Furthermore, \\textsc{MedLA} scales effectively across both open-source and commercial LLM backbones, achieving state-of-the-art performance and offering a generalizable paradigm for trustworthy medical reasoning.",
      "published": "2025-09-28",
      "updated": "2025-11-19",
      "pdf_url": "https://arxiv.org/pdf/2509.23725v2",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2509.23368v1",
      "title": "MedCritical: Enhancing Medical Reasoning in Small Language Models via Self-Collaborative Correction",
      "authors": [
        "Xinchun Su",
        "Chunxu Luo",
        "Yixuan Li",
        "Weidong Yang",
        "Lipeng Ma"
      ],
      "abstract": "In the field of medicine, complex reasoning tasks such as clinical diagnosis, treatment planning, and medical knowledge integration pose significant challenges, where small language models often underperform compared to large language models like GPT-4 and Deepseek. Recent knowledge distillation-based methods aim to address these issues through teacher-guided error correction, but this LLM as judge approach remains challenging in terms of cost, time, and efficiency. To circumvent this issue, we propose a novel two-stage framework, MedCritical, which uses a small language model fine-tuned by a large teacher model to play against itself. In the first stage, we extract high-level and detailed long-chain thought templates from the teacher model to guide the student model to generate more complex reasoning thoughts. In the second stage, we introduce direct preference optimization (DPO) through model self-iteration collaboration to enhance the reasoning ability of the student model by playing against the correction trajectory of the fine-tuned model during training. This model self-learning DPO approach teaches the student model to use its own error-driven insights to consolidate its skills and knowledge to solve complex problems, and achieves comparable results to traditional knowledge distillation methods using teacher models at a lower cost. Notably, our MedCritical 7B model outperforms the Taiyi and Huatuo-o1-7B models by 3.04\\% and 10.12\\% respectively on the CMExam benchmark, achieving new SOTA performance among 7B-class small models.",
      "published": "2025-09-27",
      "updated": "2025-09-27",
      "pdf_url": "https://arxiv.org/pdf/2509.23368v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2509.22713v1",
      "title": "RAR$^2$: Retrieval-Augmented Medical Reasoning via Thought-Driven Retrieval",
      "authors": [
        "Kaishuai Xu",
        "Wenjun Hou",
        "Yi Cheng",
        "Wenjie Li"
      ],
      "abstract": "Large Language Models (LLMs) have shown promising performance on diverse medical benchmarks, highlighting their potential in supporting real-world clinical tasks. Retrieval-Augmented Generation (RAG) has emerged as a key approach for mitigating knowledge gaps and hallucinations by incorporating external medical information. However, RAG still struggles with complex medical questions that require intensive reasoning, as surface-level input often fails to reflect the true knowledge needs of the task. Existing methods typically focus on refining queries without explicitly modeling the reasoning process, limiting their ability to retrieve and integrate clinically relevant knowledge. In this work, we propose RAR$^2$, a joint learning framework that improves both Reasoning-Augmented Retrieval and Retrieval-Augmented Reasoning. RAR$^2$ constructs a thought process to uncover implicit knowledge requirements and uses it to guide retrieval and answer generation. We build a training dataset of mixed preference pairs and apply Direct Preference Optimization (DPO) to train the model. Moreover, we design two test-time scaling strategies to explore the boundaries of our framework. Experiments demonstrate the effectiveness of RAR$^2$ across several biomedical question answering datasets, outperforming RAG baselines with or without fine-tuning.",
      "published": "2025-09-24",
      "updated": "2025-09-24",
      "pdf_url": "https://arxiv.org/pdf/2509.22713v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2509.15279v1",
      "title": "Fleming-R1: Toward Expert-Level Medical Reasoning via Reinforcement Learning",
      "authors": [
        "Chi Liu",
        "Derek Li",
        "Yan Shu",
        "Robin Chen",
        "Derek Duan",
        "Teng Fang",
        "Bryan Dai"
      ],
      "abstract": "While large language models show promise in medical applications, achieving expert-level clinical reasoning remains challenging due to the need for both accurate answers and transparent reasoning processes. To address this challenge, we introduce Fleming-R1, a model designed for verifiable medical reasoning through three complementary innovations. First, our Reasoning-Oriented Data Strategy (RODS) combines curated medical QA datasets with knowledge-graph-guided synthesis to improve coverage of underrepresented diseases, drugs, and multi-hop reasoning chains. Second, we employ Chain-of-Thought (CoT) cold start to distill high-quality reasoning trajectories from teacher models, establishing robust inference priors. Third, we implement a two-stage Reinforcement Learning from Verifiable Rewards (RLVR) framework using Group Relative Policy Optimization, which consolidates core reasoning skills while targeting persistent failure modes through adaptive hard-sample mining. Across diverse medical benchmarks, Fleming-R1 delivers substantial parameter-efficient improvements: the 7B variant surpasses much larger baselines, while the 32B model achieves near-parity with GPT-4o and consistently outperforms strong open-source alternatives. These results demonstrate that structured data design, reasoning-oriented initialization, and verifiable reinforcement learning can advance clinical reasoning beyond simple accuracy optimization. We release Fleming-R1 publicly to promote transparent, reproducible, and auditable progress in medical AI, enabling safer deployment in high-stakes clinical environments.",
      "published": "2025-09-18",
      "updated": "2025-09-18",
      "pdf_url": "https://arxiv.org/pdf/2509.15279v1",
      "keyword": "Medical reasoning LLM"
    }
  ]
}