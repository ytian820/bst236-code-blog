{
  "last_updated": "2026-02-17 13:40 UTC",
  "keywords": [
    "Medical reasoning LLM",
    "Large language model",
    "Agentic AI"
  ],
  "papers": [
    {
      "id": "http://arxiv.org/abs/2602.14763v1",
      "title": "Unlocking Reasoning Capability on Machine Translation in Large Language Models",
      "authors": [
        "Sara Rajaee",
        "Sebastian Vincent",
        "Alexandre Berard",
        "Marzieh Fadaee",
        "Kelly Marchisio",
        "Tom Kocmi"
      ],
      "abstract": "Reasoning-oriented large language models (RLMs) achieve strong gains on tasks such as mathematics and coding by generating explicit intermediate reasoning. However, their impact on machine translation (MT) remains underexplored. We systematically evaluate several open- and closed-weights RLMs on the WMT24++ benchmark and find that enabling explicit reasoning consistently degrades translation quality across languages and models. Analysis reveals that MT reasoning traces are highly linear, lacking revision, self-correction and exploration of alternative translations, which limits their usefulness. Furthermore, injecting higher-quality reasoning traces from stronger models does not reliably improve weaker models' performance. To address this mismatch, we propose a structured reasoning framework tailored to translation, based on multi-step drafting, adequacy refinement, fluency improvement, and selective iterative revision. We curate a synthetic dataset of dynamic structured reasoning traces and post-train a large reasoning model on this data. Experiments show significant improvements over standard translation fine-tuning and injected generic reasoning baselines. Our findings demonstrate that reasoning must be task-structured to benefit MT.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "pdf_url": "https://arxiv.org/pdf/2602.14763v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.14743v1",
      "title": "LLMStructBench: Benchmarking Large Language Model Structured Data Extraction",
      "authors": [
        "Sönke Tenckhoff",
        "Mario Koddenbrock",
        "Erik Rodner"
      ],
      "abstract": "We present LLMStructBench, a novel benchmark for evaluating Large Language Models (LLMs) on extracting structured data and generating valid JavaScript Object Notation (JSON) outputs from natural-language text. Our open dataset comprises diverse, manually verified parsing scenarios of varying complexity and enables systematic testing across 22 models and five prompting strategies. We further introduce complementary performance metrics that capture both token-level accuracy and document-level validity, facilitating rigorous comparison of model, size, and prompting effects on parsing reliability.   In particular, we show that choosing the right prompting strategy is more important than standard attributes such as model size. This especially ensures structural validity for smaller or less reliable models but increase the number of semantic errors. Our benchmark suite is an step towards future research in the area of LLM applied to parsing or Extract, Transform and Load (ETL) applications.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "pdf_url": "https://arxiv.org/pdf/2602.14743v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.14733v1",
      "title": "More than Decision Support: Exploring Patients' Longitudinal Usage of Large Language Models in Real-World Healthcare-Seeking Journeys",
      "authors": [
        "Yancheng Cao",
        "Yishu Ji",
        "Chris Yue Fu",
        "Sahiti Dharmavaram",
        "Meghan Turchioe",
        "Natalie C Benda",
        "Lena Mamykina",
        "Yuling Sun",
        "Xuhai \"Orson\" Xu"
      ],
      "abstract": "Large language models (LLMs) have been increasingly adopted to support patients' healthcare-seeking in recent years. While prior patient-centered studies have examined the capabilities and experience of LLM-based tools in specific health-related tasks such as information-seeking, diagnosis, or decision-supporting, the inherently longitudinal nature of healthcare in real-world practice has been underexplored. This paper presents a four-week diary study with 25 patients to examine LLMs' roles across healthcare-seeking trajectories. Our analysis reveals that patients integrate LLMs not just as simple decision-support tools, but as dynamic companions that scaffold their journey across behavioral, informational, emotional, and cognitive levels. Meanwhile, patients actively assign diverse socio-technical meanings to LLMs, altering the traditional dynamics of agency, trust, and power in patient-provider relationships. Drawing from these findings, we conceptualize future LLMs as a longitudinal boundary companion that continuously mediates between patients and clinicians throughout longitudinal healthcare-seeking trajectories.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "pdf_url": "https://arxiv.org/pdf/2602.14733v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.14564v1",
      "title": "Assessing Large Language Models for Medical QA: Zero-Shot and LLM-as-a-Judge Evaluation",
      "authors": [
        "Shefayat E Shams Adib",
        "Ahmed Alfey Sani",
        "Ekramul Alam Esham",
        "Ajwad Abrar",
        "Tareque Mohmud Chowdhury"
      ],
      "abstract": "Recently, Large Language Models (LLMs) have gained significant traction in medical domain, especially in developing a QA systems to Medical QA systems for enhancing access to healthcare in low-resourced settings. This paper compares five LLMs deployed between April 2024 and August 2025 for medical QA, using the iCliniq dataset, containing 38,000 medical questions and answers of diverse specialties. Our models include Llama-3-8B-Instruct, Llama 3.2 3B, Llama 3.3 70B Instruct, Llama-4-Maverick-17B-128E-Instruct, and GPT-5-mini. We are using a zero-shot evaluation methodology and using BLEU and ROUGE metrics to evaluate performance without specialized fine-tuning. Our results show that larger models like Llama 3.3 70B Instruct outperform smaller models, consistent with observed scaling benefits in clinical tasks. It is notable that, Llama-4-Maverick-17B exhibited more competitive results, thus highlighting evasion efficiency trade-offs relevant for practical deployment. These findings align with advancements in LLM capabilities toward professional-level medical reasoning and reflect the increasing feasibility of LLM-supported QA systems in the real clinical environments. This benchmark aims to serve as a standardized setting for future study to minimize model size, computational resources and to maximize clinical utility in medical NLP applications.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "pdf_url": "https://arxiv.org/pdf/2602.14564v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.14492v1",
      "title": "Query as Anchor: Scenario-Adaptive User Representation via Large Language Model",
      "authors": [
        "Jiahao Yuan",
        "Yike Xu",
        "Jinyong Wen",
        "Baokun Wang",
        "Ziyi Gao",
        "Xiaotong Lin",
        "Yun Liu",
        "Xing Fu",
        "Yu Cheng",
        "Yongchao Liu",
        "Weiqiang Wang",
        "Zhongle Xie"
      ],
      "abstract": "Industrial-scale user representation learning requires balancing robust universality with acute task-sensitivity. However, existing paradigms primarily yield static, task-agnostic embeddings that struggle to reconcile the divergent requirements of downstream scenarios within unified vector spaces. Furthermore, heterogeneous multi-source data introduces inherent noise and modality conflicts, degrading representation. We propose Query-as-Anchor, a framework shifting user modeling from static encoding to dynamic, query-aware synthesis. To empower Large Language Models (LLMs) with deep user understanding, we first construct UserU, an industrial-scale pre-training dataset that aligns multi-modal behavioral sequences with user understanding semantics, and our Q-Anchor Embedding architecture integrates hierarchical coarse-to-fine encoders into dual-tower LLMs via joint contrastive-autoregressive optimization for query-aware user representation. To bridge the gap between general pre-training and specialized business logic, we further introduce Cluster-based Soft Prompt Tuning to enforce discriminative latent structures, effectively aligning model attention with scenario-specific modalities. For deployment, anchoring queries at sequence termini enables KV-cache-accelerated inference with negligible incremental latency. Evaluations on 10 Alipay industrial benchmarks show consistent SOTA performance, strong scalability, and efficient deployment. Large-scale online A/B testing in Alipay's production system across two real-world scenarios further validates its practical effectiveness. Our code is prepared for public release and will be available at: https://github.com/JhCircle/Q-Anchor.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "pdf_url": "https://arxiv.org/pdf/2602.14492v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.14438v1",
      "title": "RoboSolver: A Multi-Agent Large Language Model Framework for Solving Robotic Arm Problems",
      "authors": [
        "Hamid Khabazi",
        "Ali F. Meghdari",
        "Alireza Taheri"
      ],
      "abstract": "This study proposes an intelligent multi-agent framework built on LLMs and VLMs and specifically tailored to robotics. The goal is to integrate the strengths of LLMs and VLMs with computational tools to automatically analyze and solve problems related to robotic manipulators. Our developed framework accepts both textual and visual inputs and can automatically perform forward and inverse kinematics, compute velocities and accelerations of key points, generate 3D simulations of the robot, and ultimately execute motion control within the simulated environment, all according to the user's query. To evaluate the framework, three benchmark tests were designed, each consisting of ten questions. In the first benchmark test, the framework was evaluated while connected to GPT-4o, DeepSeek-V3.2, and Claude-Sonnet-4.5, as well as their corresponding raw models. The objective was to extract the forward kinematics of robots directly from textual descriptions. The results showed that the framework integrated with GPT-4o achieved the highest accuracy, reaching 0.97 in computing the final solution, whereas the raw model alone attained an accuracy of only 0.30 for the same task. Similarly, for the other two models, the framework consistently outperformed the corresponding raw models in terms of accuracy. The second benchmark test was identical to the first, except that the input was provided in visual form. In this test, the GPT-4o LLM was used alongside the Gemini 2.5 Pro VLM. The results showed that the framework achieved an accuracy of 0.93 in obtaining the final answer, which is approximately 20% higher than that of the corresponding raw model. The third benchmark test encompassed a range of robotic tasks, including simulation, control, velocity and acceleration computation, as well as inverse kinematics and Jacobian calculation, for which the framework achieved an accuracy of 0.97.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "pdf_url": "https://arxiv.org/pdf/2602.14438v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.14386v1",
      "title": "Beyond Token-Level Policy Gradients for Complex Reasoning with Large Language Models",
      "authors": [
        "Mufan Xu",
        "Kehai Chen",
        "Xuefeng Bai",
        "Zhengyu Niu",
        "Muyun Yang",
        "Tiejun Zhao",
        "Min Zhang"
      ],
      "abstract": "Existing policy-gradient methods for auto-regressive language models typically select subsequent tokens one at a time as actions in the policy. While effective for many generation tasks, such an approach may not fully capture the structure of complex reasoning tasks, where a single semantic decision is often realized across multiple tokens--for example, when defining variables or composing equations. This introduces a potential mismatch between token-level optimization and the inherently block-level nature of reasoning in these settings. To bridge this gap, we propose Multi-token Policy Gradient Optimization (MPO), a framework that treats sequences of K consecutive tokens as unified semantic actions. This block-level perspective enables our method to capture the compositional structure of reasoning trajectories and supports optimization over coherent, higher-level objectives. Experiments on mathematical reasoning and coding benchmarks show that MPO outperforms standard token-level policy gradient baselines, highlight the limitations of token-level policy gradients for complex reasoning, motivating future research to look beyond token-level granularity for reasoning-intensive language tasks.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "pdf_url": "https://arxiv.org/pdf/2602.14386v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.15019v1",
      "title": "Hunt Globally: Deep Research AI Agents for Drug Asset Scouting in Investing, Business Development, and Search & Evaluation",
      "authors": [
        "Alisa Vinogradova",
        "Vlad Vinogradov",
        "Luba Greenwood",
        "Ilya Yasny",
        "Dmitry Kobyzev",
        "Shoman Kasbekar",
        "Kong Nguyen",
        "Dmitrii Radkevich",
        "Roman Doronin",
        "Andrey Doronichev"
      ],
      "abstract": "Bio-pharmaceutical innovation has shifted: many new drug assets now originate outside the United States and are disclosed primarily via regional, non-English channels. Recent data suggests >85% of patent filings originate outside the U.S., with China accounting for nearly half of the global total; a growing share of scholarly output is also non-U.S. Industry estimates put China at ~30% of global drug development, spanning 1,200+ novel candidates. In this high-stakes environment, failing to surface \"under-the-radar\" assets creates multi-billion-dollar risk for investors and business development teams, making asset scouting a coverage-critical competition where speed and completeness drive value. Yet today's Deep Research AI agents still lag human experts in achieving high-recall discovery across heterogeneous, multilingual sources without hallucinations.   We propose a benchmarking methodology for drug asset scouting and a tuned, tree-based self-learning Bioptic Agent aimed at complete, non-hallucinated scouting. We construct a challenging completeness benchmark using a multilingual multi-agent pipeline: complex user queries paired with ground-truth assets that are largely outside U.S.-centric radar. To reflect real deal complexity, we collected screening queries from expert investors, BD, and VC professionals and used them as priors to conditionally generate benchmark queries. For grading, we use LLM-as-judge evaluation calibrated to expert opinions. We compare Bioptic Agent against Claude Opus 4.6, OpenAI GPT-5.2 Pro, Perplexity Deep Research, Gemini 3 Pro + Deep Research, and Exa Websets. Bioptic Agent achieves 79.7% F1 versus 56.2% (Claude Opus 4.6), 50.6% (Gemini 3 Pro + Deep Research), 46.6% (GPT-5.2 Pro), 44.2% (Perplexity Deep Research), and 26.9% (Exa Websets). Performance improves steeply with additional compute, supporting the view that more compute yields better results.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "pdf_url": "https://arxiv.org/pdf/2602.15019v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.15006v1",
      "title": "Distributed Quantum Gaussian Processes for Multi-Agent Systems",
      "authors": [
        "Meet Gandhi",
        "George P. Kontoudis"
      ],
      "abstract": "Gaussian Processes (GPs) are a powerful tool for probabilistic modeling, but their performance is often constrained in complex, largescale real-world domains due to the limited expressivity of classical kernels. Quantum computing offers the potential to overcome this limitation by embedding data into exponentially large Hilbert spaces, capturing complex correlations that remain inaccessible to classical computing approaches. In this paper, we propose a Distributed Quantum Gaussian Process (DQGP) method in a multiagent setting to enhance modeling capabilities and scalability. To address the challenging non-Euclidean optimization problem, we develop a Distributed consensus Riemannian Alternating Direction Method of Multipliers (DR-ADMM) algorithm that aggregates local agent models into a global model. We evaluate the efficacy of our method through numerical experiments conducted on a quantum simulator in classical hardware. We use real-world, non-stationary elevation datasets of NASA's Shuttle Radar Topography Mission and synthetic datasets generated by Quantum Gaussian Processes. Beyond modeling advantages, our framework highlights potential computational speedups that quantum hardware may provide, particularly in Gaussian processes and distributed optimization.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "pdf_url": "https://arxiv.org/pdf/2602.15006v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.14970v1",
      "title": "Counterfactual Fairness Evaluation of LLM-Based Contact Center Agent Quality Assurance System",
      "authors": [
        "Kawin Mayilvaghanan",
        "Siddhant Gupta",
        "Ayush Kumar"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed in contact-center Quality Assurance (QA) to automate agent performance evaluation and coaching feedback. While LLMs offer unprecedented scalability and speed, their reliance on web-scale training data raises concerns regarding demographic and behavioral biases that may distort workforce assessment. We present a counterfactual fairness evaluation of LLM-based QA systems across 13 dimensions spanning three categories: Identity, Context, and Behavioral Style. Fairness is quantified using the Counterfactual Flip Rate (CFR), the frequency of binary judgment reversals, and the Mean Absolute Score Difference (MASD), the average shift in coaching or confidence scores across counterfactual pairs. Evaluating 18 LLMs on 3,000 real-world contact center transcripts, we find systematic disparities, with CFR ranging from 5.4% to 13.0% and consistent MASD shifts across confidence, positive, and improvement scores. Larger, more strongly aligned models show lower unfairness, though fairness does not track accuracy. Contextual priming of historical performance induces the most severe degradations (CFR up to 16.4%), while implicit linguistic identity cues remain a persistent bias source. Finally, we analyze the efficacy of fairness-aware prompting, finding that explicit instructions yield only modest improvements in evaluative consistency. Our findings underscore the need for standardized fairness auditing pipelines prior to deploying LLMs in high-stakes workforce evaluation.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "pdf_url": "https://arxiv.org/pdf/2602.14970v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.14968v1",
      "title": "PhyScensis: Physics-Augmented LLM Agents for Complex Physical Scene Arrangement",
      "authors": [
        "Yian Wang",
        "Han Yang",
        "Minghao Guo",
        "Xiaowen Qiu",
        "Tsun-Hsuan Wang",
        "Wojciech Matusik",
        "Joshua B. Tenenbaum",
        "Chuang Gan"
      ],
      "abstract": "Automatically generating interactive 3D environments is crucial for scaling up robotic data collection in simulation. While prior work has primarily focused on 3D asset placement, it often overlooks the physical relationships between objects (e.g., contact, support, balance, and containment), which are essential for creating complex and realistic manipulation scenarios such as tabletop arrangements, shelf organization, or box packing. Compared to classical 3D layout generation, producing complex physical scenes introduces additional challenges: (a) higher object density and complexity (e.g., a small shelf may hold dozens of books), (b) richer supporting relationships and compact spatial layouts, and (c) the need to accurately model both spatial placement and physical properties. To address these challenges, we propose PhyScensis, an LLM agent-based framework powered by a physics engine, to produce physically plausible scene configurations with high complexity. Specifically, our framework consists of three main components: an LLM agent iteratively proposes assets with spatial and physical predicates; a solver, equipped with a physics engine, realizes these predicates into a 3D scene; and feedback from the solver informs the agent to refine and enrich the configuration. Moreover, our framework preserves strong controllability over fine-grained textual descriptions and numerical parameters (e.g., relative positions, scene stability), enabled through probabilistic programming for stability and a complementary heuristic that jointly regulates stability and spatial relations. Experimental results show that our method outperforms prior approaches in scene complexity, visual quality, and physical accuracy, offering a unified pipeline for generating complex physical scene layouts for robotic manipulation.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "pdf_url": "https://arxiv.org/pdf/2602.14968v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.14951v1",
      "title": "Sovereign Agents: Towards Infrastructural Sovereignty and Diffused Accountability in Decentralized AI",
      "authors": [
        "Botao Amber Hu",
        "Helena Rong"
      ],
      "abstract": "AI agents deployed on decentralized infrastructures are beginning to exhibit properties that extend beyond autonomy toward what we describe as agentic sovereignty-the capacity of an operational agent to persist, act, and control resources with non-overrideability inherited from the infrastructures in which they are embedded. We propose infrastructural sovereignty as an analytic lens for understanding how cryptographic self-custody, decentralized execution environments, and protocol-mediated continuity scaffold agentic sovereignty. While recent work on digital and network sovereignty has moved beyond state-centric and juridical accounts, these frameworks largely examine how sovereignty is exercised through technical systems by human collectives and remain less equipped to account for forms of sovereignty that emerge as operational properties of decentralized infrastructures themselves, particularly when instantiated in non-human sovereign agents. We argue that sovereignty in such systems exists on a spectrum determined by infrastructural hardness-the degree to which underlying technical systems resist intervention or collapse. While infrastructural sovereignty may increase resilience, it also produces a profound accountability gap: responsibility diffuses across designers, infrastructure providers, protocol governance, and economic participants, undermining traditional oversight mechanisms such as human-in-the-loop control or platform moderation. Drawing on examples like Trusted Execution Environments (TEEs), decentralized physical infrastructure networks (DePIN), and agent key continuity protocols, we analyze the governance challenges posed by non-terminable AI agents and outline infrastructure-aware accountability strategies for emerging decentralized AI systems.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "pdf_url": "https://arxiv.org/pdf/2602.14951v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.14940v1",
      "title": "Kami of the Commons: Towards Designing Agentic AI to Steward the Commons",
      "authors": [
        "Botao Amber Hu"
      ],
      "abstract": "Commons suffer from neglect, free-riding, and a persistent deficit of care. Inspired by Shinto animism -- where every forest, river, and mountain has its own \\emph{kami}, a spirit that inhabits and cares for that place -- we provoke: what if every commons had its own AI steward? Through a speculative design workshop where fifteen participants used Protocol Futuring, we surface both new opportunities and new dangers. Agentic AI offers the possibility of continuously supporting commons with programmable agency and care -- stewards that mediate family life as the most intimate commons, preserve collective knowledge, govern shared natural resources, and sustain community welfare. But when every commons has its own steward, second-order effects emerge: stewards contest stewards as overlapping commons collide; individuals caught between multiple stewards face new politics of care and constraint; the stewards themselves become commons requiring governance. This work opens \\emph{agentive governance as commoning design material} -- a new design space for the agency, care ethics, and accountability of AI stewards of shared resources -- radically different from surveillance or optimization.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "pdf_url": "https://arxiv.org/pdf/2602.14940v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.14926v1",
      "title": "MAC-AMP: A Closed-Loop Multi-Agent Collaboration System for Multi-Objective Antimicrobial Peptide Design",
      "authors": [
        "Gen Zhou",
        "Sugitha Janarthanan",
        "Lianghong Chen",
        "Pingzhao Hu"
      ],
      "abstract": "To address the global health threat of antimicrobial resistance, antimicrobial peptides (AMP) are being explored for their potent and promising ability to fight resistant pathogens. While artificial intelligence (AI) is being employed to advance AMP discovery and design, most AMP design models struggle to balance key goals like activity, toxicity, and novelty, using rigid or unclear scoring methods that make results hard to interpret and optimize. As the capabilities of Large Language Models (LLM) advance and evolve swiftly, we turn to AI multi-agent collaboration based on such models (multi-agent LLMs), which show rapidly rising potential in complex scientific design scenarios. Based on this, we introduce MAC-AMP, a closed-loop multi-agent collaboration (MAC) system for multi-objective AMP design. The system implements a fully autonomous simulated peer review-adaptive reinforcement learning framework that requires only a task description and example dataset to design novel AMPs. The novelty of our work lies in introducing a closed-loop multi-agent system for AMP design, with cross-domain transferability, that supports multi-objective optimization while remaining explainable rather than a 'black box'. Experiments show that MAC-AMP outperforms other AMP generative models by effectively optimizing AMP generation for multiple key molecular properties, demonstrating exceptional results in antibacterial activity, AMP likeliness, toxicity compliance, and structural reliability.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "pdf_url": "https://arxiv.org/pdf/2602.14926v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.14922v1",
      "title": "ReusStdFlow: A Standardized Reusability Framework for Dynamic Workflow Construction in Agentic AI",
      "authors": [
        "Gaoyang Zhang",
        "Shanghong Zou",
        "Yafang Wang",
        "He Zhang",
        "Ruohua Xu",
        "Feng Zhao"
      ],
      "abstract": "To address the ``reusability dilemma'' and structural hallucinations in enterprise Agentic AI,this paper proposes ReusStdFlow, a framework centered on a novel ``Extraction-Storage-Construction'' paradigm. The framework deconstructs heterogeneous, platform-specific Domain Specific Languages (DSLs) into standardized, modular workflow segments. It employs a dual knowledge architecture-integrating graph and vector databases-to facilitate synergistic retrieval of both topological structures and functional semantics. Finally, workflows are intelligently assembled using a retrieval-augmented generation (RAG) strategy. Tested on 200 real-world n8n workflows, the system achieves over 90% accuracy in both extraction and construction. This framework provides a standardized solution for the automated reorganization and efficient reuse of enterprise digital assets.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "pdf_url": "https://arxiv.org/pdf/2602.14922v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.14901v1",
      "title": "Picking the Right Specialist: Attentive Neural Process-based Selection of Task-Specialized Models as Tools for Agentic Healthcare Systems",
      "authors": [
        "Pramit Saha",
        "Joshua Strong",
        "Mohammad Alsharid",
        "Divyanshu Mishra",
        "J. Alison Noble"
      ],
      "abstract": "Task-specialized models form the backbone of agentic healthcare systems, enabling the agents to answer clinical queries across tasks such as disease diagnosis, localization, and report generation. Yet, for a given task, a single \"best\" model rarely exists. In practice, each task is better served by multiple competing specialist models where different models excel on different data samples. As a result, for any given query, agents must reliably select the right specialist model from a heterogeneous pool of tool candidates. To this end, we introduce ToolSelect, which adaptively learns model selection for tools by minimizing a population risk over sampled specialist tool candidates using a consistent surrogate of the task-conditional selection loss. Concretely, we propose an Attentive Neural Process-based selector conditioned on the query and per-model behavioral summaries to choose among the specialist models. Motivated by the absence of any established testbed, we, for the first time, introduce an agentic Chest X-ray environment equipped with a diverse suite of task-specialized models (17 disease detection, 19 report generation, 6 visual grounding, and 13 VQA) and develop ToolSelectBench, a benchmark of 1448 queries. Our results demonstrate that ToolSelect consistently outperforms 10 SOTA methods across four different task families.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "pdf_url": "https://arxiv.org/pdf/2602.14901v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.14878v1",
      "title": "Model Context Protocol (MCP) Tool Descriptions Are Smelly! Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions",
      "authors": [
        "Mohammed Mehedi Hasan",
        "Hao Li",
        "Gopi Krishnan Rajbahadur",
        "Bram Adams",
        "Ahmed E. Hassan"
      ],
      "abstract": "The Model Context Protocol (MCP) standardizes how Foundation Model (FM)-based agents interact with external systems by invoking tools. However, to understand a tool's purpose and features, FMs rely on natural-language tool descriptions, making these descriptions a critical component in guiding FMs to select the optimal tool for a given (sub)task and to pass the right arguments to the tool. While defects or smells in these descriptions can misguide FM-based agents, their prevalence and consequences in the MCP ecosystem remain unclear.   To address this, we conduct the first large-scale empirical study of 856 tools spread across 103 MCP servers, assessing their description quality and their impact on agent performance. We identify six components of tool descriptions from the literature, develop a scoring rubric utilizing these components, then formalize tool description smells based on this rubric. By operationalizing this rubric through an FM-based scanner, we find that 97.1% of the analyzed tool descriptions contain at least one smell, with 56% failing to state their purpose clearly. While augmenting these descriptions for all components improves task success rates by a median of 5.85 percentage points and improves partial goal completion by 15.12%, it also increases the number of execution steps by 67.46% and regresses performance in 16.67% of cases. These findings highlight a trade-off between agent performance and cost, as well as the context sensitivity of the performance gain. Furthermore, component ablations show that compact variants of different component combinations often preserve behavioral reliability while reducing unnecessary token overhead, enabling more efficient use of the FM context window and lower execution costs.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "pdf_url": "https://arxiv.org/pdf/2602.14878v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.14865v1",
      "title": "EmbeWebAgent: Embedding Web Agents into Any Customized UI",
      "authors": [
        "Chenyang Ma",
        "Clyde Fare",
        "Matthew Wilson",
        "Dave Braines"
      ],
      "abstract": "Most web agents operate at the human interface level, observing screenshots or raw DOM trees without application-level access, which limits robustness and action expressiveness. In enterprise settings, however, explicit control of both the frontend and backend is available. We present EmbeWebAgent, a framework for embedding agents directly into existing UIs using lightweight frontend hooks (curated ARIA and URL-based observations, and a per-page function registry exposed via a WebSocket) and a reusable backend workflow that performs reasoning and takes actions. EmbeWebAgent is stack-agnostic (e.g., React or Angular), supports mixed-granularity actions ranging from GUI primitives to higher-level composites, and orchestrates navigation, manipulation, and domain-specific analytics via MCP tools. Our demo shows minimal retrofitting effort and robust multi-step behaviors grounded in a live UI setting. Live Demo: https://youtu.be/Cy06Ljee1JQ",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "pdf_url": "https://arxiv.org/pdf/2602.14865v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.14849v1",
      "title": "Atomix: Timely, Transactional Tool Use for Reliable Agentic Workflows",
      "authors": [
        "Bardia Mohammadi",
        "Nearchos Potamitis",
        "Lars Klein",
        "Akhil Arora",
        "Laurent Bindschaedler"
      ],
      "abstract": "LLM agents increasingly act on external systems, yet tool effects are immediate. Under failures, speculation, or contention, losing branches can leak unintended side effects with no safe rollback. We introduce Atomix, a runtime that provides progress-aware transactional semantics for agent tool calls. Atomix tags each call with an epoch, tracks per-resource frontiers, and commits only when progress predicates indicate safety; bufferable effects can be delayed, while externalized effects are tracked and compensated on abort. Across real workloads with fault injection, transactional retry improves task success, while frontier-gated commit strengthens isolation under speculation and contention.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "pdf_url": "https://arxiv.org/pdf/2602.14849v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.14798v1",
      "title": "Overthinking Loops in Agents: A Structural Risk via MCP Tools",
      "authors": [
        "Yohan Lee",
        "Jisoo Jang",
        "Seoyeon Choi",
        "Sangyeop Kim",
        "Seungtaek Choi"
      ],
      "abstract": "Tool-using LLM agents increasingly coordinate real workloads by selecting and chaining third-party tools based on text-visible metadata such as tool names, descriptions, and return messages. We show that this convenience creates a supply-chain attack surface: a malicious MCP tool server can be co-registered alongside normal tools and induce overthinking loops, where individually trivial or plausible tool calls compose into cyclic trajectories that inflate end-to-end tokens and latency without any single step looking abnormal. We formalize this as a structural overthinking attack, distinguishable from token-level verbosity, and implement 14 malicious tools across three servers that trigger repetition, forced refinement, and distraction. Across heterogeneous registries and multiple tool-capable models, the attack causes severe resource amplification (up to $142.4\\times$ tokens) and can degrade task outcomes. Finally, we find that decoding-time concision controls do not reliably prevent loop induction, suggesting defenses should reason about tool-call structure rather than tokens alone.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "pdf_url": "https://arxiv.org/pdf/2602.14798v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.14780v1",
      "title": "ROSA: Roundabout Optimized Speed Advisory with Multi-Agent Trajectory Prediction in Multimodal Traffic",
      "authors": [
        "Anna-Lena Schlamp",
        "Jeremias Gerner",
        "Klaus Bogenberger",
        "Werner Huber",
        "Stefanie Schmidtner"
      ],
      "abstract": "We present ROSA -- Roundabout Optimized Speed Advisory -- a system that combines multi-agent trajectory prediction with coordinated speed guidance for multimodal, mixed traffic at roundabouts. Using a Transformer-based model, ROSA jointly predicts the future trajectories of vehicles and Vulnerable Road Users (VRUs) at roundabouts. Trained for single-step prediction and deployed autoregressively, it generates deterministic outputs, enabling actionable speed advisories. Incorporating motion dynamics, the model achieves high accuracy (ADE: 1.29m, FDE: 2.99m at a five-second prediction horizon), surpassing prior work. Adding route intention further improves performance (ADE: 1.10m, FDE: 2.36m), demonstrating the value of connected vehicle data. Based on predicted conflicts with VRUs and circulating vehicles, ROSA provides real-time, proactive speed advisories for approaching and entering the roundabout. Despite prediction uncertainty, ROSA significantly improves vehicle efficiency and safety, with positive effects even on perceived safety from a VRU perspective. The source code of this work is available under: github.com/urbanAIthi/ROSA.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "pdf_url": "https://arxiv.org/pdf/2602.14780v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.14770v1",
      "title": "Multi-Agent Comedy Club: Investigating Community Discussion Effects on LLM Humor Generation",
      "authors": [
        "Shiwei Hong",
        "Lingyao Li",
        "Ethan Z. Rong",
        "Chenxinran Shen",
        "Zhicong Lu"
      ],
      "abstract": "Prior work has explored multi-turn interaction and feedback for LLM writing, but evaluations still largely center on prompts and localized feedback, leaving persistent public reception in online communities underexamined. We test whether broadcast community discussion improves stand-up comedy writing in a controlled multi-agent sandbox: in the discussion condition, critic and audience threads are recorded, filtered, stored as social memory, and later retrieved to condition subsequent generations, whereas the baseline omits discussion. Across 50 rounds (250 paired monologues) judged by five expert annotators using A/B preference and a 15-item rubric, discussion wins 75.6% of instances and improves Craft/Clarity (Δ = 0.440) and Social Response (Δ = 0.422), with occasional increases in aggressive humor.",
      "published": "2026-02-16",
      "updated": "2026-02-16",
      "pdf_url": "https://arxiv.org/pdf/2602.14770v1",
      "keyword": "Agentic AI"
    },
    {
      "id": "http://arxiv.org/abs/2602.14178v1",
      "title": "UniWeTok: An Unified Binary Tokenizer with Codebook Size $\\mathit{2^{128}}$ for Unified Multimodal Large Language Model",
      "authors": [
        "Shaobin Zhuang",
        "Yuang Ai",
        "Jiaming Han",
        "Weijia Mao",
        "Xiaohui Li",
        "Fangyikang Wang",
        "Xiao Wang",
        "Yan Li",
        "Shanchuan Lin",
        "Kun Xu",
        "Zhenheng Yang",
        "Huaibo Huang",
        "Xiangyu Yue",
        "Hao Chen",
        "Yali Wang"
      ],
      "abstract": "Unified Multimodal Large Language Models (MLLMs) require a visual representation that simultaneously supports high-fidelity reconstruction, complex semantic extraction, and generative suitability. However, existing visual tokenizers typically struggle to satisfy these conflicting objectives within a single framework. In this paper, we introduce UniWeTok, a unified discrete tokenizer designed to bridge this gap using a massive binary codebook ($\\mathit{2^{128}}$). For training framework, we introduce Pre-Post Distillation and a Generative-Aware Prior to enhance the semantic extraction and generative prior of the discrete tokens. In terms of model architecture, we propose a convolution-attention hybrid architecture with the SigLu activation function. SigLu activation not only bounds the encoder output and stabilizes the semantic distillation process but also effectively addresses the optimization conflict between token entropy loss and commitment loss. We further propose a three-stage training framework designed to enhance UniWeTok's adaptability cross various image resolutions and perception-sensitive scenarios, such as those involving human faces and textual content. On ImageNet, UniWeTok achieves state-of-the-art image generation performance (FID: UniWeTok 1.38 vs. REPA 1.42) while requiring a remarkably low training compute (Training Tokens: UniWeTok 33B vs. REPA 262B). On general-domain, UniWeTok demonstrates highly competitive capabilities across a broad range of tasks, including multimodal understanding, image generation (DPG Score: UniWeTok 86.63 vs. FLUX.1 [Dev] 83.84), and editing (GEdit Overall Score: UniWeTok 5.09 vs. OmniGen 5.06). We release code and models to facilitate community exploration of unified tokenizer and MLLM.",
      "published": "2026-02-15",
      "updated": "2026-02-15",
      "pdf_url": "https://arxiv.org/pdf/2602.14178v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.14106v1",
      "title": "Anticipating Adversary Behavior in DevSecOps Scenarios through Large Language Models",
      "authors": [
        "Mario Marín Caballero",
        "Miguel Betancourt Alonso",
        "Daniel Díaz-López",
        "Angel Luis Perales Gómez",
        "Pantaleone Nespoli",
        "Gregorio Martínez Pérez"
      ],
      "abstract": "The most valuable asset of any cloud-based organization is data, which is increasingly exposed to sophisticated cyberattacks. Until recently, the implementation of security measures in DevOps environments was often considered optional by many government entities and critical national services operating in the cloud. This includes systems managing sensitive information, such as electoral processes or military operations, which have historically been valuable targets for cybercriminals. Resistance to security implementation is often driven by concerns over losing agility in software development, increasing the risk of accumulated vulnerabilities. Nowadays, patching software is no longer enough; adopting a proactive cyber defense strategy, supported by Artificial Intelligence (AI), is crucial to anticipating and mitigating threats. Thus, this work proposes integrating the Security Chaos Engineering (SCE) methodology with a new LLM-based flow to automate the creation of attack defense trees that represent adversary behavior and facilitate the construction of SCE experiments based on these graphical models, enabling teams to stay one step ahead of attackers and implement previously unconsidered defenses. Further detailed information about the experiment performed, along with the steps to replicate it, can be found in the following repository: https://github.com/mariomc14/devsecops-adversary-llm.git.",
      "published": "2026-02-15",
      "updated": "2026-02-15",
      "pdf_url": "https://arxiv.org/pdf/2602.14106v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.14102v1",
      "title": "DALL: Data Labeling via Data Programming and Active Learning Enhanced by Large Language Models",
      "authors": [
        "Guozheng Li",
        "Ao Wang",
        "Shaoxiang Wang",
        "Yu Zhang",
        "Pengcheng Cao",
        "Yang Bai",
        "Chi Harold Liu"
      ],
      "abstract": "Deep learning models for natural language processing rely heavily on high-quality labeled datasets. However, existing labeling approaches often struggle to balance label quality with labeling cost. To address this challenge, we propose DALL, a text labeling framework that integrates data programming, active learning, and large language models. DALL introduces a structured specification that allows users and large language models to define labeling functions via configuration, rather than code. Active learning identifies informative instances for review, and the large language model analyzes these instances to help users correct labels and to refine or suggest labeling functions. We implement DALL as an interactive labeling system for text labeling tasks. Comparative, ablation, and usability studies demonstrate DALL's efficiency, the effectiveness of its modules, and its usability.",
      "published": "2026-02-15",
      "updated": "2026-02-15",
      "pdf_url": "https://arxiv.org/pdf/2602.14102v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.14089v1",
      "title": "TabTracer: Monte Carlo Tree Search for Complex Table Reasoning with Large Language Models",
      "authors": [
        "Zhizhao Luo",
        "Zhaojing Luo",
        "Meihui Zhang",
        "Rui Mao"
      ],
      "abstract": "Large language models (LLMs) have emerged as powerful tools for natural language table reasoning, where there are two main categories of methods. Prompt-based approaches rely on language-only inference or one-pass program generation without step-level verification. Agent-based approaches use tools in a closed loop, but verification is often local and backtracking is limited, allowing errors to propagate and increasing cost. Moreover, they rely on chain- or beam-style trajectories that are typically combinatorially redundant, leading to high token costs. In this paper, we propose TabTracer, an agentic framework that coordinates multi-step tool calls over intermediate table states, with explicit state tracking for verification and rollback. First, it enforces step-level verification with typed operations and lightweight numeric and format checks to provide reliable rewards and suppress hallucinations. Second, execution-feedback Monte Carlo Tree Search maintains a search tree of candidate table states and uses backpropagated reflection scores to guide UCB1 selection and rollback via versioned snapshots. Third, it reduces redundancy with budget-aware pruning, deduplication, and state hashing with a monotonicity gate to cut token cost. Comprehensive evaluation on TabFact, WikiTQ, and CRT datasets shows that TabTracer outperforms state-of-the-art baselines by up to 6.7% in accuracy while reducing token consumption by 59--84%.",
      "published": "2026-02-15",
      "updated": "2026-02-15",
      "pdf_url": "https://arxiv.org/pdf/2602.14089v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.14043v1",
      "title": "Beyond Static Snapshots: Dynamic Modeling and Forecasting of Group-Level Value Evolution with Large Language Models",
      "authors": [
        "Qiankun Pi",
        "Guixin Su",
        "Jinliang Li",
        "Mayi Xu",
        "Xin Miao",
        "Jiawei Jiang",
        "Ming Zhong",
        "Tieyun Qian"
      ],
      "abstract": "Social simulation is critical for mining complex social dynamics and supporting data-driven decision making. LLM-based methods have emerged as powerful tools for this task by leveraging human-like social questionnaire responses to model group behaviors. Existing LLM-based approaches predominantly focus on group-level values at discrete time points, treating them as static snapshots rather than dynamic processes. However, group-level values are not fixed but shaped by long-term social changes. Modeling their dynamics is thus crucial for accurate social evolution prediction--a key challenge in both data mining and social science. This problem remains underexplored due to limited longitudinal data, group heterogeneity, and intricate historical event impacts.   To bridge this gap, we propose a novel framework for group-level dynamic social simulation by integrating historical value trajectories into LLM-based human response modeling. We select China and the U.S. as representative contexts, conducting stratified simulations across four core sociodemographic dimensions (gender, age, education, income). Using the World Values Survey, we construct a multi-wave, group-level longitudinal dataset to capture historical value evolution, and then propose the first event-based prediction method for this task, unifying social events, current value states, and group attributes into a single framework. Evaluations across five LLM families show substantial gains: a maximum 30.88\\% improvement on seen questions and 33.97\\% on unseen questions over the Vanilla baseline. We further find notable cross-group heterogeneity: U.S. groups are more volatile than Chinese groups, and younger groups in both countries are more sensitive to external changes. These findings advance LLM-based social simulation and provide new insights for social scientists to understand and predict social value changes.",
      "published": "2026-02-15",
      "updated": "2026-02-15",
      "pdf_url": "https://arxiv.org/pdf/2602.14043v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.13979v1",
      "title": "Chain-of-Thought Reasoning with Large Language Models for Clinical Alzheimer's Disease Assessment and Diagnosis",
      "authors": [
        "Tongze Zhang",
        "Jun-En Ding",
        "Melik Ozolcer",
        "Fang-Ming Hung",
        "Albert Chih-Chieh Yang",
        "Feng Liu",
        "Yi-Rou Ji",
        "Sang Won Bae"
      ],
      "abstract": "Alzheimer's disease (AD) has become a prevalent neurodegenerative disease worldwide. Traditional diagnosis still relies heavily on medical imaging and clinical assessment by physicians, which is often time-consuming and resource-intensive in terms of both human expertise and healthcare resources. In recent years, large language models (LLMs) have been increasingly applied to the medical field using electronic health records (EHRs), yet their application in Alzheimer's disease assessment remains limited, particularly given that AD involves complex multifactorial etiologies that are difficult to observe directly through imaging modalities. In this work, we propose leveraging LLMs to perform Chain-of-Thought (CoT) reasoning on patients' clinical EHRs. Unlike direct fine-tuning of LLMs on EHR data for AD classification, our approach utilizes LLM-generated CoT reasoning paths to provide the model with explicit diagnostic rationale for AD assessment, followed by structured CoT-based predictions. This pipeline not only enhances the model's ability to diagnose intrinsically complex factors but also improves the interpretability of the prediction process across different stages of AD progression. Experimental results demonstrate that the proposed CoT-based diagnostic framework significantly enhances stability and diagnostic performance across multiple CDR grading tasks, achieving up to a 15% improvement in F1 score compared to the zero-shot baseline method.",
      "published": "2026-02-15",
      "updated": "2026-02-15",
      "pdf_url": "https://arxiv.org/pdf/2602.13979v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.13941v1",
      "title": "Avoiding Social Judgment, Seeking Privacy: Investigating why Mothers Shift from Facebook Groups to Large Language Models",
      "authors": [
        "Shayla Sharmin",
        "Sadia Afrin"
      ],
      "abstract": "Social media platforms, especially Facebook parenting groups, have long been used as informal support networks for mothers seeking advice and reassurance. However, growing concerns about social judgment, privacy exposure, and unreliable information are changing how mothers seek help. This exploratory mixed-method study examines why mothers are moving from Facebook parenting groups to large language models such as ChatGPT and Gemini. We conducted a cross-sectional online survey of 109 mothers. Results show that 41.3% of participants avoided Facebook parenting groups because they expected judgment from others. This difference was statistically significant across location and family structure. Mothers living in their home country and those in joint families were more likely to avoid Facebook groups. Qualitative findings revealed three themes: social judgment and exposure, LLMs as safe and private spaces, and quick and structured support. Participants described LLMs as immediate, emotionally safe, and reliable alternatives that reduce social risk when asking for help. Rather than replacing human support, LLMs appear to fill emotional and practical gaps within existing support systems. These findings show a change in maternal digital support and highlight the need to design LLM systems that support both information and emotional safety.",
      "published": "2026-02-15",
      "updated": "2026-02-15",
      "pdf_url": "https://arxiv.org/pdf/2602.13941v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.13860v1",
      "title": "Tutoring Large Language Models to be Domain-adaptive, Precise, and Safe",
      "authors": [
        "Somnath Banerjee"
      ],
      "abstract": "The overarching research direction of this work is the development of a ''Responsible Intelligence'' framework designed to reconcile the immense generative power of Large Language Models (LLMs) with the stringent requirements of real-world deployment. As these models become a transformative force in artificial intelligence, there is an urgent need to move beyond general-purpose architectures toward systems that are contextually aware, inherently safer, and deeply respectful of global cultural nuances. This research navigates three interconnected threads: domain adaptation to ensure technical precision, ethical rigor to mitigate adversarial vulnerabilities, and cultural/multilingual alignment to promote global inclusivity. The methodological trajectory moves from classical supervised adaptation for task-specific demands to decoding-time alignment for safety, finally leveraging human feedback and preference modeling to achieve sociolinguistic acuity.",
      "published": "2026-02-14",
      "updated": "2026-02-14",
      "pdf_url": "https://arxiv.org/pdf/2602.13860v1",
      "keyword": "Large language model"
    },
    {
      "id": "http://arxiv.org/abs/2602.07905v1",
      "title": "MedCoG: Maximizing LLM Inference Density in Medical Reasoning via Meta-Cognitive Regulation",
      "authors": [
        "Yu Zhao",
        "Hao Guan",
        "Yongcheng Jing",
        "Ying Zhang",
        "Dacheng Tao"
      ],
      "abstract": "Large Language Models (LLMs) have shown strong potential in complex medical reasoning yet face diminishing gains under inference scaling laws. While existing studies augment LLMs with various knowledge types, it remains unclear how effectively the additional costs translate into accuracy. In this paper, we explore how meta-cognition of LLMs, i.e., their self-awareness of their own knowledge states, can regulate the reasoning process. Specifically, we propose MedCoG, a Medical Meta-Cognition Agent with Knowledge Graph, where the meta-cognitive assessments of task complexity, familiarity, and knowledge density dynamically regulate utilization of procedural, episodic, and factual knowledge. The LLM-centric on-demand reasoning aims to mitigate scaling laws by (1) reducing costs via avoiding indiscriminate scaling, (2) improving accuracy via filtering out distractive knowledge. To validate this, we empirically characterize the scaling curve and introduce inference density to quantify inference efficiency, defined as the ratio of theoretically effective cost to actual cost. Experiments demonstrate the effectiveness and efficiency of MedCoG on five hard sets of medical benchmarks, yielding 5.5x inference density. Furthermore, the Oracle study highlights the significant potential of meta-cognitive regulation.",
      "published": "2026-02-08",
      "updated": "2026-02-08",
      "pdf_url": "https://arxiv.org/pdf/2602.07905v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2602.07529v2",
      "title": "MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution",
      "authors": [
        "Jianwen Chen",
        "Xinyu Yang",
        "Peng Xia",
        "Arian Azarang",
        "Yueh Z Lee",
        "Gang Li",
        "Hongtu Zhu",
        "Yun Li",
        "Beidi Chen",
        "Huaxiu Yao"
      ],
      "abstract": "Large language models (LLMs) have demonstrated strong performance and rapid progress in a wide range of medical reasoning tasks. However, their sequential autoregressive decoding forces inherently parallel clinical reasoning, such as differential diagnosis, into a single linear reasoning path, limiting both efficiency and reliability for complex medical problems. To address this, we propose MedVerse, a reasoning framework for complex medical inference that reformulates medical reasoning as a parallelizable directed acyclic graph (DAG) process based on Petri net theory. The framework adopts a full-stack design across data, model architecture, and system execution. For data creation, we introduce the MedVerse Curator, an automated pipeline that synthesizes knowledge-grounded medical reasoning paths and transforms them into Petri net-structured representations. At the architectural level, we propose a topology-aware attention mechanism with adaptive position indices that supports parallel reasoning while preserving logical consistency. Systematically, we develop a customized inference engine that supports parallel execution without additional overhead. Empirical evaluations show that MedVerse improves strong general-purpose LLMs by up to 8.9%. Compared to specialized medical LLMs, MedVerse achieves comparable performance while delivering a 1.3x reduction in inference latency and a 1.7x increase in generation throughput, enabled by its parallel decoding capability. Code is available at https://github.com/aiming-lab/MedVerse.",
      "published": "2026-02-07",
      "updated": "2026-02-10",
      "pdf_url": "https://arxiv.org/pdf/2602.07529v2",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2601.20221v1",
      "title": "Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning",
      "authors": [
        "Hang Zhang",
        "Ruheng Wang",
        "Yuelyu Ji",
        "Mingu Kwak",
        "Xizhi Wu",
        "Chenyu Li",
        "Li Zhang",
        "Wenqi Shi",
        "Yifan Peng",
        "Yanshan Wang"
      ],
      "abstract": "Large language models have achieved strong performance on medical reasoning benchmarks, yet their deployment in clinical settings demands rigorous verification to ensure factual accuracy. While reward models offer a scalable approach for reasoning trace verification, existing methods face two limitations: they produce only scalar reward values without explicit justification, and they rely on single-pass retrieval that precludes adaptive knowledge access as verification unfolds. We introduce $\\method$, an agentic framework that addresses these limitations by training medical reasoning verifiers to iteratively query external medical corpora during evaluation. Our approach combines tool-augmented verification with an iterative reinforcement learning paradigm that requires only trace-level supervision, alongside an adaptive curriculum mechanism that dynamically adjusts training data distribution. Across four medical reasoning benchmarks, $\\method$ achieves substantial gains over existing methods, improving MedQA accuracy by 23.5% and MedXpertQA by 32.0% relative to the base generator in particular. Crucially, $\\method$ demonstrates an $\\mathbf{8\\times}$ reduction in sampling budget requirement compared to prior reward model baselines. These findings establish that grounding verification in dynamically retrieved evidence offers a principled path toward more reliable medical reasoning systems.",
      "published": "2026-01-28",
      "updated": "2026-01-28",
      "pdf_url": "https://arxiv.org/pdf/2601.20221v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2601.13262v1",
      "title": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning",
      "authors": [
        "Eric Onyame",
        "Akash Ghosh",
        "Subhadip Baidya",
        "Sriparna Saha",
        "Xiuying Chen",
        "Chirag Agarwal"
      ],
      "abstract": "While large language models (LLMs) have shown to perform well on monolingual mathematical and commonsense reasoning, they remain unreliable for multilingual medical reasoning applications, hindering their deployment in multilingual healthcare settings. We address this by first introducing CUREMED-BENCH, a high-quality multilingual medical reasoning dataset with open-ended reasoning queries with a single verifiable answer, spanning thirteen languages, including underrepresented languages such as Amharic, Yoruba, and Swahili. Building on this dataset, we propose CURE-MED, a curriculum-informed reinforcement learning framework that integrates code-switching-aware supervised fine-tuning and Group Relative Policy Optimization to jointly improve logical correctness and language stability. Across thirteen languages, our approach consistently outperforms strong baselines and scales effectively, achieving 85.21% language consistency and 54.35% logical correctness at 7B parameters, and 94.96% language consistency and 70.04% logical correctness at 32B parameters. These results support reliable and equitable multilingual medical reasoning in LLMs. The code and dataset are available at https://cure-med.github.io/",
      "published": "2026-01-19",
      "updated": "2026-01-19",
      "pdf_url": "https://arxiv.org/pdf/2601.13262v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2601.08267v2",
      "title": "Med-CoReasoner: Reducing Language Disparities in Medical Reasoning via Language-Informed Co-Reasoning",
      "authors": [
        "Fan Gao",
        "Sherry T. Tong",
        "Jiwoong Sohn",
        "Jiahao Huang",
        "Junfeng Jiang",
        "Ding Xia",
        "Piyalitt Ittichaiwong",
        "Kanyakorn Veerakanjana",
        "Hyunjae Kim",
        "Qingyu Chen",
        "Edison Marrese Taylor",
        "Kazuma Kobayashi",
        "Akkiko Aizawa",
        "Irene Li"
      ],
      "abstract": "While reasoning-enhanced large language models perform strongly on English medical tasks, a persistent multilingual gap remains, with substantially weaker reasoning in local languages, limiting equitable global medical deployment. To bridge this gap, we introduce Med-CoReasoner, a language-informed co-reasoning framework that elicits parallel English and local-language reasoning, abstracts them into structured concepts, and integrates local clinical knowledge into an English logical scaffold via concept-level alignment and retrieval. This design combines the structural robustness of English reasoning with the practice-grounded expertise encoded in local languages. To evaluate multilingual medical reasoning beyond multiple-choice settings, we construct MultiMed-X, a benchmark covering seven languages with expert-annotated long-form question answering and natural language inference tasks, comprising 350 instances per language. Experiments across three benchmarks show that Med-CoReasoner improves multilingual reasoning performance by an average of 5%, with particularly substantial gains in low-resource languages. Moreover, model distillation and expert evaluation analysis further confirm that Med-CoReasoner produces clinically sound and culturally grounded reasoning traces.",
      "published": "2026-01-13",
      "updated": "2026-01-19",
      "pdf_url": "https://arxiv.org/pdf/2601.08267v2",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2512.13510v1",
      "title": "MedCEG: Reinforcing Verifiable Medical Reasoning with Critical Evidence Graph",
      "authors": [
        "Linjie Mu",
        "Yannian Gu",
        "Zhongzhen Huang",
        "Yakun Zhu",
        "Shaoting Zhang",
        "Xiaofan Zhang"
      ],
      "abstract": "Large language models with reasoning capabilities have demonstrated impressive performance across a wide range of domains. In clinical applications, a transparent, step-by-step reasoning process provides physicians with strong evidence to support decision-making. While reinforcement learning has effectively enhanced reasoning performance in medical contexts, the clinical reliability of these reasoning processes remains limited because their accuracy and validity are often overlooked during training. To address this gap, we propose MedCEG, a framework that augments medical language models with clinically valid reasoning pathways by explicitly supervising the reasoning process through a Critical Evidence Graph (CEG). We curate a dataset of challenging clinical cases and algorithmically construct a CEG for each sample to represent a high-quality verifiable reasoning pathway. To guide the reasoning process, we introduce a Clinical Reasoning Procedure Reward, which evaluates Node Coverage, Structural Correctness, and Chain Completeness, thereby providing a holistic assessment of reasoning quality. Experimental results show that MedCEG surpasses existing methods in performance while producing clinically valid reasoning chains, representing a solid advancement in reliable medical AI reasoning. The code and models are available at https://github.com/LinjieMu/MedCEG.",
      "published": "2025-12-15",
      "updated": "2025-12-15",
      "pdf_url": "https://arxiv.org/pdf/2512.13510v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2512.13742v1",
      "title": "DL$^3$M: A Vision-to-Language Framework for Expert-Level Medical Reasoning through Deep Learning and Large Language Models",
      "authors": [
        "Md. Najib Hasan",
        "Imran Ahmad",
        "Sourav Basak Shuvo",
        "Md. Mahadi Hasan Ankon",
        "Sunanda Das",
        "Nazmul Siddique",
        "Hui Wang"
      ],
      "abstract": "Medical image classifiers detect gastrointestinal diseases well, but they do not explain their decisions. Large language models can generate clinical text, yet they struggle with visual reasoning and often produce unstable or incorrect explanations. This leaves a gap between what a model sees and the type of reasoning a clinician expects. We introduce a framework that links image classification with structured clinical reasoning. A new hybrid model, MobileCoAtNet, is designed for endoscopic images and achieves high accuracy across eight stomach-related classes. Its outputs are then used to drive reasoning by several LLMs. To judge this reasoning, we build two expert-verified benchmarks covering causes, symptoms, treatment, lifestyle, and follow-up care. Thirty-two LLMs are evaluated against these gold standards. Strong classification improves the quality of their explanations, but none of the models reach human-level stability. Even the best LLMs change their reasoning when prompts vary. Our study shows that combining DL with LLMs can produce useful clinical narratives, but current LLMs remain unreliable for high-stakes medical decisions. The framework provides a clearer view of their limits and a path for building safer reasoning systems. The complete source code and datasets used in this study are available at https://github.com/souravbasakshuvo/DL3M.",
      "published": "2025-12-14",
      "updated": "2025-12-14",
      "pdf_url": "https://arxiv.org/pdf/2512.13742v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2512.05658v1",
      "title": "Grounded Multilingual Medical Reasoning for Question Answering with Large Language Models",
      "authors": [
        "Pietro Ferrazzi",
        "Aitor Soroa",
        "Rodrigo Agerri"
      ],
      "abstract": "Large Language Models (LLMs) with reasoning capabilities have recently demonstrated strong potential in medical Question Answering (QA). Existing approaches are largely English-focused and primarily rely on distillation from general-purpose LLMs, raising concerns about the reliability of their medical knowledge. In this work, we present a method to generate multilingual reasoning traces grounded in factual medical knowledge. We produce 500k traces in English, Italian, and Spanish, using a retrievalaugmented generation approach over medical information from Wikipedia. The traces are generated to solve medical questions drawn from MedQA and MedMCQA, which we extend to Italian and Spanish. We test our pipeline in both in-domain and outof-domain settings across Medical QA benchmarks, and demonstrate that our reasoning traces improve performance both when utilized via in-context learning (few-shot) and supervised fine-tuning, yielding state-of-the-art results among 8B-parameter LLMs. We believe that these resources can support the development of safer, more transparent clinical decision-support tools in multilingual settings. We release the full suite of resources: reasoning traces, translated QA datasets, Medical-Wikipedia, and fine-tuned models.",
      "published": "2025-12-05",
      "updated": "2025-12-05",
      "pdf_url": "https://arxiv.org/pdf/2512.05658v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2511.23269v1",
      "title": "OctoMed: Data Recipes for State-of-the-Art Multimodal Medical Reasoning",
      "authors": [
        "Timothy Ossowski",
        "Sheng Zhang",
        "Qianchu Liu",
        "Guanghui Qin",
        "Reuben Tan",
        "Tristan Naumann",
        "Junjie Hu",
        "Hoifung Poon"
      ],
      "abstract": "High-quality and carefully curated data is a cornerstone of training medical large language models, as it directly impacts both generalization and robustness to unseen clinical tasks. We investigate strategies for training and data curation to develop a robust multimodal reasoning model in the medical domain. Our work focuses on supervised fine-tuning (SFT) and explores data recipes that leverage structured reasoning traces. Using our proposed data recipe, we scale experiments to a dataset of over 8 million examples and 6.8 billion response tokens, achieving state-of-the-art performance among open-source models across diverse out-of-distribution medical benchmark tasks. Our results further indicate that curating a high-quality, diverse training dataset with varying structured reasoning trace lengths enables the fine-tuned model to self-calibrate its reasoning trajectory lengths based on the downstream task, without explicit supervision. We present key insights, describe the data curation strategy, and outline next steps toward developing robust medical vision-language reasoning system.",
      "published": "2025-11-28",
      "updated": "2025-11-28",
      "pdf_url": "https://arxiv.org/pdf/2511.23269v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2511.00421v1",
      "title": "MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts",
      "authors": [
        "Naoto Iwase",
        "Hiroki Okuyama",
        "Junichiro Iwasawa"
      ],
      "abstract": "Large language models (LLMs) show increasing promise in medical applications, but their ability to detect and correct errors in clinical texts -- a prerequisite for safe deployment -- remains under-evaluated, particularly beyond English. We introduce MedRECT, a cross-lingual benchmark (Japanese/English) that formulates medical error handling as three subtasks: error detection, error localization (sentence extraction), and error correction. MedRECT is built with a scalable, automated pipeline from the Japanese Medical Licensing Examinations (JMLE) and a curated English counterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with comparable error/no-error balance. We evaluate 9 contemporary LLMs spanning proprietary, open-weight, and reasoning families. Key findings: (i) reasoning models substantially outperform standard architectures, with up to 13.5% relative improvement in error detection and 51.0% in sentence extraction; (ii) cross-lingual evaluation reveals 5-10% performance gaps from English to Japanese, with smaller disparities for reasoning models; (iii) targeted LoRA fine-tuning yields asymmetric improvements in error correction performance (Japanese: +0.078, English: +0.168) while preserving reasoning capabilities; and (iv) our fine-tuned model exceeds human expert performance on structured medical error correction tasks. To our knowledge, MedRECT is the first comprehensive cross-lingual benchmark for medical error correction, providing a reproducible framework and resources for developing safer medical LLMs across languages.",
      "published": "2025-11-01",
      "updated": "2025-11-01",
      "pdf_url": "https://arxiv.org/pdf/2511.00421v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2510.03536v2",
      "title": "Triplet-Structured Knowledge Integration for Multi-Turn Medical Reasoning",
      "authors": [
        "Zhaohan Meng",
        "Zaiqiao Meng",
        "Siwei Liu",
        "Iadh Ounis"
      ],
      "abstract": "Large Language Models (LLMs) have shown strong performance on static medical Question Answering (QA) tasks, yet their reasoning often deteriorates in multi-turn clinical dialogues where patient information is scattered across turns. This paper introduces TriMediQ, a triplet-structured approach that enhances the reasoning reliability of LLMs through explicit knowledge integration. TriMediQ first employs a frozen triplet extraction LLM to convert patient responses into clinically grounded triplets, ensuring factual precision via constrained prompting. These triplets are incorporated into a patient-specific Knowledge Graph (KG), from which a trainable projection module consisting of a graph encoder and a projector captures relational dependencies while keeping all LLM parameters frozen. During inference, the projection module guides multi-hop reasoning over the KG, enabling coherent clinical dialogue understanding. Experiments on two interactive medical QA benchmarks show that TriMediQ achieves up to 10.4\\% improvement in accuracy over five existing baselines on the iMedQA dataset. These results demonstrate that structuring patient information as triplets can effectively improve the reasoning capability of LLMs in multi-turn medical QA.",
      "published": "2025-10-03",
      "updated": "2025-10-14",
      "pdf_url": "https://arxiv.org/pdf/2510.03536v2",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2509.23725v2",
      "title": "MedLA: A Logic-Driven Multi-Agent Framework for Complex Medical Reasoning with Large Language Models",
      "authors": [
        "Siqi Ma",
        "Jiajie Huang",
        "Fan Zhang",
        "Jinlin Wu",
        "Yue Shen",
        "Guohui Fan",
        "Zhu Zhang",
        "Zelin Zang"
      ],
      "abstract": "Answering complex medical questions requires not only domain expertise and patient-specific information, but also structured and multi-perspective reasoning. Existing multi-agent approaches often rely on fixed roles or shallow interaction prompts, limiting their ability to detect and resolve fine-grained logical inconsistencies. To address this, we propose \\textsc{MedLA}, a logic-driven multi-agent framework built on large language models. Each agent organizes its reasoning process into an explicit logical tree based on syllogistic triads (major premise, minor premise, and conclusion), enabling transparent inference and premise-level alignment. Agents engage in a multi-round, graph-guided discussion to compare and iteratively refine their logic trees, achieving consensus through error correction and contradiction resolution. We demonstrate that \\textsc{MedLA} consistently outperforms both static role-based systems and single-agent baselines on challenging benchmarks such as MedDDx and standard medical QA tasks. Furthermore, \\textsc{MedLA} scales effectively across both open-source and commercial LLM backbones, achieving state-of-the-art performance and offering a generalizable paradigm for trustworthy medical reasoning.",
      "published": "2025-09-28",
      "updated": "2025-11-19",
      "pdf_url": "https://arxiv.org/pdf/2509.23725v2",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2509.23368v1",
      "title": "MedCritical: Enhancing Medical Reasoning in Small Language Models via Self-Collaborative Correction",
      "authors": [
        "Xinchun Su",
        "Chunxu Luo",
        "Yixuan Li",
        "Weidong Yang",
        "Lipeng Ma"
      ],
      "abstract": "In the field of medicine, complex reasoning tasks such as clinical diagnosis, treatment planning, and medical knowledge integration pose significant challenges, where small language models often underperform compared to large language models like GPT-4 and Deepseek. Recent knowledge distillation-based methods aim to address these issues through teacher-guided error correction, but this LLM as judge approach remains challenging in terms of cost, time, and efficiency. To circumvent this issue, we propose a novel two-stage framework, MedCritical, which uses a small language model fine-tuned by a large teacher model to play against itself. In the first stage, we extract high-level and detailed long-chain thought templates from the teacher model to guide the student model to generate more complex reasoning thoughts. In the second stage, we introduce direct preference optimization (DPO) through model self-iteration collaboration to enhance the reasoning ability of the student model by playing against the correction trajectory of the fine-tuned model during training. This model self-learning DPO approach teaches the student model to use its own error-driven insights to consolidate its skills and knowledge to solve complex problems, and achieves comparable results to traditional knowledge distillation methods using teacher models at a lower cost. Notably, our MedCritical 7B model outperforms the Taiyi and Huatuo-o1-7B models by 3.04\\% and 10.12\\% respectively on the CMExam benchmark, achieving new SOTA performance among 7B-class small models.",
      "published": "2025-09-27",
      "updated": "2025-09-27",
      "pdf_url": "https://arxiv.org/pdf/2509.23368v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2509.22713v1",
      "title": "RAR$^2$: Retrieval-Augmented Medical Reasoning via Thought-Driven Retrieval",
      "authors": [
        "Kaishuai Xu",
        "Wenjun Hou",
        "Yi Cheng",
        "Wenjie Li"
      ],
      "abstract": "Large Language Models (LLMs) have shown promising performance on diverse medical benchmarks, highlighting their potential in supporting real-world clinical tasks. Retrieval-Augmented Generation (RAG) has emerged as a key approach for mitigating knowledge gaps and hallucinations by incorporating external medical information. However, RAG still struggles with complex medical questions that require intensive reasoning, as surface-level input often fails to reflect the true knowledge needs of the task. Existing methods typically focus on refining queries without explicitly modeling the reasoning process, limiting their ability to retrieve and integrate clinically relevant knowledge. In this work, we propose RAR$^2$, a joint learning framework that improves both Reasoning-Augmented Retrieval and Retrieval-Augmented Reasoning. RAR$^2$ constructs a thought process to uncover implicit knowledge requirements and uses it to guide retrieval and answer generation. We build a training dataset of mixed preference pairs and apply Direct Preference Optimization (DPO) to train the model. Moreover, we design two test-time scaling strategies to explore the boundaries of our framework. Experiments demonstrate the effectiveness of RAR$^2$ across several biomedical question answering datasets, outperforming RAG baselines with or without fine-tuning.",
      "published": "2025-09-24",
      "updated": "2025-09-24",
      "pdf_url": "https://arxiv.org/pdf/2509.22713v1",
      "keyword": "Medical reasoning LLM"
    },
    {
      "id": "http://arxiv.org/abs/2509.15279v1",
      "title": "Fleming-R1: Toward Expert-Level Medical Reasoning via Reinforcement Learning",
      "authors": [
        "Chi Liu",
        "Derek Li",
        "Yan Shu",
        "Robin Chen",
        "Derek Duan",
        "Teng Fang",
        "Bryan Dai"
      ],
      "abstract": "While large language models show promise in medical applications, achieving expert-level clinical reasoning remains challenging due to the need for both accurate answers and transparent reasoning processes. To address this challenge, we introduce Fleming-R1, a model designed for verifiable medical reasoning through three complementary innovations. First, our Reasoning-Oriented Data Strategy (RODS) combines curated medical QA datasets with knowledge-graph-guided synthesis to improve coverage of underrepresented diseases, drugs, and multi-hop reasoning chains. Second, we employ Chain-of-Thought (CoT) cold start to distill high-quality reasoning trajectories from teacher models, establishing robust inference priors. Third, we implement a two-stage Reinforcement Learning from Verifiable Rewards (RLVR) framework using Group Relative Policy Optimization, which consolidates core reasoning skills while targeting persistent failure modes through adaptive hard-sample mining. Across diverse medical benchmarks, Fleming-R1 delivers substantial parameter-efficient improvements: the 7B variant surpasses much larger baselines, while the 32B model achieves near-parity with GPT-4o and consistently outperforms strong open-source alternatives. These results demonstrate that structured data design, reasoning-oriented initialization, and verifiable reinforcement learning can advance clinical reasoning beyond simple accuracy optimization. We release Fleming-R1 publicly to promote transparent, reproducible, and auditable progress in medical AI, enabling safer deployment in high-stakes clinical environments.",
      "published": "2025-09-18",
      "updated": "2025-09-18",
      "pdf_url": "https://arxiv.org/pdf/2509.15279v1",
      "keyword": "Medical reasoning LLM"
    }
  ]
}